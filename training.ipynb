{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds  # TFDS to download MNIST.\n",
    "import tensorflow as tf  # TensorFlow / `tf.data` operations.\n",
    "from flax import nnx  # The Flax NNX API.\n",
    "from functools import partial\n",
    "import jax.numpy as jnp  # JAX NumPy\n",
    "import jax\n",
    "import optax\n",
    "from linearRNN import forward_h\n",
    "from linearRNN import forward\n",
    "from linearRNN import init_lru_parameters\n",
    "import numpy as np\n",
    "from flax import linen as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn=1 #rnn=0:transformation of the inputs with fixed RNN weights, rnn=1: adding the RNN module on the model to learn the weight matrices\n",
    "hidden_neuron=256\n",
    "hidden_size=2048\n",
    "learning_rate = 0.005\n",
    "momentum = 0.9\n",
    "train_steps=2500\n",
    "eval_every = 100\n",
    "batch_size=32\n",
    "method_name=\"LRUMLPfix2\"\n",
    "dataset_name=\"MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import data\n",
    "cifar=tf.keras.datasets.mnist.load_data()\n",
    "train=cifar[0]\n",
    "test=cifar[1]\n",
    "\n",
    "train_x_len=train[0].shape[0]\n",
    "train_x_size=int(jnp.prod(jnp.array(train[0].shape[1:])))\n",
    "test_x_len=test[0].shape[0]\n",
    "test_x_size=int(jnp.prod(jnp.array(test[0].shape[1:])))\n",
    "\n",
    "train_x=train[0].reshape((train_x_len,train_x_size))/255\n",
    "train_y=train[1].reshape(train_x_len)\n",
    "\n",
    "\n",
    "test_x=test[0].reshape((test_x_len,test_x_size))/255\n",
    "test_y=test[1].reshape(test_x_len)\n",
    "\n",
    "#test_x=jnp.append(train_x[30000:],test_x,axis=0)\n",
    "#test_y=jnp.append(train_y[30000:],test_y)\n",
    "#\n",
    "#train_x=train_x[:30000]\n",
    "#train_y=train_y[:30000]\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rnn:\n",
    "    train_hidden_state=train_x\n",
    "    test_hidden_state=test_x\n",
    "else:\n",
    "    param=init_lru_parameters(hidden_size,train_x_size,r_min=0.999,r_max=0.9999,max_phase=6.28)\n",
    "    param2=init_lru_parameters(hidden_size,train_x_size,r_min=0.999,r_max=0.9999,max_phase=6.28)\n",
    "    train_hidden_state=jnp.real(forward_h(param,train_x))\n",
    "    test_hidden_state=jnp.real(forward_h(param2,test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds=tf.data.Dataset.from_tensor_slices((jnp.real(train_hidden_state),jnp.array(train_y,dtype=int)))\n",
    "test_ds=tf.data.Dataset.from_tensor_slices((jnp.real(test_hidden_state),jnp.array(test_y,dtype=int)))\n",
    "\n",
    "train_ds = train_ds.repeat().shuffle(100)\n",
    "# Group into batches of `batch_size` and skip incomplete batches, prefetch the next sample to improve latency.\n",
    "train_ds = train_ds.batch(batch_size, drop_remainder=True).take(train_steps).prefetch(1)\n",
    "# Group into batches of `batch_size` and skip incomplete batches, prefetch the next sample to improve latency.\n",
    "test_ds = test_ds.batch(batch_size, drop_remainder=True).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_hidden_state.shape)\n",
    "print(test_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import nnx\n",
    "import optax\n",
    "from flax.nnx.nn.recurrent import LSTMCell,GRUCell\n",
    "from func import LRNNCell,LRUCell\n",
    "\n",
    "class MLP(nnx.Module):\n",
    "  def __init__(self, din, dmid, dout, rngs: nnx.Rngs):\n",
    "    self.rnn=nnx.RNN(LSTMCell(in_features=train_x_size, hidden_features=din, rngs=nnx.Rngs(0)))\n",
    "    self.linear = nnx.Linear(din, dmid, rngs=rngs)\n",
    "    self.linear_out = nnx.Linear(dmid, dout, rngs=rngs)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    if rnn:\n",
    "      x=self.rnn(x)\n",
    "    x=self.linear(x)\n",
    "    x = nnx.softplus(x)\n",
    "    return self.linear_out(x)\n",
    "\n",
    "model = MLP(hidden_size, hidden_neuron, 10, rngs=nnx.Rngs(0))  # eager initialization\n",
    "\n",
    "nnx.display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "optimizer = nnx.Optimizer(model, optax.adamw(learning_rate, momentum))\n",
    "metrics = nnx.MultiMetric(\n",
    "  accuracy=nnx.metrics.Accuracy(),\n",
    "  loss=nnx.metrics.Average('loss'),\n",
    ")\n",
    "\n",
    "nnx.display(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "def loss_fn(model: MLP, batch):\n",
    "  logits = model(batch[0])\n",
    "  loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "    logits=logits, labels=batch[1]\n",
    "  ).mean()\n",
    "  #print(logits.shape)\n",
    "  #print(batch[1].shape)\n",
    "  return loss, logits\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(model: MLP, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
    "  \"\"\"Train for a single step.\"\"\"\n",
    "  grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
    "  (loss, logits), grads = grad_fn(model, batch)\n",
    "  metrics.update(loss=loss, logits=logits, labels=batch[1])  # In-place updates.\n",
    "  optimizer.update(grads)  # In-place updates.\n",
    "  predicted_labels = jnp.argmax(logits, axis=-1)\n",
    "  actual_labels = batch[1]\n",
    "  #jax.debug.print(\"Predictions: {}\",predicted_labels[:5].astype(int))\n",
    "  #jax.debug.print(\"Actual Labels: {}\",actual_labels[:5].astype(int))\n",
    "  \n",
    "@nnx.jit\n",
    "def eval_step(model: MLP, metrics: nnx.MultiMetric, batch):\n",
    "  loss, logits = loss_fn(model, batch)\n",
    "  metrics.update(loss=loss, logits=logits, labels=batch[1])  # In-place updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model + evaluation with the test data\n",
    "metrics_history = {\n",
    "    'train_loss': [],\n",
    "    'train_accuracy': [],\n",
    "    'test_loss': [],\n",
    "    'test_accuracy': [],\n",
    "}\n",
    "\n",
    "for step, batch in enumerate(train_ds.as_numpy_iterator()):\n",
    "  # Run the optimization for one step and make a stateful update to the following:\n",
    "  # - The train state's model parameters\n",
    "  # - The optimizer state\n",
    "  # - The training loss and accuracy batch metrics\n",
    "  train_step(model, optimizer, metrics, batch)\n",
    "\n",
    "  if step > 0 and (step % eval_every == 0 or step == train_steps - 1):  # One training epoch has passed.\n",
    "    # Log the training metrics.\n",
    "    for metric, value in metrics.compute().items():  # Compute the metrics.\n",
    "      metrics_history[f'train_{metric}'].append(value)  # Record the metrics.\n",
    "    metrics.reset()  # Reset the metrics for the test set.\n",
    "\n",
    "    # Compute the metrics on the test set after each training epoch.\n",
    "    for test_batch in test_ds.as_numpy_iterator():\n",
    "      eval_step(model, metrics, test_batch)\n",
    "\n",
    "    # Log the test metrics.\n",
    "    for metric, value in metrics.compute().items():\n",
    "      metrics_history[f'test_{metric}'].append(value)\n",
    "    metrics.reset()  # Reset the metrics for the next training epoch.\n",
    "\n",
    "    print(\n",
    "      f\"[train] step: {step}, \"\n",
    "      f\"loss: {metrics_history['train_loss'][-1]}, \"\n",
    "      f\"accuracy: {metrics_history['train_accuracy'][-1] * 100}\"\n",
    "    )\n",
    "    print(\n",
    "      f\"[test] step: {step}, \"\n",
    "      f\"loss: {metrics_history['test_loss'][-1]}, \"\n",
    "      f\"accuracy: {metrics_history['test_accuracy'][-1] * 100}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the training results into csv\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "data=pd.DataFrame({\"step\":np.arange(eval_every,train_steps+eval_every,eval_every),\"train_loss\":metrics_history['train_loss'],\n",
    "                   \"test_loss\":metrics_history['test_loss'],\"train_accuracy\":metrics_history['train_accuracy'],\n",
    "                   \"test_accuracy\":metrics_history['test_accuracy']})\n",
    "data.to_csv(method_name+\"_nr\"+str(hidden_neuron)+\"_d\"+str(hidden_size)+\"_\"+dataset_name+\"_step\"+str(train_steps)+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.arange(eval_every,train_steps+eval_every,eval_every),metrics_history['train_loss'],label=\"train loss\")\n",
    "plt.plot(np.arange(eval_every,train_steps+eval_every,eval_every),metrics_history['test_loss'],label=\"test loss\")\n",
    "plt.title(\"Train loss of MNIST dataset with GRU+MLP, \\nhidden dimension=\"+str(hidden_size)+\", number of neuron=\"+str(hidden_neuron))\n",
    "plt.xlabel(\"Training step\")\n",
    "plt.ylabel(\"Train loss (cross entropy)\")\n",
    "plt.legend()\n",
    "plt.savefig(\"loss_\"+method_name+\"_\"+str(hidden_neuron)+\"_\"+dataset_name+\"_step\"+str(train_steps)+\".jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the accuracy\n",
    "plt.plot(np.arange(eval_every,train_steps+eval_every,eval_every),metrics_history['train_accuracy'],label=\"train\")\n",
    "plt.plot(np.arange(eval_every,train_steps+eval_every,eval_every),metrics_history['test_accuracy'],label=\"test\")\n",
    "plt.title(\"Accuracy of MNIST dataset with LRUfix+MLP, \\nhidden dimension=\"+str(hidden_size)+\", number of neuron=\"+str(hidden_neuron))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.savefig(\"accuracy_\"+method_name+\"_\"+str(hidden_neuron)+\"_\"+dataset_name+\"_step\"+str(train_steps)+\".jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
