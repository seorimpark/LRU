{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds  # TFDS to download MNIST.\n",
    "import tensorflow as tf  # TensorFlow / `tf.data` operations.\n",
    "from flax import nnx  # The Flax NNX API.\n",
    "from functools import partial\n",
    "import jax.numpy as jnp  # JAX NumPy\n",
    "import jax\n",
    "import optax\n",
    "from linearRNN import forward_h\n",
    "from linearRNN import forward\n",
    "from linearRNN import init_lru_parameters\n",
    "from linearRNN import binary_operator_diag\n",
    "from linearRNN import LRU\n",
    "import numpy as np\n",
    "from flax import linen as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool=0 #pooling layer after MLP is taking the average over the numbers\n",
    "transformation=0 #transformation of the data from decimals between 0 and 255 to binary 8 bit numbers\n",
    "leave_data=1 #download csv data of the results\n",
    "hidden_neuron=128 #no details in the 2023 paper => 2024 paper fixed to 512\n",
    "encoded_size=256\n",
    "hidden_size=128\n",
    "learning_rate = 0.004\n",
    "momentum = 0.9\n",
    "train_steps=3000\n",
    "eval_every = 50\n",
    "batch_size=50\n",
    "r_min = 0\n",
    "r_max = 1\n",
    "max_phase = 6.28\n",
    "depth=1\n",
    "method_name=\"LRUMLP\"\n",
    "dataset_name=\"MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr(mode:str,d,m,l,k):\n",
    "    lr=0\n",
    "    sigma=0\n",
    "    if mode==\"input\":\n",
    "        lr=m/(jnp.power(l,3/2)*d)\n",
    "        sigma=1/jnp.sqrt(d)\n",
    "    if mode==\"hidden\":\n",
    "        lr=1/jnp.power(l,3/2)\n",
    "        sigma=2/jnp.sqrt((m+d)/2)\n",
    "    if mode==\"output\":\n",
    "        lr=k/(jnp.power(l,3/2)*m)\n",
    "        sigma=jnp.sqrt(k)/m\n",
    "    else:\n",
    "        raise ValueError\n",
    "    return lr, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_bin_array(arr, m):#https://stackoverflow.com/questions/22227595/convert-integer-to-binary-array-with-suitable-padding\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "    arr: Numpy array of positive integers\n",
    "    m: Number of bits of each integer to retain\n",
    "\n",
    "    Returns a copy of arr with every element replaced with a bit vector.\n",
    "    Bits encoded as int8's.\n",
    "    \"\"\"\n",
    "    to_str_func = np.vectorize(lambda x: np.binary_repr(x).zfill(m))\n",
    "    strs = to_str_func(arr)\n",
    "    ret = np.zeros(list(arr.shape) + [m], dtype=np.int8)\n",
    "    for bit_ix in range(0, m):\n",
    "        fetch_bit_func = np.vectorize(lambda x: x[bit_ix] == '1')\n",
    "        ret[...,bit_ix] = fetch_bit_func(strs).astype(\"int8\")\n",
    "\n",
    "    return ret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import data\n",
    "\n",
    "if dataset_name==\"MNIST\":\n",
    "    dataset=tf.keras.datasets.mnist.load_data()\n",
    "    train=dataset[0]\n",
    "    test=dataset[1]\n",
    "\n",
    "    train_x_seq=train[0].shape[0]\n",
    "    train_x_len=int(jnp.prod(jnp.array(train[0].shape[1:])))\n",
    "    train_x_size=1\n",
    "    test_x_seq=test[0].shape[0]\n",
    "    test_x_len=int(jnp.prod(jnp.array(test[0].shape[1:])))\n",
    "    test_x_size=1\n",
    "    if transformation: #transform the information of the pixel to 8-bit binary numbers\n",
    "        train_x=train[0].reshape((train_x_seq,train_x_len,train_x_size))\n",
    "        train_x=vec_bin_array(train_x,8)\n",
    "        train_y=train[1].reshape(train_x_seq)\n",
    "        train_y_class=len(jnp.unique(train_y))\n",
    "        test_x=test[0].reshape((test_x_seq,test_x_len,test_x_size))\n",
    "        test_x=vec_bin_array(test_x,8)\n",
    "        test_y=test[1].reshape(test_x_seq)\n",
    "    else:\n",
    "        train_x=train[0].reshape((train_x_seq,train_x_len,train_x_size))/255\n",
    "        train_y=train[1].reshape(train_x_seq)\n",
    "        train_y_class=len(jnp.unique(train_y))\n",
    "        test_x=test[0].reshape((test_x_seq,test_x_len,test_x_size))/255\n",
    "        test_y=test[1].reshape(test_x_seq)\n",
    "\n",
    "if dataset_name==\"CIFAR10\":\n",
    "    dataset=tf.keras.datasets.cifar10.load_data()\n",
    "    train=dataset[0]\n",
    "    test=dataset[1]\n",
    "\n",
    "    train_x_seq=train[0].shape[0]\n",
    "    train_x_len=int(jnp.prod(jnp.array(train[0].shape[1:-1])))\n",
    "    train_x_size=int(jnp.prod(jnp.array(train[0].shape[-1])))\n",
    "\n",
    "    test_x_seq=test[0].shape[0]\n",
    "    test_x_len=int(jnp.prod(jnp.array(test[0].shape[1:-1])))\n",
    "    test_x_size=int(jnp.prod(jnp.array(train[0].shape[-1])))\n",
    "\n",
    "    if transformation:#transform the information of the pixel to 3*8-bit binary numbers\n",
    "        train_x=train[0].reshape((train_x_seq,train_x_len,train_x_size))\n",
    "        train_x=vec_bin_array(train_x,8)\n",
    "        train_y=train[1].reshape(train_x_seq)\n",
    "        train_y_class=len(jnp.unique(train_y))\n",
    "        test_x=test[0].reshape((test_x_seq,test_x_len,test_x_size))\n",
    "        test_x=vec_bin_array(test_x,8)\n",
    "        test_y=test[1].reshape(test_x_seq)\n",
    "    else:\n",
    "        train_x=train[0].reshape((train_x_seq,train_x_len,train_x_size))/255\n",
    "        train_y=train[1].reshape(train_x_seq)\n",
    "        train_y_class=len(jnp.unique(train_y))\n",
    "        test_x=test[0].reshape((test_x_seq,test_x_len,test_x_size))/255\n",
    "        test_y=test[1].reshape(test_x_seq)\n",
    "\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds=tf.data.Dataset.from_tensor_slices((jnp.real(train_x),jnp.array(train_y,dtype=int)))\n",
    "test_ds=tf.data.Dataset.from_tensor_slices((jnp.real(test_x),jnp.array(test_y,dtype=int)))\n",
    "\n",
    "train_ds = train_ds.repeat().shuffle(100)\n",
    "# Group into batches of `batch_size` and skip incomplete batches, prefetch the next sample to improve latency.\n",
    "train_ds = train_ds.batch(batch_size, drop_remainder=True).take(train_steps).prefetch(1)\n",
    "# Group into batches of `batch_size` and skip incomplete batches, prefetch the next sample to improve latency.\n",
    "test_ds = test_ds.batch(batch_size, drop_remainder=True).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  lin_encoder=Linear(\n",
      "    kernel=Param(\n",
      "      value=Array(shape=(1, 256), dtype=float32)\n",
      "    ),\n",
      "    bias=Param(\n",
      "      value=Array(shape=(256,), dtype=float32)\n",
      "    ),\n",
      "    in_features=1,\n",
      "    out_features=256,\n",
      "    use_bias=True,\n",
      "    dtype=None,\n",
      "    param_dtype=<class 'jax.numpy.float32'>,\n",
      "    precision=None,\n",
      "    kernel_init=<function variance_scaling.<locals>.init at 0x0000024A433FAE80>,\n",
      "    bias_init=<function zeros at 0x0000024A400263E0>,\n",
      "    dot_general=<function dot_general at 0x0000024A3FB90FE0>\n",
      "  ),\n",
      "  rnn=LRU(\n",
      "    in_features=256,\n",
      "    hidden_features=128,\n",
      "    nu_log=Param(\n",
      "      value=Array(shape=(128,), dtype=float64)\n",
      "    ),\n",
      "    theta_log=Param(\n",
      "      value=Array(shape=(128,), dtype=float64)\n",
      "    ),\n",
      "    B_re=Param(\n",
      "      value=Array(shape=(128, 256), dtype=float64)\n",
      "    ),\n",
      "    B_im=Param(\n",
      "      value=Array(shape=(128, 256), dtype=float64)\n",
      "    ),\n",
      "    C_re=Param(\n",
      "      value=Array(shape=(256, 128), dtype=float64)\n",
      "    ),\n",
      "    C_im=Param(\n",
      "      value=Array(shape=(256, 128), dtype=float64)\n",
      "    ),\n",
      "    D=Param(\n",
      "      value=Array(shape=(256,), dtype=float64)\n",
      "    ),\n",
      "    gamma_log=Param(\n",
      "      value=Array(shape=(128,), dtype=float64)\n",
      "    )\n",
      "  ),\n",
      "  linear1=Linear(\n",
      "    kernel=Param(\n",
      "      value=Array(shape=(256, 128), dtype=float32)\n",
      "    ),\n",
      "    bias=Param(\n",
      "      value=Array(shape=(128,), dtype=float32)\n",
      "    ),\n",
      "    in_features=256,\n",
      "    out_features=128,\n",
      "    use_bias=True,\n",
      "    dtype=None,\n",
      "    param_dtype=<class 'jax.numpy.float32'>,\n",
      "    precision=None,\n",
      "    kernel_init=<function variance_scaling.<locals>.init at 0x0000024A433FAE80>,\n",
      "    bias_init=<function zeros at 0x0000024A400263E0>,\n",
      "    dot_general=<function dot_general at 0x0000024A3FB90FE0>\n",
      "  ),\n",
      "  linear2=Linear(\n",
      "    kernel=Param(\n",
      "      value=Array(shape=(64, 256), dtype=float32)\n",
      "    ),\n",
      "    bias=Param(\n",
      "      value=Array(shape=(256,), dtype=float32)\n",
      "    ),\n",
      "    in_features=64,\n",
      "    out_features=256,\n",
      "    use_bias=True,\n",
      "    dtype=None,\n",
      "    param_dtype=<class 'jax.numpy.float32'>,\n",
      "    precision=None,\n",
      "    kernel_init=<function variance_scaling.<locals>.init at 0x0000024A433FAE80>,\n",
      "    bias_init=<function zeros at 0x0000024A400263E0>,\n",
      "    dot_general=<function dot_general at 0x0000024A3FB90FE0>\n",
      "  ),\n",
      "  batchnorm=BatchNorm(\n",
      "    mean=BatchStat(\n",
      "      value=Array(shape=(256,), dtype=float32)\n",
      "    ),\n",
      "    var=BatchStat(\n",
      "      value=Array(shape=(256,), dtype=float32)\n",
      "    ),\n",
      "    scale=Param(\n",
      "      value=Array(shape=(256,), dtype=float32)\n",
      "    ),\n",
      "    bias=Param(\n",
      "      value=Array(shape=(256,), dtype=float32)\n",
      "    ),\n",
      "    num_features=256,\n",
      "    use_running_average=True,\n",
      "    axis=-1,\n",
      "    momentum=0.99,\n",
      "    epsilon=1e-05,\n",
      "    dtype=None,\n",
      "    param_dtype=<class 'jax.numpy.float32'>,\n",
      "    use_bias=True,\n",
      "    use_scale=True,\n",
      "    bias_init=<function zeros at 0x0000024A400263E0>,\n",
      "    scale_init=<function ones at 0x0000024A400B32E0>,\n",
      "    axis_name=None,\n",
      "    axis_index_groups=None,\n",
      "    use_fast_variance=True\n",
      "  ),\n",
      "  linear3=Linear(\n",
      "    kernel=Param(\n",
      "      value=Array(shape=(784, 1), dtype=float32)\n",
      "    ),\n",
      "    bias=Param(\n",
      "      value=Array([0.], dtype=float32)\n",
      "    ),\n",
      "    in_features=784,\n",
      "    out_features=1,\n",
      "    use_bias=True,\n",
      "    dtype=None,\n",
      "    param_dtype=<class 'jax.numpy.float32'>,\n",
      "    precision=None,\n",
      "    kernel_init=<function variance_scaling.<locals>.init at 0x0000024A433FAE80>,\n",
      "    bias_init=<function zeros at 0x0000024A400263E0>,\n",
      "    dot_general=<function dot_general at 0x0000024A3FB90FE0>\n",
      "  ),\n",
      "  linear4=Linear(\n",
      "    kernel=Param(\n",
      "      value=Array(shape=(256, 10), dtype=float32)\n",
      "    ),\n",
      "    bias=Param(\n",
      "      value=Array(shape=(10,), dtype=float32)\n",
      "    ),\n",
      "    in_features=256,\n",
      "    out_features=10,\n",
      "    use_bias=True,\n",
      "    dtype=None,\n",
      "    param_dtype=<class 'jax.numpy.float32'>,\n",
      "    precision=None,\n",
      "    kernel_init=<function variance_scaling.<locals>.init at 0x0000024A433FAE80>,\n",
      "    bias_init=<function zeros at 0x0000024A400263E0>,\n",
      "    dot_general=<function dot_general at 0x0000024A3FB90FE0>\n",
      "  ),\n",
      "  out_dim=10,\n",
      "  token_len=784\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from flax import nnx\n",
    "import optax\n",
    "from flax.nnx.nn.recurrent import LSTMCell,GRUCell\n",
    "import copy\n",
    "import random\n",
    "class MLP(nnx.Module):\n",
    "  #DON'T FORGET TO CHANGE THE MODEL NAME BEFORE RUNNING\n",
    "  #According to the scheme of the paper (Figure 1), input_size=M, encoded_size=H,layer_dim=number of neurons in MLP, out_dim=number of classes\n",
    "  def __init__(self, token_size, token_len,encoded_dim,hidden_dim, layer_dim, out_dim, rngs: nnx.Rngs):\n",
    "\n",
    "    #linear encoder\n",
    "    self.lin_encoder = nnx.Linear(in_features=token_size, out_features=encoded_dim,rngs=rngs)\n",
    "    #self.lin_encoder=nnx.Param(jnp.array(np.random.rand(token_size,encoded_dim)))\n",
    "    #LRU+MLP block\n",
    "    self.rnn = LRU(in_features=encoded_dim, hidden_features=hidden_dim, r_min=r_min,r_max=r_max,max_phase=max_phase)\n",
    "    self.linear1 = nnx.Linear(in_features=encoded_dim, out_features=layer_dim, rngs=rngs)\n",
    "    self.linear2 = nnx.Linear(in_features=layer_dim//2,out_features=encoded_dim,rngs=rngs)\n",
    "    self.batchnorm=nnx.BatchNorm(num_features=encoded_dim,rngs=rngs,use_running_average=True)\n",
    "    #Linear layers\n",
    "    if pool: #If pooling layer takes the average over the token sequence length \n",
    "      self.linear3=(lambda x: jnp.mean(x,axis=1))\n",
    "    else: #learn the parameters of the linear transformation\n",
    "      self.linear3= nnx.Linear(in_features=token_len,out_features=1,rngs=rngs)\n",
    "    self.linear4= nnx.Linear(in_features=encoded_dim,out_features=out_dim,rngs=rngs)\n",
    "    #self.weight = nnx.Param(jnp.array(np.random.rand(token_len,1)))\n",
    "    #self.bias = nnx.Param(jnp.array(np.random.rand(encoded_dim,1)))\n",
    "    #self.weight2 = nnx.Param(jnp.array(np.random.rand(out_dim,encoded_dim)))\n",
    "    #self.bias2 = nnx.Param(jnp.array(np.random.rand(out_dim,1)))\n",
    "    self.out_dim = out_dim\n",
    "    self.token_len=token_len\n",
    "\n",
    "    \n",
    "  @nnx.vmap(in_axes=(None,0)) \n",
    "  def __call__(self, x):\n",
    "    x = self.lin_encoder(x)\n",
    "    #x=x@self.lin_encoder\n",
    "    y = x.copy()\n",
    "    #LRU+MLP block\n",
    "    for i in range(depth):\n",
    "      x = self.rnn(x)\n",
    "      x = self.linear1(x)\n",
    "      x = nnx.glu(x,axis=-1)\n",
    "      x = self.linear2(x)\n",
    "      x += y #Skip connection -> p.21 adding for each block\n",
    "      x=self.batchnorm(x)#batch normalization\n",
    "\n",
    "    #x = x.T@self.weight #+ self.bias #project from L*H to H*1\n",
    "    #x = self.weight2@x #+ self.bias2#project from H*1 to out_dim\n",
    "    x=self.linear3(x.T)\n",
    "    x=self.linear4(x.T)\n",
    "    return x.reshape(self.out_dim)\n",
    "\n",
    "\n",
    "model = MLP(train_x_size,train_x_len,encoded_size,hidden_size, hidden_neuron, train_y_class, rngs=nnx.Rngs(0))  # eager initialization\n",
    "\n",
    "nnx.display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.03168893e-01 -9.66499567e-01  1.49352655e-01 -8.09825361e-01\n",
      "   4.07850713e-01 -9.96469975e-01 -4.30168621e-02  2.73884200e-02\n",
      "  -8.04168224e-01 -6.10215425e-01]\n",
      " [ 1.59678176e-01 -4.98017341e-01  1.36498183e-01 -3.53908539e-01\n",
      "   1.80679873e-01 -4.69490588e-01 -4.97057177e-02  1.37789287e-02\n",
      "  -4.29928005e-01 -3.55306953e-01]\n",
      " [ 3.59912932e-01 -8.56598556e-01  2.32310012e-01 -7.57348418e-01\n",
      "   3.99321884e-01 -8.78117383e-01  6.85079098e-02  1.50126338e-01\n",
      "  -8.27721655e-01 -4.97802675e-01]\n",
      " [ 4.93077710e-02 -7.27952600e-01  1.02801889e-01 -5.36433995e-01\n",
      "   2.78145850e-01 -6.34684324e-01  7.33526349e-02 -6.96805641e-02\n",
      "  -5.64702094e-01 -3.73918504e-01]\n",
      " [ 3.80657285e-01 -5.62055886e-01  1.57801181e-01 -4.98182625e-01\n",
      "   2.35917374e-01 -6.69640064e-01 -3.03118993e-02  1.45991340e-01\n",
      "  -5.50191522e-01 -3.86757404e-01]\n",
      " [-1.12192608e-01 -8.37540090e-01  1.55021325e-01 -6.01021886e-01\n",
      "   2.32720032e-01 -6.22119606e-01  1.71095565e-01 -2.57776558e-01\n",
      "  -4.74768907e-01 -3.05242956e-01]\n",
      " [-6.79273367e-01  7.38229632e-01 -3.40703934e-01  6.80850625e-01\n",
      "  -3.00379336e-01  9.73264098e-01  1.03438325e-01 -1.72035769e-01\n",
      "   7.10892797e-01  5.07568240e-01]\n",
      " [ 5.36487877e-01 -1.30214930e+00  3.57769608e-01 -1.13841391e+00\n",
      "   4.67605025e-01 -1.36232793e+00  7.73141335e-04  8.18365812e-02\n",
      "  -1.15281892e+00 -7.34213054e-01]\n",
      " [ 2.92367697e-01 -1.62253571e+00  3.24002922e-01 -1.40412736e+00\n",
      "   6.04623795e-01 -1.50198543e+00  2.31020674e-01 -2.29745373e-01\n",
      "  -1.30925882e+00 -8.43219995e-01]\n",
      " [ 3.37384552e-01 -1.41808701e+00  3.21166903e-01 -1.14151680e+00\n",
      "   4.73209828e-01 -1.32600021e+00  4.13420945e-02 -1.01551406e-01\n",
      "  -1.11749446e+00 -7.71858454e-01]\n",
      " [ 5.87971248e-02  1.22963898e-02 -1.27798142e-02 -1.81840756e-03\n",
      "  -6.13636151e-03 -4.24157791e-02  4.16644327e-02  1.21275760e-01\n",
      "  -1.84575766e-02 -1.69632246e-03]\n",
      " [ 2.16444373e-01 -1.91051587e-01  8.73407125e-02 -1.61799341e-01\n",
      "   1.46935925e-01 -3.25376570e-01 -2.81692669e-02  1.12928294e-01\n",
      "  -2.44641870e-01 -2.46011987e-01]\n",
      " [ 2.55374372e-01 -6.69150949e-01  1.88013211e-01 -6.03184581e-01\n",
      "   2.13580757e-01 -7.01853693e-01  1.68935899e-02  1.12526819e-01\n",
      "  -6.09279037e-01 -3.70347202e-01]\n",
      " [ 1.16795130e-01 -3.54532808e-01  4.82460335e-02 -2.69613653e-01\n",
      "   1.58885166e-01 -3.51611525e-01 -1.59746721e-01 -2.28613783e-02\n",
      "  -2.79088348e-01 -2.61386693e-01]\n",
      " [ 1.02699049e-01 -2.82127440e-01  1.61076188e-01 -1.92535684e-01\n",
      "   6.90768706e-03 -3.04510474e-01  1.47069052e-01  1.96683958e-01\n",
      "  -1.76368490e-01 -1.53438240e-01]\n",
      " [ 1.64223015e-01 -1.38890341e-01  3.96524183e-02 -1.36438429e-01\n",
      "   5.06422073e-02 -2.64478803e-01 -1.06963590e-02  1.13738909e-01\n",
      "  -1.46225929e-01 -1.60338715e-01]\n",
      " [ 5.28409958e-01 -1.70356131e+00  4.85247612e-01 -1.54965234e+00\n",
      "   5.64318538e-01 -1.66835606e+00  9.24668089e-02  7.99455568e-02\n",
      "  -1.42230535e+00 -8.26181233e-01]\n",
      " [ 1.19577102e-01 -7.14189053e-01  1.06560543e-01 -5.78497469e-01\n",
      "   2.38628954e-01 -6.69443488e-01  1.78587157e-02 -1.32666091e-02\n",
      "  -5.70888162e-01 -3.78755271e-01]\n",
      " [ 8.02581608e-02 -9.20521438e-01  2.07051188e-01 -8.59351218e-01\n",
      "   3.94974858e-01 -8.08596075e-01  6.46555424e-02 -9.42950770e-02\n",
      "  -7.56522059e-01 -3.61277908e-01]\n",
      " [ 5.50777186e-03 -4.09934580e-01  8.22132900e-02 -2.94046432e-01\n",
      "   1.57198608e-01 -2.79295087e-01  1.25032797e-01 -1.41150683e-01\n",
      "  -3.48999918e-01 -1.84819147e-01]\n",
      " [ 1.60777643e-01 -7.61212111e-01  2.02820420e-01 -6.51083469e-01\n",
      "   2.39341646e-01 -6.67231202e-01  1.90067291e-01  3.04049626e-02\n",
      "  -6.53529644e-01 -3.71102959e-01]\n",
      " [-2.57859588e-01 -4.66621608e-01 -7.56717995e-02 -5.71240723e-01\n",
      "   3.06903899e-01 -2.72269607e-01 -7.85054266e-02 -3.55596244e-01\n",
      "  -4.80906636e-01 -1.69433042e-01]\n",
      " [ 3.92558306e-01 -7.68422246e-01  1.74510673e-01 -6.74098611e-01\n",
      "   2.39010155e-01 -8.45810950e-01 -7.94946924e-02  1.19075917e-01\n",
      "  -6.98487043e-01 -5.11189878e-01]\n",
      " [ 2.92459458e-01 -9.06564593e-01  1.93820357e-01 -7.18711197e-01\n",
      "   3.83967757e-01 -9.10016298e-01  1.12703115e-01  1.72308534e-02\n",
      "  -7.71043420e-01 -5.81719577e-01]\n",
      " [ 2.03218177e-01 -9.01799262e-01  2.57076621e-01 -7.30783999e-01\n",
      "   2.53517061e-01 -8.27611268e-01  2.01433122e-01  2.93618366e-02\n",
      "  -6.68404460e-01 -4.31273192e-01]\n",
      " [ 3.36881369e-01 -4.07912016e-01  1.79044664e-01 -3.34784448e-01\n",
      "   1.31572887e-01 -5.39921284e-01 -3.40953097e-02  2.70246387e-01\n",
      "  -3.68215978e-01 -3.33719730e-01]\n",
      " [ 2.72477835e-01 -7.51158416e-01  1.90357730e-01 -6.13414943e-01\n",
      "   2.02816322e-01 -8.04063976e-01  1.21440038e-01  2.06826776e-01\n",
      "  -6.35095954e-01 -3.55722338e-01]\n",
      " [ 2.51992136e-01 -8.01863253e-01  2.47287631e-01 -7.40034044e-01\n",
      "   2.95466453e-01 -7.35400438e-01  7.09523708e-02 -3.64259742e-02\n",
      "  -6.68442786e-01 -3.88834357e-01]\n",
      " [-3.60624433e-01  1.98399350e-01 -6.49780035e-02  2.44801655e-01\n",
      "  -2.14474499e-01  3.45398039e-01  6.98330700e-02 -5.01354225e-02\n",
      "   2.73819774e-01  2.75467932e-01]\n",
      " [ 2.23545089e-01 -8.28202248e-01  1.83319047e-01 -6.37345910e-01\n",
      "   3.89115363e-01 -8.35173011e-01  1.55129373e-01 -8.40858966e-02\n",
      "  -6.76884055e-01 -4.85096842e-01]\n",
      " [ 4.70393270e-01 -8.99974525e-01  2.73374975e-01 -7.51391411e-01\n",
      "   2.60296345e-01 -1.04086804e+00  6.99723437e-02  2.67634630e-01\n",
      "  -7.90840089e-01 -5.25599122e-01]\n",
      " [ 4.73106027e-01 -6.80598855e-01  2.52035409e-01 -5.33307552e-01\n",
      "   2.04535484e-01 -8.51309001e-01 -1.50252715e-01  2.24084333e-01\n",
      "  -6.18682742e-01 -4.47384775e-01]\n",
      " [ 7.87926197e-01 -1.57423782e+00  5.19777060e-01 -1.32921529e+00\n",
      "   5.38892508e-01 -1.77100682e+00 -1.36197265e-03  3.44525456e-01\n",
      "  -1.40180993e+00 -1.02540946e+00]\n",
      " [ 1.37485832e-01 -5.03167927e-01  1.71213105e-01 -4.43732858e-01\n",
      "   3.32554504e-02 -4.75521147e-01  6.86407611e-02  7.74510130e-02\n",
      "  -4.07066405e-01 -2.46393323e-01]\n",
      " [ 4.14128304e-01 -7.87943721e-01  2.66589999e-01 -6.36425376e-01\n",
      "   2.33637497e-01 -8.98753583e-01 -7.71231204e-02  1.63842335e-01\n",
      "  -6.75183594e-01 -5.12210369e-01]\n",
      " [ 3.35899480e-02 -6.79813087e-01  1.55464634e-01 -6.34474516e-01\n",
      "   3.28961104e-01 -6.83724165e-01  1.31090909e-01  4.68797684e-02\n",
      "  -5.39410949e-01 -2.49935552e-01]\n",
      " [ 1.41198948e-01 -1.00443089e+00  2.08433628e-01 -7.89745748e-01\n",
      "   2.56843835e-01 -8.08689833e-01  2.23474875e-01 -2.31330603e-01\n",
      "  -7.47174382e-01 -4.68678683e-01]\n",
      " [ 1.09338239e-01 -2.01019689e-01  1.14539914e-01 -2.22628489e-01\n",
      "   1.12929121e-02 -2.31487229e-01 -2.52945889e-02  2.96954308e-02\n",
      "  -2.07904950e-01 -9.46633145e-02]\n",
      " [-1.10400282e-01 -5.32874405e-01  8.78350362e-02 -4.55735385e-01\n",
      "   1.24847248e-01 -3.03684920e-01  2.16776237e-01 -1.63416535e-01\n",
      "  -3.94289702e-01 -1.73963264e-01]\n",
      " [ 2.26576418e-01 -1.25821340e+00  3.74488950e-01 -1.06333315e+00\n",
      "   5.89285433e-01 -1.14462781e+00  1.99447513e-01 -8.28802884e-02\n",
      "  -1.09322858e+00 -5.20805657e-01]\n",
      " [-3.54674369e-01  7.68176690e-02 -9.26438197e-02  5.93526922e-02\n",
      "  -5.68515547e-02  2.58081168e-01  1.26870885e-01 -2.10986927e-01\n",
      "   1.69293731e-01  1.81036413e-01]\n",
      " [ 4.55929577e-01 -7.34455645e-01  2.30134904e-01 -6.72616720e-01\n",
      "   3.26177716e-01 -8.88125956e-01 -6.18952960e-02  1.60442218e-01\n",
      "  -7.70585775e-01 -5.52737772e-01]\n",
      " [ 3.37678939e-01 -9.94146287e-01  1.83641613e-01 -8.29732418e-01\n",
      "   4.70240414e-01 -1.06122923e+00  3.39770690e-02  1.81353895e-03\n",
      "  -9.46139812e-01 -5.88020027e-01]\n",
      " [ 3.62167746e-01 -9.14404511e-01  2.52074838e-01 -7.36221313e-01\n",
      "   3.25798243e-01 -9.54689682e-01  7.17912987e-02  9.92881656e-02\n",
      "  -7.87337780e-01 -5.68751931e-01]\n",
      " [ 1.42171457e-01 -7.44219840e-01  2.44027391e-01 -5.99362612e-01\n",
      "   1.04974948e-01 -6.63028002e-01  1.38699204e-01  2.66964007e-02\n",
      "  -5.22030234e-01 -2.90697604e-01]\n",
      " [-3.60481776e-02 -4.72689092e-01 -4.15224321e-02 -3.01885903e-01\n",
      "   1.67750433e-01 -3.27095836e-01 -5.98594435e-02 -2.82942086e-01\n",
      "  -3.62816751e-01 -2.91097194e-01]\n",
      " [-9.82091054e-02 -2.79663175e-01 -4.72710421e-03 -2.52752930e-01\n",
      "   6.30433112e-02 -1.42089903e-01  1.66668862e-01 -3.71684879e-02\n",
      "  -2.01699972e-01 -9.89612490e-02]\n",
      " [ 3.14042121e-01 -1.04116547e+00  3.21984828e-01 -7.58289635e-01\n",
      "   2.60054678e-01 -9.74653602e-01  7.63294995e-02  7.16198795e-03\n",
      "  -7.83216894e-01 -5.55043936e-01]\n",
      " [ 3.60247135e-01 -8.59486043e-01  2.50589609e-01 -7.46384740e-01\n",
      "   2.90528834e-01 -8.36720943e-01  1.32796168e-01  1.52044609e-01\n",
      "  -7.61533260e-01 -5.06493151e-01]\n",
      " [ 2.83949580e-02 -2.55510211e-01 -4.41123284e-02 -2.27684826e-01\n",
      "   1.99600339e-01 -2.61499345e-01  1.20960236e-01  5.58368191e-02\n",
      "  -3.26461166e-01 -1.98275194e-01]]\n"
     ]
    }
   ],
   "source": [
    "#Test the model with the first batch\n",
    "for step, batch in enumerate(train_ds.as_numpy_iterator()):\n",
    "    batch1=batch\n",
    "    break\n",
    "a=model(batch[0])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "#scheduler = optax.piecewise_constant_schedule(init_value=learning_rate, boundaries_and_scales={int(train_steps*0.1):0.1})\n",
    "#optimizer = nnx.Optimizer(model, optax.adamw(scheduler, momentum,weight_decay=0.05))\n",
    "optimizer = nnx.Optimizer(model, optax.adamw(learning_rate, momentum,weight_decay=0.05))\n",
    "metrics = nnx.MultiMetric(\n",
    "  accuracy=nnx.metrics.Accuracy(),\n",
    "  loss=nnx.metrics.Average('loss'),\n",
    ")\n",
    "\n",
    "nnx.display(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "def loss_fn(model: MLP, batch):\n",
    "  logits = model(batch[0])\n",
    "  loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "    logits=logits, labels=batch[1]\n",
    "  ).mean()\n",
    "  #print(logits.shape)\n",
    "  #print(batch[1].shape)\n",
    "  return loss, logits\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(model: MLP, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
    "  \"\"\"Train for a single step.\"\"\"\n",
    "  grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
    "  (loss, logits), grads = grad_fn(model, batch)\n",
    "  metrics.update(loss=loss, logits=logits, labels=batch[1])  # In-place updates.\n",
    "  optimizer.update(grads)  # In-place updates.\n",
    "\n",
    "  #Print the predicted labels and the actual labels of the first five images from the batch\n",
    "  #predicted_labels = jnp.argmax(logits, axis=-1)\n",
    "  #actual_labels = batch[1]\n",
    "  #jax.debug.print(\"Predictions: {}\",predicted_labels[:5].astype(int))\n",
    "  #jax.debug.print(\"Actual Labels: {}\",actual_labels[:5].astype(int))\n",
    "  \n",
    "@nnx.jit\n",
    "def eval_step(model: MLP, metrics: nnx.MultiMetric, batch):\n",
    "  loss, logits = loss_fn(model, batch)\n",
    "  metrics.update(loss=loss, logits=logits, labels=batch[1])  # In-place updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model + evaluation with the test data\n",
    "metrics_history = {\n",
    "    'train_loss': [],\n",
    "    'train_accuracy': [],\n",
    "    'test_loss': [],\n",
    "    'test_accuracy': [],\n",
    "}\n",
    "\n",
    "for step, batch in enumerate(train_ds.as_numpy_iterator()):\n",
    "  # Run the optimization for one step and make a stateful update to the following:\n",
    "  # - The train state's model parameters\n",
    "  # - The optimizer state\n",
    "  # - The training loss and accuracy batch metrics\n",
    "  train_step(model, optimizer, metrics, batch)\n",
    "\n",
    "  if step > 0 and (step % eval_every == 0 or step == train_steps - 1):  # One training epoch has passed.\n",
    "    # Log the training metrics.\n",
    "    for metric, value in metrics.compute().items():  # Compute the metrics.\n",
    "      metrics_history[f'train_{metric}'].append(value)  # Record the metrics.\n",
    "    metrics.reset()  # Reset the metrics for the test set.\n",
    "\n",
    "    # Compute the metrics on the test set after each training epoch.\n",
    "    for test_batch in test_ds.as_numpy_iterator():\n",
    "      eval_step(model, metrics, test_batch)\n",
    "\n",
    "    # Log the test metrics.\n",
    "    for metric, value in metrics.compute().items():\n",
    "      metrics_history[f'test_{metric}'].append(value)\n",
    "    metrics.reset()  # Reset the metrics for the next training epoch.\n",
    "\n",
    "    print(\n",
    "      f\"[train] step: {step}, \"\n",
    "      f\"loss: {metrics_history['train_loss'][-1]}, \"\n",
    "      f\"accuracy: {metrics_history['train_accuracy'][-1] * 100}\"\n",
    "    )\n",
    "    print(\n",
    "      f\"[test] step: {step}, \"\n",
    "      f\"loss: {metrics_history['test_loss'][-1]}, \"\n",
    "      f\"accuracy: {metrics_history['test_accuracy'][-1] * 100}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the training results into csv file\n",
    "import pandas as pd\n",
    "\n",
    "if leave_data:\n",
    "    data=pd.DataFrame({\"step\":np.arange(eval_every,train_steps+eval_every,eval_every),\"train_loss\":metrics_history['train_loss'],\n",
    "                       \"test_loss\":metrics_history['test_loss'],\"train_accuracy\":metrics_history['train_accuracy'],\n",
    "                       \"test_accuracy\":metrics_history['test_accuracy']})\n",
    "\n",
    "    data.to_csv(method_name+\"_enc\"+str(encoded_size)+\"_nr\"+str(hidden_neuron)+\"_d\"+str(hidden_size)+\"_\"+dataset_name+\"_step\"+str(train_steps)+\"r_min_\"+str(r_min)+\"r_max\"+str(r_max)+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the loss \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.arange(eval_every,train_steps+eval_every,eval_every),metrics_history['train_loss'],label=\"train loss\")\n",
    "plt.plot(np.arange(eval_every,train_steps+eval_every,eval_every),metrics_history['test_loss'],label=\"test loss\")\n",
    "plt.title(\"Train loss of \"+dataset_name+\" dataset with \"+method_name+\n",
    "              \", \\nhidden dimension=\"+str(hidden_size)+\", number of neuron=\"+str(hidden_neuron))\n",
    "plt.xlabel(\"Training step\")\n",
    "plt.ylabel(\"Train loss (cross entropy)\")\n",
    "plt.legend()\n",
    "if leave_data:\n",
    "    plt.savefig(\"loss_\"+method_name+\"_\"+str(encoded_size)+\"_\"+str(hidden_neuron)+\"_\"+dataset_name+\"_step\"+str(train_steps)+\"r_min_\"+str(r_min)+\"r_max\"+str(r_max)+\".jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the accuracy\n",
    "plt.plot(np.arange(eval_every,train_steps+eval_every,eval_every),metrics_history['train_accuracy'],label=\"train\")\n",
    "plt.plot(np.arange(eval_every,train_steps+eval_every,eval_every),metrics_history['test_accuracy'],label=\"test\")\n",
    "plt.title(\"Accuracy of \"+dataset_name+\" dataset with \"+method_name+\", \\nhidden dimension=\"+\n",
    "          str(hidden_size)+\", number of neuron=\"+str(hidden_neuron))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "if leave_data:\n",
    "    plt.savefig(\"accuracy_\"+method_name+\"_\"+str(encoded_size)+\"_\"+str(hidden_neuron)+\"_\"+dataset_name+\"_step\"+str(train_steps)+\"r_min_\"+str(r_min)+\"r_max\"+str(r_max)+\".jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
