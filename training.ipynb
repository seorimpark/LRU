{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds  # TFDS to download MNIST.\n",
    "import tensorflow as tf  # TensorFlow / `tf.data` operations.\n",
    "from flax import nnx  # The Flax NNX API.\n",
    "from functools import partial\n",
    "import jax.numpy as jnp  # JAX NumPy\n",
    "import jax\n",
    "import optax\n",
    "from linearRNN import forward_h\n",
    "from linearRNN import forward\n",
    "from linearRNN import init_lru_parameters\n",
    "import numpy as np\n",
    "from flax import linen as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn=1 #rnn=0:transformation of the inputs with fixed RNN weights, rnn=1: adding the RNN module on the model to learn the weight matrices\n",
    "hidden_neuron=256\n",
    "hidden_size=2048\n",
    "learning_rate = 0.005\n",
    "momentum = 0.9\n",
    "train_steps=2500\n",
    "eval_every = 100\n",
    "batch_size=32\n",
    "method_name=\"LRUMLP\"\n",
    "dataset_name=\"MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "#Import data\n",
    "cifar=tf.keras.datasets.mnist.load_data()\n",
    "train=cifar[0]\n",
    "test=cifar[1]\n",
    "\n",
    "train_x_len=train[0].shape[0]\n",
    "train_x_size=int(jnp.prod(jnp.array(train[0].shape[1:])))\n",
    "test_x_len=test[0].shape[0]\n",
    "test_x_size=int(jnp.prod(jnp.array(test[0].shape[1:])))\n",
    "\n",
    "train_x=train[0].reshape((train_x_len,train_x_size))/255\n",
    "train_y=train[1].reshape(train_x_len)\n",
    "\n",
    "\n",
    "test_x=test[0].reshape((test_x_len,test_x_size))/255\n",
    "test_y=test[1].reshape(test_x_len)\n",
    "\n",
    "#test_x=jnp.append(train_x[30000:],test_x,axis=0)\n",
    "#test_y=jnp.append(train_y[30000:],test_y)\n",
    "#\n",
    "#train_x=train_x[:30000]\n",
    "#train_y=train_y[:30000]\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rnn:\n",
    "    train_hidden_state=train_x\n",
    "    test_hidden_state=test_x\n",
    "else:\n",
    "    param=init_lru_parameters(hidden_size,train_x_size,r_min=0.999,r_max=0.9999,max_phase=6.28)\n",
    "    param2=init_lru_parameters(hidden_size,train_x_size,r_min=0.999,r_max=0.9999,max_phase=6.28)\n",
    "    train_hidden_state=jnp.real(forward_h(param,train_x))\n",
    "    test_hidden_state=jnp.real(forward_h(param2,test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds=tf.data.Dataset.from_tensor_slices((jnp.real(train_hidden_state),jnp.array(train_y,dtype=int)))\n",
    "test_ds=tf.data.Dataset.from_tensor_slices((jnp.real(test_hidden_state),jnp.array(test_y,dtype=int)))\n",
    "\n",
    "train_ds = train_ds.repeat().shuffle(100)\n",
    "# Group into batches of `batch_size` and skip incomplete batches, prefetch the next sample to improve latency.\n",
    "train_ds = train_ds.batch(batch_size, drop_remainder=True).take(train_steps).prefetch(1)\n",
    "# Group into batches of `batch_size` and skip incomplete batches, prefetch the next sample to improve latency.\n",
    "test_ds = test_ds.batch(batch_size, drop_remainder=True).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(train_hidden_state.shape)\n",
    "print(test_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  rnn=RNN(\n",
      "    cell=LSTMCell(\n",
      "      in_features=784,\n",
      "      hidden_features=2048,\n",
      "      gate_fn=<PjitFunction of <function sigmoid at 0x00000207EFD39080>>,\n",
      "      activation_fn=<PjitFunction of <function tanh at 0x00000207EEA91580>>,\n",
      "      kernel_init=<function variance_scaling.<locals>.init at 0x00000207F2BD2F20>,\n",
      "      recurrent_kernel_init=<function modified_orthogonal at 0x00000207F2C05120>,\n",
      "      bias_init=<function zeros at 0x00000207EFC658A0>,\n",
      "      dtype=None,\n",
      "      param_dtype=<class 'jax.numpy.float32'>,\n",
      "      carry_init=<function zeros at 0x00000207EFC658A0>,\n",
      "      rngs=Rngs(\n",
      "        default=RngStream(\n",
      "          key=RngKey(\n",
      "            value=Array((), dtype=key<fry>) overlaying:\n",
      "            [0 0],\n",
      "            tag='default'\n",
      "          ),\n",
      "          count=RngCount(\n",
      "            value=Array(12, dtype=uint32),\n",
      "            tag='default'\n",
      "          )\n",
      "        )\n",
      "      ),\n",
      "      ii=Linear(\n",
      "        kernel=Param(\n",
      "          value=Array(shape=(784, 2048), dtype=float32)\n",
      "        ),\n",
      "        bias=Param(\n",
      "          value=None\n",
      "        ),\n",
      "        in_features=784,\n",
      "        out_features=2048,\n",
      "        use_bias=False,\n",
      "        dtype=None,\n",
      "        param_dtype=<class 'jax.numpy.float32'>,\n",
      "        precision=None,\n",
      "        kernel_init=<function variance_scaling.<locals>.init at 0x00000207F2BD2F20>,\n",
      "        bias_init=<function zeros at 0x00000207EFC658A0>,\n",
      "        dot_general=<function dot_general at 0x00000207EE7D44A0>\n",
      "      ),\n",
      "      if_=Linear(\n",
      "        kernel=Param(\n",
      "          value=Array(shape=(784, 2048), dtype=float32)\n",
      "        ),\n",
      "        bias=Param(\n",
      "          value=None\n",
      "        ),\n",
      "        in_features=784,\n",
      "        out_features=2048,\n",
      "        use_bias=False,\n",
      "        dtype=None,\n",
      "        param_dtype=<class 'jax.numpy.float32'>,\n",
      "        precision=None,\n",
      "        kernel_init=<function variance_scaling.<locals>.init at 0x00000207F2BD2F20>,\n",
      "        bias_init=<function zeros at 0x00000207EFC658A0>,\n",
      "        dot_general=<function dot_general at 0x00000207EE7D44A0>\n",
      "      ),\n",
      "      ig=Linear(\n",
      "        kernel=Param(\n",
      "          value=Array(shape=(784, 2048), dtype=float32)\n",
      "        ),\n",
      "        bias=Param(\n",
      "          value=None\n",
      "        ),\n",
      "        in_features=784,\n",
      "        out_features=2048,\n",
      "        use_bias=False,\n",
      "        dtype=None,\n",
      "        param_dtype=<class 'jax.numpy.float32'>,\n",
      "        precision=None,\n",
      "        kernel_init=<function variance_scaling.<locals>.init at 0x00000207F2BD2F20>,\n",
      "        bias_init=<function zeros at 0x00000207EFC658A0>,\n",
      "        dot_general=<function dot_general at 0x00000207EE7D44A0>\n",
      "      ),\n",
      "      io=Linear(\n",
      "        kernel=Param(\n",
      "          value=Array(shape=(784, 2048), dtype=float32)\n",
      "        ),\n",
      "        bias=Param(\n",
      "          value=None\n",
      "        ),\n",
      "        in_features=784,\n",
      "        out_features=2048,\n",
      "        use_bias=False,\n",
      "        dtype=None,\n",
      "        param_dtype=<class 'jax.numpy.float32'>,\n",
      "        precision=None,\n",
      "        kernel_init=<function variance_scaling.<locals>.init at 0x00000207F2BD2F20>,\n",
      "        bias_init=<function zeros at 0x00000207EFC658A0>,\n",
      "        dot_general=<function dot_general at 0x00000207EE7D44A0>\n",
      "      ),\n",
      "      hi=Linear(\n",
      "        kernel=Param(\n",
      "          value=Array(shape=(2048, 2048), dtype=float32)\n",
      "        ),\n",
      "        bias=Param(\n",
      "          value=Array(shape=(2048,), dtype=float32)\n",
      "        ),\n",
      "        in_features=2048,\n",
      "        out_features=2048,\n",
      "        use_bias=True,\n",
      "        dtype=None,\n",
      "        param_dtype=<class 'jax.numpy.float32'>,\n",
      "        precision=None,\n",
      "        kernel_init=<function modified_orthogonal at 0x00000207F2C05120>,\n",
      "        bias_init=<function zeros at 0x00000207EFC658A0>,\n",
      "        dot_general=<function dot_general at 0x00000207EE7D44A0>\n",
      "      ),\n",
      "      hf=Linear(\n",
      "        kernel=Param(\n",
      "          value=Array(shape=(2048, 2048), dtype=float32)\n",
      "        ),\n",
      "        bias=Param(\n",
      "          value=Array(shape=(2048,), dtype=float32)\n",
      "        ),\n",
      "        in_features=2048,\n",
      "        out_features=2048,\n",
      "        use_bias=True,\n",
      "        dtype=None,\n",
      "        param_dtype=<class 'jax.numpy.float32'>,\n",
      "        precision=None,\n",
      "        kernel_init=<function modified_orthogonal at 0x00000207F2C05120>,\n",
      "        bias_init=<function zeros at 0x00000207EFC658A0>,\n",
      "        dot_general=<function dot_general at 0x00000207EE7D44A0>\n",
      "      ),\n",
      "      hg=Linear(\n",
      "        kernel=Param(\n",
      "          value=Array(shape=(2048, 2048), dtype=float32)\n",
      "        ),\n",
      "        bias=Param(\n",
      "          value=Array(shape=(2048,), dtype=float32)\n",
      "        ),\n",
      "        in_features=2048,\n",
      "        out_features=2048,\n",
      "        use_bias=True,\n",
      "        dtype=None,\n",
      "        param_dtype=<class 'jax.numpy.float32'>,\n",
      "        precision=None,\n",
      "        kernel_init=<function modified_orthogonal at 0x00000207F2C05120>,\n",
      "        bias_init=<function zeros at 0x00000207EFC658A0>,\n",
      "        dot_general=<function dot_general at 0x00000207EE7D44A0>\n",
      "      ),\n",
      "      ho=Linear(\n",
      "        kernel=Param(\n",
      "          value=Array(shape=(2048, 2048), dtype=float32)\n",
      "        ),\n",
      "        bias=Param(\n",
      "          value=Array(shape=(2048,), dtype=float32)\n",
      "        ),\n",
      "        in_features=2048,\n",
      "        out_features=2048,\n",
      "        use_bias=True,\n",
      "        dtype=None,\n",
      "        param_dtype=<class 'jax.numpy.float32'>,\n",
      "        precision=None,\n",
      "        kernel_init=<function modified_orthogonal at 0x00000207F2C05120>,\n",
      "        bias_init=<function zeros at 0x00000207EFC658A0>,\n",
      "        dot_general=<function dot_general at 0x00000207EE7D44A0>\n",
      "      )\n",
      "    ),\n",
      "    time_major=False,\n",
      "    return_carry=False,\n",
      "    reverse=False,\n",
      "    keep_order=False,\n",
      "    unroll=1,\n",
      "    rngs=Rngs(\n",
      "      default=RngStream(\n",
      "        key=RngKey(\n",
      "          value=Array((), dtype=key<fry>) overlaying:\n",
      "          [0 0],\n",
      "          tag='default'\n",
      "        ),\n",
      "        count=RngCount(\n",
      "          value=Array(0, dtype=uint32),\n",
      "          tag='default'\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  ),\n",
      "  linear=Linear(\n",
      "    kernel=Param(\n",
      "      value=Array(shape=(2048, 256), dtype=float32)\n",
      "    ),\n",
      "    bias=Param(\n",
      "      value=Array(shape=(256,), dtype=float32)\n",
      "    ),\n",
      "    in_features=2048,\n",
      "    out_features=256,\n",
      "    use_bias=True,\n",
      "    dtype=None,\n",
      "    param_dtype=<class 'jax.numpy.float32'>,\n",
      "    precision=None,\n",
      "    kernel_init=<function variance_scaling.<locals>.init at 0x00000207F2BD31A0>,\n",
      "    bias_init=<function zeros at 0x00000207EFC658A0>,\n",
      "    dot_general=<function dot_general at 0x00000207EE7D44A0>\n",
      "  ),\n",
      "  linear_out=Linear(\n",
      "    kernel=Param(\n",
      "      value=Array(shape=(256, 10), dtype=float32)\n",
      "    ),\n",
      "    bias=Param(\n",
      "      value=Array(shape=(10,), dtype=float32)\n",
      "    ),\n",
      "    in_features=256,\n",
      "    out_features=10,\n",
      "    use_bias=True,\n",
      "    dtype=None,\n",
      "    param_dtype=<class 'jax.numpy.float32'>,\n",
      "    precision=None,\n",
      "    kernel_init=<function variance_scaling.<locals>.init at 0x00000207F2BD31A0>,\n",
      "    bias_init=<function zeros at 0x00000207EFC658A0>,\n",
      "    dot_general=<function dot_general at 0x00000207EE7D44A0>\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from flax import nnx\n",
    "import optax\n",
    "from flax.nnx.nn.recurrent import LSTMCell,GRUCell\n",
    "from func import LRNNCell,LRUCell\n",
    "\n",
    "class MLP(nnx.Module):\n",
    "  def __init__(self, din, dmid, dout, rngs: nnx.Rngs):\n",
    "    self.rnn=nnx.RNN(LRUCell(in_features=train_x_size, hidden_features=din, rngs=nnx.Rngs(0)))\n",
    "    self.linear = nnx.Linear(din, dmid, rngs=rngs)\n",
    "    self.linear_out = nnx.Linear(dmid, dout, rngs=rngs)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    if rnn:\n",
    "      x=self.rnn(x)\n",
    "    x=self.linear(x)\n",
    "    x = nnx.softplus(x)\n",
    "    return self.linear_out(x)\n",
    "\n",
    "model = MLP(hidden_size, hidden_neuron, 10, rngs=nnx.Rngs(0))  # eager initialization\n",
    "\n",
    "nnx.display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer(\n",
      "  step=OptState(\n",
      "    value=Array(0, dtype=uint32)\n",
      "  ),\n",
      "  model=MLP(\n",
      "    rnn=RNN(\n",
      "      cell=LSTMCell(\n",
      "        in_features=784,\n",
      "        hidden_features=2048,\n",
      "        gate_fn=<PjitFunction of <function sigmoid at 0x00000207EFD39080>>,\n",
      "        activation_fn=<PjitFunction of <function tanh at 0x00000207EEA91580>>,\n",
      "        kernel_init=<function variance_scaling.<locals>.init at 0x00000207F2BD2F20>,\n",
      "        recurrent_kernel_init=<function modified_orthogonal at 0x00000207F2C05120>,\n",
      "        bias_init=<function zeros at 0x00000207EFC658A0>,\n",
      "        dtype=None,\n",
      "        param_dtype=<class 'jax.numpy.float32'>,\n",
      "        carry_init=<function zeros at 0x00000207EFC658A0>,\n",
      "        rngs=Rngs(\n",
      "          default=RngStream(\n",
      "            key=RngKey(\n",
      "              value=Array((), dtype=key<fry>) overlaying:\n",
      "              [0 0],\n",
      "              tag='default'\n",
      "            ),\n",
      "            count=RngCount(\n",
      "              value=Array(12, dtype=uint32),\n",
      "              tag='default'\n",
      "            )\n",
      "          )\n",
      "        ),\n",
      "        ii=Linear(\n",
      "          kernel=Param(\n",
      "            value=Array(shape=(784, 2048), dtype=float32)\n",
      "          ),\n",
      "          bias=Param(\n",
      "            value=None\n",
      "          ),\n",
      "          in_features=784,\n",
      "          out_features=2048,\n",
      "          use_bias=False,\n",
      "          dtype=None,\n",
      "          param_dtype=<class 'jax.numpy.float32'>,\n",
      "          precision=None,\n",
      "          kernel_init=<function variance_scaling.<locals>.init at 0x00000207F2BD2F20>,\n",
      "          bias_init=<function zeros at 0x00000207EFC658A0>,\n",
      "          dot_general=<function dot_general at 0x00000207EE7D44A0>\n",
      "        ),\n",
      "        if_=Linear(\n",
      "          kernel=Param(\n",
      "            value=Array(shape=(784, 2048), dtype=float32)\n",
      "          ),\n",
      "          bias=Param(\n",
      "            value=None\n",
      "          ),\n",
      "          in_features=784,\n",
      "          out_features=2048,\n",
      "          use_bias=False,\n",
      "          dtype=None,\n",
      "          param_dtype=<class 'jax.numpy.float32'>,\n",
      "          precision=None,\n",
      "          kernel_init=<function variance_scaling.<locals>.init at 0x00000207F2BD2F20>,\n",
      "          bias_init=<function zeros at 0x00000207EFC658A0>,\n",
      "          dot_general=<function dot_general at 0x00000207EE7D44A0>\n",
      "        ),\n",
      "        ig=Linear(\n",
      "          kernel=Param(\n",
      "            value=Array(shape=(784, 2048), dtype=float32)\n",
      "          ),\n",
      "          bias=Param(\n",
      "            value=None\n",
      "          ),\n",
      "          in_features=784,\n",
      "          out_features=2048,\n",
      "          use_bias=False,\n",
      "          dtype=None,\n",
      "          param_dtype=<class 'jax.numpy.float32'>,\n",
      "          precision=None,\n",
      "          kernel_init=<function variance_scaling.<locals>.init at 0x00000207F2BD2F20>,\n",
      "          bias_init=<function zeros at 0x00000207EFC658A0>,\n",
      "          dot_general=<function dot_general at 0x00000207EE7D44A0>\n",
      "        ),\n",
      "        io=Linear(\n",
      "          kernel=Param(\n",
      "            value=Array(shape=(784, 2048), dtype=float32)\n",
      "          ),\n",
      "          bias=Param(\n",
      "            value=None\n",
      "          ),\n",
      "          in_features=784,\n",
      "          out_features=2048,\n",
      "          use_bias=False,\n",
      "          dtype=None,\n",
      "          param_dtype=<class 'jax.numpy.float32'>,\n",
      "          precision=None,\n",
      "          kernel_init=<function variance_scaling.<locals>.init at 0x00000207F2BD2F20>,\n",
      "          bias_init=<function zeros at 0x00000207EFC658A0>,\n",
      "          dot_general=<function dot_general at 0x00000207EE7D44A0>\n",
      "        ),\n",
      "        hi=Linear(\n",
      "          kernel=Param(\n",
      "            value=Array(shape=(2048, 2048), dtype=float32)\n",
      "          ),\n",
      "          bias=Param(\n",
      "            value=Array(shape=(2048,), dtype=float32)\n",
      "          ),\n",
      "          in_features=2048,\n",
      "          out_features=2048,\n",
      "          use_bias=True,\n",
      "          dtype=None,\n",
      "          param_dtype=<class 'jax.numpy.float32'>,\n",
      "          precision=None,\n",
      "          kernel_init=<function modified_orthogonal at 0x00000207F2C05120>,\n",
      "          bias_init=<function zeros at 0x00000207EFC658A0>,\n",
      "          dot_general=<function dot_general at 0x00000207EE7D44A0>\n",
      "        ),\n",
      "        hf=Linear(\n",
      "          kernel=Param(\n",
      "            value=Array(shape=(2048, 2048), dtype=float32)\n",
      "          ),\n",
      "          bias=Param(\n",
      "            value=Array(shape=(2048,), dtype=float32)\n",
      "          ),\n",
      "          in_features=2048,\n",
      "          out_features=2048,\n",
      "          use_bias=True,\n",
      "          dtype=None,\n",
      "          param_dtype=<class 'jax.numpy.float32'>,\n",
      "          precision=None,\n",
      "          kernel_init=<function modified_orthogonal at 0x00000207F2C05120>,\n",
      "          bias_init=<function zeros at 0x00000207EFC658A0>,\n",
      "          dot_general=<function dot_general at 0x00000207EE7D44A0>\n",
      "        ),\n",
      "        hg=Linear(\n",
      "          kernel=Param(\n",
      "            value=Array(shape=(2048, 2048), dtype=float32)\n",
      "          ),\n",
      "          bias=Param(\n",
      "            value=Array(shape=(2048,), dtype=float32)\n",
      "          ),\n",
      "          in_features=2048,\n",
      "          out_features=2048,\n",
      "          use_bias=True,\n",
      "          dtype=None,\n",
      "          param_dtype=<class 'jax.numpy.float32'>,\n",
      "          precision=None,\n",
      "          kernel_init=<function modified_orthogonal at 0x00000207F2C05120>,\n",
      "          bias_init=<function zeros at 0x00000207EFC658A0>,\n",
      "          dot_general=<function dot_general at 0x00000207EE7D44A0>\n",
      "        ),\n",
      "        ho=Linear(\n",
      "          kernel=Param(\n",
      "            value=Array(shape=(2048, 2048), dtype=float32)\n",
      "          ),\n",
      "          bias=Param(\n",
      "            value=Array(shape=(2048,), dtype=float32)\n",
      "          ),\n",
      "          in_features=2048,\n",
      "          out_features=2048,\n",
      "          use_bias=True,\n",
      "          dtype=None,\n",
      "          param_dtype=<class 'jax.numpy.float32'>,\n",
      "          precision=None,\n",
      "          kernel_init=<function modified_orthogonal at 0x00000207F2C05120>,\n",
      "          bias_init=<function zeros at 0x00000207EFC658A0>,\n",
      "          dot_general=<function dot_general at 0x00000207EE7D44A0>\n",
      "        )\n",
      "      ),\n",
      "      time_major=False,\n",
      "      return_carry=False,\n",
      "      reverse=False,\n",
      "      keep_order=False,\n",
      "      unroll=1,\n",
      "      rngs=Rngs(\n",
      "        default=RngStream(\n",
      "          key=RngKey(\n",
      "            value=Array((), dtype=key<fry>) overlaying:\n",
      "            [0 0],\n",
      "            tag='default'\n",
      "          ),\n",
      "          count=RngCount(\n",
      "            value=Array(0, dtype=uint32),\n",
      "            tag='default'\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    ),\n",
      "    linear=Linear(\n",
      "      kernel=Param(\n",
      "        value=Array(shape=(2048, 256), dtype=float32)\n",
      "      ),\n",
      "      bias=Param(\n",
      "        value=Array(shape=(256,), dtype=float32)\n",
      "      ),\n",
      "      in_features=2048,\n",
      "      out_features=256,\n",
      "      use_bias=True,\n",
      "      dtype=None,\n",
      "      param_dtype=<class 'jax.numpy.float32'>,\n",
      "      precision=None,\n",
      "      kernel_init=<function variance_scaling.<locals>.init at 0x00000207F2BD31A0>,\n",
      "      bias_init=<function zeros at 0x00000207EFC658A0>,\n",
      "      dot_general=<function dot_general at 0x00000207EE7D44A0>\n",
      "    ),\n",
      "    linear_out=Linear(\n",
      "      kernel=Param(\n",
      "        value=Array(shape=(256, 10), dtype=float32)\n",
      "      ),\n",
      "      bias=Param(\n",
      "        value=Array(shape=(10,), dtype=float32)\n",
      "      ),\n",
      "      in_features=256,\n",
      "      out_features=10,\n",
      "      use_bias=True,\n",
      "      dtype=None,\n",
      "      param_dtype=<class 'jax.numpy.float32'>,\n",
      "      precision=None,\n",
      "      kernel_init=<function variance_scaling.<locals>.init at 0x00000207F2BD31A0>,\n",
      "      bias_init=<function zeros at 0x00000207EFC658A0>,\n",
      "      dot_general=<function dot_general at 0x00000207EE7D44A0>\n",
      "    )\n",
      "  ),\n",
      "  tx=GradientTransformationExtraArgs(init=<function chain.<locals>.init_fn at 0x00000207FDA0ECA0>, update=<function chain.<locals>.update_fn at 0x00000207FDA0ED40>),\n",
      "  opt_state=(ScaleByAdamState(count=OptArray(\n",
      "    value=Array(0, dtype=int32)\n",
      "  ), mu=State({\n",
      "    'linear': {\n",
      "      'bias': OptVariable(\n",
      "        source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "        value=Array(shape=(256,), dtype=float32)\n",
      "      ),\n",
      "      'kernel': OptVariable(\n",
      "        source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "        value=Array(shape=(2048, 256), dtype=float32)\n",
      "      )\n",
      "    },\n",
      "    'linear_out': {\n",
      "      'bias': OptVariable(\n",
      "        source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "        value=Array(shape=(10,), dtype=float32)\n",
      "      ),\n",
      "      'kernel': OptVariable(\n",
      "        source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "        value=Array(shape=(256, 10), dtype=float32)\n",
      "      )\n",
      "    },\n",
      "    'rnn': {\n",
      "      'cell': {\n",
      "        'hf': {\n",
      "          'bias': OptVariable(\n",
      "            source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "            value=Array(shape=(2048,), dtype=float32)\n",
      "          ),\n",
      "          'kernel': OptVariable(\n",
      "            source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "            value=Array(shape=(2048, 2048), dtype=float32)\n",
      "          )\n",
      "        },\n",
      "        'hg': {\n",
      "          'bias': OptVariable(\n",
      "            source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "            value=Array(shape=(2048,), dtype=float32)\n",
      "          ),\n",
      "          'kernel': OptVariable(\n",
      "            source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "            value=Array(shape=(2048, 2048), dtype=float32)\n",
      "          )\n",
      "        },\n",
      "        'hi': {\n",
      "          'bias': OptVariable(\n",
      "            source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "            value=Array(shape=(2048,), dtype=float32)\n",
      "          ),\n",
      "          'kernel': OptVariable(\n",
      "            source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "            value=Array(shape=(2048, 2048), dtype=float32)\n",
      "          )\n",
      "        },\n",
      "        'ho': {\n",
      "          'bias': OptVariable(\n",
      "            source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "            value=Array(shape=(2048,), dtype=float32)\n",
      "          ),\n",
      "          'kernel': OptVariable(\n",
      "            source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "            value=Array(shape=(2048, 2048), dtype=float32)\n",
      "          )\n",
      "        },\n",
      "        'if_': {\n",
      "          'bias': OptVariable(\n",
      "            source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "            value=None\n",
      "          ),\n",
      "          'kernel': OptVariable(\n",
      "            source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "            value=Array(shape=(784, 2048), dtype=float32)\n",
      "          )\n",
      "        },\n",
      "        'ig': {\n",
      "          'bias': OptVariable(\n",
      "            source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "            value=None\n",
      "          ),\n",
      "          'kernel': OptVariable(\n",
      "            source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "            value=Array(shape=(784, 2048), dtype=float32)\n",
      "          )\n",
      "        },\n",
      "        'ii': {\n",
      "          'bias': OptVariable(\n",
      "            source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "            value=None\n",
      "          ),\n",
      "          'kernel': OptVariable(\n",
      "            source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "            value=Array(shape=(784, 2048), dtype=float32)\n",
      "          )\n",
      "        },\n",
      "        'io': {\n",
      "          'bias': OptVariable(\n",
      "            source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "            value=None\n",
      "          ),\n",
      "          'kernel': OptVariable(\n",
      "            source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "            value=Array(shape=(784, 2048), dtype=float32)\n",
      "          )\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }), nu=State({\n",
      "    'linear': {\n",
      "      'bias': OptVariable(\n",
      "        source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "        value=Array(shape=(256,), dtype=float32)\n",
      "      ),\n",
      "      'kernel': OptVariable(\n",
      "        source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "        value=Array(shape=(2048, 256), dtype=float32)\n",
      "      )\n",
      "    },\n",
      "    'linear_out': {\n",
      "      'bias': OptVariable(\n",
      "        source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "        value=Array(shape=(10,), dtype=float32)\n",
      "      ),\n",
      "      'kernel': OptVariable(\n",
      "        source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "        value=Array(shape=(256, 10), dtype=float32)\n",
      "      )\n",
      "    },\n",
      "    'rnn': {\n",
      "      'cell': {\n",
      "        'hf': {\n",
      "          'bias': OptVariable(\n",
      "            source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "            value=Array(shape=(2048,), dtype=float32)\n",
      "          ),\n",
      "          'kernel': OptVariable(\n",
      "            source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "            value=Array(shape=(2048, 2048), dtype=float32)\n",
      "          )\n",
      "        },\n",
      "        'hg': {\n",
      "          'bias': OptVariable(\n",
      "            source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "            value=Array(shape=(2048,), dtype=float32)\n",
      "          ),\n",
      "          'kernel': OptVariable(\n",
      "            source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "            value=Array(shape=(2048, 2048), dtype=float32)\n",
      "          )\n",
      "        },\n",
      "        'hi': {\n",
      "          'bias': OptVariable(\n",
      "            source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "            value=Array(shape=(2048,), dtype=float32)\n",
      "          ),\n",
      "          'kernel': OptVariable(\n",
      "            source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "            value=Array(shape=(2048, 2048), dtype=float32)\n",
      "          )\n",
      "        },\n",
      "        'ho': {\n",
      "          'bias': OptVariable(\n",
      "            source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "            value=Array(shape=(2048,), dtype=float32)\n",
      "          ),\n",
      "          'kernel': OptVariable(\n",
      "            source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "            value=Array(shape=(2048, 2048), dtype=float32)\n",
      "          )\n",
      "        },\n",
      "        'if_': {\n",
      "          'bias': OptVariable(\n",
      "            source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "            value=None\n",
      "          ),\n",
      "          'kernel': OptVariable(\n",
      "            source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "            value=Array(shape=(784, 2048), dtype=float32)\n",
      "          )\n",
      "        },\n",
      "        'ig': {\n",
      "          'bias': OptVariable(\n",
      "            source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "            value=None\n",
      "          ),\n",
      "          'kernel': OptVariable(\n",
      "            source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "            value=Array(shape=(784, 2048), dtype=float32)\n",
      "          )\n",
      "        },\n",
      "        'ii': {\n",
      "          'bias': OptVariable(\n",
      "            source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "            value=None\n",
      "          ),\n",
      "          'kernel': OptVariable(\n",
      "            source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "            value=Array(shape=(784, 2048), dtype=float32)\n",
      "          )\n",
      "        },\n",
      "        'io': {\n",
      "          'bias': OptVariable(\n",
      "            source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "            value=None\n",
      "          ),\n",
      "          'kernel': OptVariable(\n",
      "            source_type=<class 'flax.nnx.variablelib.Param'>,\n",
      "            value=Array(shape=(784, 2048), dtype=float32)\n",
      "          )\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  })), EmptyState(), EmptyState()),\n",
      "  wrt=<class 'flax.nnx.variablelib.Param'>\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import optax\n",
    "\n",
    "optimizer = nnx.Optimizer(model, optax.adamw(learning_rate, momentum))\n",
    "metrics = nnx.MultiMetric(\n",
    "  accuracy=nnx.metrics.Accuracy(),\n",
    "  loss=nnx.metrics.Average('loss'),\n",
    ")\n",
    "\n",
    "nnx.display(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "def loss_fn(model: MLP, batch):\n",
    "  logits = model(batch[0])\n",
    "  loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "    logits=logits, labels=batch[1]\n",
    "  ).mean()\n",
    "  #print(logits.shape)\n",
    "  #print(batch[1].shape)\n",
    "  return loss, logits\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(model: MLP, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
    "  \"\"\"Train for a single step.\"\"\"\n",
    "  grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
    "  (loss, logits), grads = grad_fn(model, batch)\n",
    "  metrics.update(loss=loss, logits=logits, labels=batch[1])  # In-place updates.\n",
    "  optimizer.update(grads)  # In-place updates.\n",
    "  predicted_labels = jnp.argmax(logits, axis=-1)\n",
    "  actual_labels = batch[1]\n",
    "  #jax.debug.print(\"Predictions: {}\",predicted_labels[:5].astype(int))\n",
    "  #jax.debug.print(\"Actual Labels: {}\",actual_labels[:5].astype(int))\n",
    "  \n",
    "@nnx.jit\n",
    "def eval_step(model: MLP, metrics: nnx.MultiMetric, batch):\n",
    "  loss, logits = loss_fn(model, batch)\n",
    "  metrics.update(loss=loss, logits=logits, labels=batch[1])  # In-place updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] step: 100, loss: 1.1737847328186035, accuracy: 66.58415985107422\n",
      "[test] step: 100, loss: 0.4096709191799164, accuracy: 87.23957824707031\n",
      "[train] step: 200, loss: 0.36811864376068115, accuracy: 88.875\n",
      "[test] step: 200, loss: 0.3984302580356598, accuracy: 87.67027282714844\n",
      "[train] step: 300, loss: 0.3494749069213867, accuracy: 89.03125\n",
      "[test] step: 300, loss: 0.32557395100593567, accuracy: 89.71354675292969\n",
      "[train] step: 400, loss: 0.2804947793483734, accuracy: 91.90625\n",
      "[test] step: 400, loss: 0.32973578572273254, accuracy: 88.80207824707031\n",
      "[train] step: 500, loss: 0.27946534752845764, accuracy: 91.125\n",
      "[test] step: 500, loss: 0.21903975307941437, accuracy: 93.03886413574219\n",
      "[train] step: 600, loss: 0.20861873030662537, accuracy: 93.5625\n",
      "[test] step: 600, loss: 0.20950168371200562, accuracy: 93.359375\n"
     ]
    }
   ],
   "source": [
    "#Train the model + evaluation with the test data\n",
    "metrics_history = {\n",
    "    'train_loss': [],\n",
    "    'train_accuracy': [],\n",
    "    'test_loss': [],\n",
    "    'test_accuracy': [],\n",
    "}\n",
    "\n",
    "for step, batch in enumerate(train_ds.as_numpy_iterator()):\n",
    "  # Run the optimization for one step and make a stateful update to the following:\n",
    "  # - The train state's model parameters\n",
    "  # - The optimizer state\n",
    "  # - The training loss and accuracy batch metrics\n",
    "  train_step(model, optimizer, metrics, batch)\n",
    "\n",
    "  if step > 0 and (step % eval_every == 0 or step == train_steps - 1):  # One training epoch has passed.\n",
    "    # Log the training metrics.\n",
    "    for metric, value in metrics.compute().items():  # Compute the metrics.\n",
    "      metrics_history[f'train_{metric}'].append(value)  # Record the metrics.\n",
    "    metrics.reset()  # Reset the metrics for the test set.\n",
    "\n",
    "    # Compute the metrics on the test set after each training epoch.\n",
    "    for test_batch in test_ds.as_numpy_iterator():\n",
    "      eval_step(model, metrics, test_batch)\n",
    "\n",
    "    # Log the test metrics.\n",
    "    for metric, value in metrics.compute().items():\n",
    "      metrics_history[f'test_{metric}'].append(value)\n",
    "    metrics.reset()  # Reset the metrics for the next training epoch.\n",
    "\n",
    "    print(\n",
    "      f\"[train] step: {step}, \"\n",
    "      f\"loss: {metrics_history['train_loss'][-1]}, \"\n",
    "      f\"accuracy: {metrics_history['train_accuracy'][-1] * 100}\"\n",
    "    )\n",
    "    print(\n",
    "      f\"[test] step: {step}, \"\n",
    "      f\"loss: {metrics_history['test_loss'][-1]}, \"\n",
    "      f\"accuracy: {metrics_history['test_accuracy'][-1] * 100}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the training results into csv\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "data=pd.DataFrame({\"step\":np.arange(eval_every,train_steps+eval_every,eval_every),\"train_loss\":metrics_history['train_loss'],\n",
    "                   \"test_loss\":metrics_history['test_loss'],\"train_accuracy\":metrics_history['train_accuracy'],\n",
    "                   \"test_accuracy\":metrics_history['test_accuracy']})\n",
    "data.to_csv(method_name+\"_nr\"+str(hidden_neuron)+\"_d\"+str(hidden_size)+\"_\"+dataset_name+\"_step\"+str(train_steps)+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.arange(eval_every,train_steps+eval_every,eval_every),metrics_history['train_loss'],label=\"train loss\")\n",
    "plt.plot(np.arange(eval_every,train_steps+eval_every,eval_every),metrics_history['test_loss'],label=\"test loss\")\n",
    "plt.title(\"Train loss of MNIST dataset with GRU+MLP, \\nhidden dimension=\"+str(hidden_size)+\", number of neuron=\"+str(hidden_neuron))\n",
    "plt.xlabel(\"Training step\")\n",
    "plt.ylabel(\"Train loss (cross entropy)\")\n",
    "plt.legend()\n",
    "plt.savefig(\"loss_\"+method_name+\"_\"+str(hidden_neuron)+\"_\"+dataset_name+\"_step\"+str(train_steps)+\".jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the accuracy\n",
    "plt.plot(np.arange(eval_every,train_steps+eval_every,eval_every),metrics_history['train_accuracy'],label=\"train\")\n",
    "plt.plot(np.arange(eval_every,train_steps+eval_every,eval_every),metrics_history['test_accuracy'],label=\"test\")\n",
    "plt.title(\"Accuracy of MNIST dataset with LRUfix+MLP, \\nhidden dimension=\"+str(hidden_size)+\", number of neuron=\"+str(hidden_neuron))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.savefig(\"accuracy_\"+method_name+\"_\"+str(hidden_neuron)+\"_\"+dataset_name+\"_step\"+str(train_steps)+\".jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
