{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds  # TFDS to download MNIST.\n",
    "import tensorflow as tf  # TensorFlow / `tf.data` operations.\n",
    "from flax import nnx  # The Flax NNX API.\n",
    "from functools import partial\n",
    "import jax.numpy as jnp  # JAX NumPy\n",
    "import jax\n",
    "import optax\n",
    "from linearRNN import forward_h\n",
    "from linearRNN import forward\n",
    "from linearRNN import init_lru_parameters\n",
    "from linearRNN import binary_operator_diag\n",
    "from linearRNN import LRU\n",
    "import numpy as np\n",
    "from flax import linen as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool=0 #pooling layer after MLP is taking the average over the numbers\n",
    "transformation=0 #transformation of the data from decimals between 0 and 255 to binary 8 bit numbers\n",
    "leave_data=1 #download csv data of the results\n",
    "hidden_neuron=128 #no details in the 2023 paper => 2024 paper fixed to 512\n",
    "encoded_size=256\n",
    "hidden_size=128\n",
    "learning_rate = 0.004\n",
    "momentum = 0.9\n",
    "train_steps=3000\n",
    "eval_every = 50\n",
    "batch_size=50\n",
    "r_min = 0\n",
    "r_max = 1\n",
    "max_phase = 6.28\n",
    "depth=1\n",
    "method_name=\"LRUMLP\"\n",
    "dataset_name=\"MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_bin_array(arr, m):#https://stackoverflow.com/questions/22227595/convert-integer-to-binary-array-with-suitable-padding\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "    arr: Numpy array of positive integers\n",
    "    m: Number of bits of each integer to retain\n",
    "\n",
    "    Returns a copy of arr with every element replaced with a bit vector.\n",
    "    Bits encoded as int8's.\n",
    "    \"\"\"\n",
    "    to_str_func = np.vectorize(lambda x: np.binary_repr(x).zfill(m))\n",
    "    strs = to_str_func(arr)\n",
    "    ret = np.zeros(list(arr.shape) + [m], dtype=np.int8)\n",
    "    for bit_ix in range(0, m):\n",
    "        fetch_bit_func = np.vectorize(lambda x: x[bit_ix] == '1')\n",
    "        ret[...,bit_ix] = fetch_bit_func(strs).astype(\"int8\")\n",
    "\n",
    "    return ret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import data\n",
    "\n",
    "if dataset_name==\"MNIST\":\n",
    "    dataset=tf.keras.datasets.mnist.load_data()\n",
    "    train=dataset[0]\n",
    "    test=dataset[1]\n",
    "\n",
    "    train_x_seq=train[0].shape[0]\n",
    "    train_x_len=int(jnp.prod(jnp.array(train[0].shape[1:])))\n",
    "    train_x_size=1\n",
    "    test_x_seq=test[0].shape[0]\n",
    "    test_x_len=int(jnp.prod(jnp.array(test[0].shape[1:])))\n",
    "    test_x_size=1\n",
    "    if transformation: #transform the information of the pixel to 8-bit binary numbers\n",
    "        train_x=train[0].reshape((train_x_seq,train_x_len,train_x_size))\n",
    "        train_x=vec_bin_array(train_x,8)\n",
    "        train_y=train[1].reshape(train_x_seq)\n",
    "        train_y_class=len(jnp.unique(train_y))\n",
    "        test_x=test[0].reshape((test_x_seq,test_x_len,test_x_size))\n",
    "        test_x=vec_bin_array(test_x,8)\n",
    "        test_y=test[1].reshape(test_x_seq)\n",
    "    else:\n",
    "        train_x=train[0].reshape((train_x_seq,train_x_len,train_x_size))/255\n",
    "        train_y=train[1].reshape(train_x_seq)\n",
    "        train_y_class=len(jnp.unique(train_y))\n",
    "        test_x=test[0].reshape((test_x_seq,test_x_len,test_x_size))/255\n",
    "        test_y=test[1].reshape(test_x_seq)\n",
    "\n",
    "if dataset_name==\"CIFAR10\":\n",
    "    dataset=tf.keras.datasets.cifar10.load_data()\n",
    "    train=dataset[0]\n",
    "    test=dataset[1]\n",
    "\n",
    "    train_x_seq=train[0].shape[0]\n",
    "    train_x_len=int(jnp.prod(jnp.array(train[0].shape[1:-1])))\n",
    "    train_x_size=int(jnp.prod(jnp.array(train[0].shape[-1])))\n",
    "\n",
    "    test_x_seq=test[0].shape[0]\n",
    "    test_x_len=int(jnp.prod(jnp.array(test[0].shape[1:-1])))\n",
    "    test_x_size=int(jnp.prod(jnp.array(train[0].shape[-1])))\n",
    "\n",
    "    if transformation:#transform the information of the pixel to 3*8-bit binary numbers\n",
    "        train_x=train[0].reshape((train_x_seq,train_x_len,train_x_size))\n",
    "        train_x=vec_bin_array(train_x,8)\n",
    "        train_y=train[1].reshape(train_x_seq)\n",
    "        train_y_class=len(jnp.unique(train_y))\n",
    "        test_x=test[0].reshape((test_x_seq,test_x_len,test_x_size))\n",
    "        test_x=vec_bin_array(test_x,8)\n",
    "        test_y=test[1].reshape(test_x_seq)\n",
    "    else:\n",
    "        train_x=train[0].reshape((train_x_seq,train_x_len,train_x_size))/255\n",
    "        train_y=train[1].reshape(train_x_seq)\n",
    "        train_y_class=len(jnp.unique(train_y))\n",
    "        test_x=test[0].reshape((test_x_seq,test_x_len,test_x_size))/255\n",
    "        test_y=test[1].reshape(test_x_seq)\n",
    "\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds=tf.data.Dataset.from_tensor_slices((jnp.real(train_x),jnp.array(train_y,dtype=int)))\n",
    "test_ds=tf.data.Dataset.from_tensor_slices((jnp.real(test_x),jnp.array(test_y,dtype=int)))\n",
    "\n",
    "train_ds = train_ds.repeat().shuffle(100)\n",
    "# Group into batches of `batch_size` and skip incomplete batches, prefetch the next sample to improve latency.\n",
    "train_ds = train_ds.batch(batch_size, drop_remainder=True).take(train_steps).prefetch(1)\n",
    "# Group into batches of `batch_size` and skip incomplete batches, prefetch the next sample to improve latency.\n",
    "test_ds = test_ds.batch(batch_size, drop_remainder=True).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import nnx\n",
    "import optax\n",
    "from flax.nnx.nn.recurrent import LSTMCell,GRUCell\n",
    "import copy\n",
    "import random\n",
    "class MLP(nnx.Module):\n",
    "  #DON'T FORGET TO CHANGE THE MODEL NAME BEFORE RUNNING\n",
    "  #According to the scheme of the paper (Figure 1), input_size=M, encoded_size=H,layer_dim=number of neurons in MLP, out_dim=number of classes\n",
    "  def __init__(self, token_size, token_len,encoded_dim,hidden_dim, layer_dim, out_dim, rngs: nnx.Rngs):\n",
    "\n",
    "    #linear encoder\n",
    "    self.lin_encoder = nnx.Linear(in_features=token_size, out_features=encoded_dim,rngs=rngs)\n",
    "    #self.lin_encoder=nnx.Param(jnp.array(np.random.rand(token_size,encoded_dim)))\n",
    "    #LRU+MLP block\n",
    "    self.rnn = LRU(in_features=encoded_dim, hidden_features=hidden_dim, r_min=r_min,r_max=r_max,max_phase=max_phase)\n",
    "    self.linear1 = nnx.Linear(in_features=encoded_dim, out_features=layer_dim, rngs=rngs)\n",
    "    self.linear2 = nnx.Linear(in_features=layer_dim,out_features=encoded_dim,rngs=rngs)\n",
    "\n",
    "    #Linear layers\n",
    "    if pool: #If pooling layer takes the average over the token sequence length \n",
    "      self.linear3=(lambda x: jnp.mean(x,axis=1))\n",
    "    else: #learn the parameters of the linear transformation\n",
    "      self.linear3= nnx.Linear(in_features=token_len,out_features=1,rngs=rngs)\n",
    "    self.linear4= nnx.Linear(in_features=encoded_dim,out_features=out_dim,rngs=rngs)\n",
    "    #self.weight = nnx.Param(jnp.array(np.random.rand(token_len,1)))\n",
    "    #self.bias = nnx.Param(jnp.array(np.random.rand(encoded_dim,1)))\n",
    "    #self.weight2 = nnx.Param(jnp.array(np.random.rand(out_dim,encoded_dim)))\n",
    "    #self.bias2 = nnx.Param(jnp.array(np.random.rand(out_dim,1)))\n",
    "    self.out_dim = out_dim\n",
    "\n",
    "    \n",
    "  @nnx.vmap(in_axes=(None,0)) \n",
    "  def __call__(self, x):\n",
    "    x = self.lin_encoder(x)\n",
    "    #x=x@self.lin_encoder\n",
    "    y = x.copy()\n",
    "    #LRU+MLP block\n",
    "    for i in range(depth):\n",
    "      x = self.rnn(x)\n",
    "      x = self.linear1(x)\n",
    "      x = nnx.softplus(x)\n",
    "      x = self.linear2(x)\n",
    "\n",
    "    x += y #Skip connection\n",
    "    #x = x.T@self.weight #+ self.bias #project from L*H to H*1\n",
    "    #x = self.weight2@x #+ self.bias2#project from H*1 to out_dim\n",
    "    x=self.linear3(x.T)\n",
    "    x=self.linear4(x.T)\n",
    "    return x.reshape(self.out_dim)\n",
    "\n",
    "\n",
    "model = MLP(train_x_size,train_x_len,encoded_size,hidden_size, hidden_neuron, train_y_class, rngs=nnx.Rngs(0))  # eager initialization\n",
    "\n",
    "nnx.display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the model with the first batch\n",
    "#for step, batch in enumerate(train_ds.as_numpy_iterator()):\n",
    "#    batch1=batch\n",
    "#    break\n",
    "#a=model(batch[0])\n",
    "#print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "#scheduler = optax.piecewise_constant_schedule(init_value=learning_rate, boundaries_and_scales={int(train_steps*0.1):0.1})\n",
    "#optimizer = nnx.Optimizer(model, optax.adamw(scheduler, momentum,weight_decay=0.05))\n",
    "optimizer = nnx.Optimizer(model, optax.adamw(learning_rate, momentum,weight_decay=0.05))\n",
    "metrics = nnx.MultiMetric(\n",
    "  accuracy=nnx.metrics.Accuracy(),\n",
    "  loss=nnx.metrics.Average('loss'),\n",
    ")\n",
    "\n",
    "nnx.display(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "def loss_fn(model: MLP, batch):\n",
    "  logits = model(batch[0])\n",
    "  loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "    logits=logits, labels=batch[1]\n",
    "  ).mean()\n",
    "  #print(logits.shape)\n",
    "  #print(batch[1].shape)\n",
    "  return loss, logits\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(model: MLP, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
    "  \"\"\"Train for a single step.\"\"\"\n",
    "  grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
    "  (loss, logits), grads = grad_fn(model, batch)\n",
    "  metrics.update(loss=loss, logits=logits, labels=batch[1])  # In-place updates.\n",
    "  optimizer.update(grads)  # In-place updates.\n",
    "\n",
    "  #Print the predicted labels and the actual labels of the first five images from the batch\n",
    "  #predicted_labels = jnp.argmax(logits, axis=-1)\n",
    "  #actual_labels = batch[1]\n",
    "  #jax.debug.print(\"Predictions: {}\",predicted_labels[:5].astype(int))\n",
    "  #jax.debug.print(\"Actual Labels: {}\",actual_labels[:5].astype(int))\n",
    "  \n",
    "@nnx.jit\n",
    "def eval_step(model: MLP, metrics: nnx.MultiMetric, batch):\n",
    "  loss, logits = loss_fn(model, batch)\n",
    "  metrics.update(loss=loss, logits=logits, labels=batch[1])  # In-place updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model + evaluation with the test data\n",
    "metrics_history = {\n",
    "    'train_loss': [],\n",
    "    'train_accuracy': [],\n",
    "    'test_loss': [],\n",
    "    'test_accuracy': [],\n",
    "}\n",
    "\n",
    "for step, batch in enumerate(train_ds.as_numpy_iterator()):\n",
    "  # Run the optimization for one step and make a stateful update to the following:\n",
    "  # - The train state's model parameters\n",
    "  # - The optimizer state\n",
    "  # - The training loss and accuracy batch metrics\n",
    "  train_step(model, optimizer, metrics, batch)\n",
    "\n",
    "  if step > 0 and (step % eval_every == 0 or step == train_steps - 1):  # One training epoch has passed.\n",
    "    # Log the training metrics.\n",
    "    for metric, value in metrics.compute().items():  # Compute the metrics.\n",
    "      metrics_history[f'train_{metric}'].append(value)  # Record the metrics.\n",
    "    metrics.reset()  # Reset the metrics for the test set.\n",
    "\n",
    "    # Compute the metrics on the test set after each training epoch.\n",
    "    for test_batch in test_ds.as_numpy_iterator():\n",
    "      eval_step(model, metrics, test_batch)\n",
    "\n",
    "    # Log the test metrics.\n",
    "    for metric, value in metrics.compute().items():\n",
    "      metrics_history[f'test_{metric}'].append(value)\n",
    "    metrics.reset()  # Reset the metrics for the next training epoch.\n",
    "\n",
    "    print(\n",
    "      f\"[train] step: {step}, \"\n",
    "      f\"loss: {metrics_history['train_loss'][-1]}, \"\n",
    "      f\"accuracy: {metrics_history['train_accuracy'][-1] * 100}\"\n",
    "    )\n",
    "    print(\n",
    "      f\"[test] step: {step}, \"\n",
    "      f\"loss: {metrics_history['test_loss'][-1]}, \"\n",
    "      f\"accuracy: {metrics_history['test_accuracy'][-1] * 100}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the training results into csv file\n",
    "import pandas as pd\n",
    "\n",
    "if leave_data:\n",
    "    data=pd.DataFrame({\"step\":np.arange(eval_every,train_steps+eval_every,eval_every),\"train_loss\":metrics_history['train_loss'],\n",
    "                       \"test_loss\":metrics_history['test_loss'],\"train_accuracy\":metrics_history['train_accuracy'],\n",
    "                       \"test_accuracy\":metrics_history['test_accuracy']})\n",
    "\n",
    "    data.to_csv(method_name+\"_enc\"+str(encoded_size)+\"_nr\"+str(hidden_neuron)+\"_d\"+str(hidden_size)+\"_\"+dataset_name+\"_step\"+str(train_steps)+\"r_min_\"+str(r_min)+\"r_max\"+str(r_max)+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the loss \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.arange(eval_every,train_steps+eval_every,eval_every),metrics_history['train_loss'],label=\"train loss\")\n",
    "plt.plot(np.arange(eval_every,train_steps+eval_every,eval_every),metrics_history['test_loss'],label=\"test loss\")\n",
    "plt.title(\"Train loss of \"+dataset_name+\" dataset with \"+method_name+\n",
    "              \", \\nhidden dimension=\"+str(hidden_size)+\", number of neuron=\"+str(hidden_neuron))\n",
    "plt.xlabel(\"Training step\")\n",
    "plt.ylabel(\"Train loss (cross entropy)\")\n",
    "plt.legend()\n",
    "if leave_data:\n",
    "    plt.savefig(\"loss_\"+method_name+\"_\"+str(encoded_size)+\"_\"+str(hidden_neuron)+\"_\"+dataset_name+\"_step\"+str(train_steps)+\"r_min_\"+str(r_min)+\"r_max\"+str(r_max)+\".jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the accuracy\n",
    "plt.plot(np.arange(eval_every,train_steps+eval_every,eval_every),metrics_history['train_accuracy'],label=\"train\")\n",
    "plt.plot(np.arange(eval_every,train_steps+eval_every,eval_every),metrics_history['test_accuracy'],label=\"test\")\n",
    "plt.title(\"Accuracy of \"+dataset_name+\" dataset with \"+method_name+\", \\nhidden dimension=\"+\n",
    "          str(hidden_size)+\", number of neuron=\"+str(hidden_neuron))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "if leave_data:\n",
    "    plt.savefig(\"accuracy_\"+method_name+\"_\"+str(encoded_size)+\"_\"+str(hidden_neuron)+\"_\"+dataset_name+\"_step\"+str(train_steps)+\"r_min_\"+str(r_min)+\"r_max\"+str(r_max)+\".jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
