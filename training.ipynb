{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds  # TFDS to download MNIST.\n",
    "import tensorflow as tf  # TensorFlow / `tf.data` operations.\n",
    "from flax import nnx  # The Flax NNX API.\n",
    "import flax\n",
    "from functools import partial\n",
    "import jax.numpy as jnp  # JAX NumPy\n",
    "import jax\n",
    "import optax\n",
    "from linearRNN import forward_h\n",
    "from linearRNN import forward\n",
    "from linearRNN import init_lru_parameters\n",
    "from linearRNN import binary_operator_diag\n",
    "from linearRNN import compute_lr_sigma\n",
    "from linearRNN import LRU\n",
    "import numpy as np\n",
    "import random\n",
    "from flax import linen as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = 0  # pooling layer after MLP is taking the average over the numbers\n",
    "transformation = 0  # transformation of the data from decimals between 0 and 255 to binary 8 bit numbers\n",
    "leave_data = 1  # download csv data of the results\n",
    "multi_opt = 0  # change the learning rate and the variance of the initialized weights per layer by using the multi-optimizer\n",
    "hidden_neuron = 64  # no details in the 2023 paper => 2024 paper fixed to 512\n",
    "encoded_size = 256\n",
    "hidden_size = 128\n",
    "learning_rate = 0.004\n",
    "momentum = 0.9\n",
    "train_steps = 3000\n",
    "eval_every = 50\n",
    "batch_size = 50\n",
    "r_min = 0.9\n",
    "r_max = 0.999\n",
    "max_phase = 6.28\n",
    "depth = 1\n",
    "lr_factor=0.25\n",
    "method_name = \"LRUMLP\"\n",
    "dataset_name = \"MNIST\"\n",
    "rngs1=nnx.Rngs(random.randint(0,10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_bin_array(arr, m):\n",
    "    # https://stackoverflow.com/questions/22227595/convert-integer-to-binary-array-with-suitable-padding\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    arr: Numpy array of positive integers\n",
    "    m: Number of bits of each integer to retain\n",
    "\n",
    "    Returns a copy of arr with every element replaced with a bit vector.\n",
    "    Bits encoded as int8's.\n",
    "    \"\"\"\n",
    "    to_str_func = np.vectorize(lambda x: np.binary_repr(x).zfill(m))\n",
    "    strs = to_str_func(arr)\n",
    "    ret = np.zeros(list(arr.shape) + [m], dtype=np.int8)\n",
    "    for bit_ix in range(0, m):\n",
    "        fetch_bit_func = np.vectorize(lambda x: x[bit_ix] == \"1\")\n",
    "        ret[..., bit_ix] = fetch_bit_func(strs).astype(\"int8\")\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784, 1)\n",
      "(60000,)\n",
      "(10000, 784, 1)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "\n",
    "if dataset_name == \"MNIST\":\n",
    "    dataset = tf.keras.datasets.mnist.load_data()\n",
    "    train = dataset[0]\n",
    "    test = dataset[1]\n",
    "\n",
    "    train_x_seq = train[0].shape[0]\n",
    "    train_x_len = int(jnp.prod(jnp.array(train[0].shape[1:])))\n",
    "    train_x_size = 1\n",
    "    test_x_seq = test[0].shape[0]\n",
    "    test_x_len = int(jnp.prod(jnp.array(test[0].shape[1:])))\n",
    "    test_x_size = 1\n",
    "    if transformation:  # transform the information of the pixel to 8-bit binary numbers\n",
    "        train_x = train[0].reshape((train_x_seq, train_x_len, train_x_size))\n",
    "        train_x = vec_bin_array(train_x, 8)\n",
    "        train_y = train[1].reshape(train_x_seq)\n",
    "        train_y_class = len(jnp.unique(train_y))\n",
    "        test_x = test[0].reshape((test_x_seq, test_x_len, test_x_size))\n",
    "        test_x = vec_bin_array(test_x, 8)\n",
    "        test_y = test[1].reshape(test_x_seq)\n",
    "    else:\n",
    "        train_x = train[0].reshape((train_x_seq, train_x_len, train_x_size)) / 255\n",
    "        train_y = train[1].reshape(train_x_seq)\n",
    "        train_y_class = len(jnp.unique(train_y))\n",
    "        test_x = test[0].reshape((test_x_seq, test_x_len, test_x_size)) / 255\n",
    "        test_y = test[1].reshape(test_x_seq)\n",
    "\n",
    "if dataset_name == \"CIFAR10\":\n",
    "    dataset = tf.keras.datasets.cifar10.load_data()\n",
    "    train = dataset[0]\n",
    "    test = dataset[1]\n",
    "\n",
    "    train_x_seq = train[0].shape[0]\n",
    "    train_x_len = int(jnp.prod(jnp.array(train[0].shape[1:-1])))\n",
    "    train_x_size = int(jnp.prod(jnp.array(train[0].shape[-1])))\n",
    "\n",
    "    test_x_seq = test[0].shape[0]\n",
    "    test_x_len = int(jnp.prod(jnp.array(test[0].shape[1:-1])))\n",
    "    test_x_size = int(jnp.prod(jnp.array(train[0].shape[-1])))\n",
    "\n",
    "    if (\n",
    "        transformation\n",
    "    ):  # transform the information of the pixel to 3*8-bit binary numbers\n",
    "        train_x = train[0].reshape((train_x_seq, train_x_len, train_x_size))\n",
    "        train_x = vec_bin_array(train_x, 8)\n",
    "        train_y = train[1].reshape(train_x_seq)\n",
    "        train_y_class = len(jnp.unique(train_y))\n",
    "        test_x = test[0].reshape((test_x_seq, test_x_len, test_x_size))\n",
    "        test_x = vec_bin_array(test_x, 8)\n",
    "        test_y = test[1].reshape(test_x_seq)\n",
    "    else:\n",
    "        train_x = train[0].reshape((train_x_seq, train_x_len, train_x_size)) / 255\n",
    "        train_y = train[1].reshape(train_x_seq)\n",
    "        train_y_class = len(jnp.unique(train_y))\n",
    "        test_x = test[0].reshape((test_x_seq, test_x_len, test_x_size)) / 255\n",
    "        test_y = test[1].reshape(test_x_seq)\n",
    "\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (jnp.real(train_x), jnp.array(train_y, dtype=int))\n",
    ")\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (jnp.real(test_x), jnp.array(test_y, dtype=int))\n",
    ")\n",
    "\n",
    "train_ds = train_ds.repeat().shuffle(100)\n",
    "# Group into batches of `batch_size` and skip incomplete batches, prefetch the next sample to improve latency.\n",
    "train_ds = train_ds.batch(batch_size, drop_remainder=True).take(train_steps).prefetch(1)\n",
    "# Group into batches of `batch_size` and skip incomplete batches, prefetch the next sample to improve latency.\n",
    "test_ds = test_ds.batch(batch_size, drop_remainder=True).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nnx.Module):\n",
    "    # DON'T FORGET TO CHANGE THE MODEL NAME BEFORE RUNNING\n",
    "    # According to the scheme of the paper (Figure 1), input_size=M, encoded_size=H,layer_dim=number of neurons in MLP, out_dim=number of classes\n",
    "    def __init__(\n",
    "        self,\n",
    "        token_size,\n",
    "        token_len,\n",
    "        encoded_dim,\n",
    "        hidden_dim,\n",
    "        layer_dim,\n",
    "        out_dim,\n",
    "        rngs: nnx.Rngs,\n",
    "    ):\n",
    "\n",
    "        # linear encoder\n",
    "        # lrE,sigmaE=compute_lr_sigma(\"input\",token_size,encoded_dim,0,1)\n",
    "        # self.lin_encoder = nnx.Linear(in_features=token_size, out_features=encoded_dim,rngs=rngs,kernel_init=sigmaE*jax.random.normal)\n",
    "        self.lin_encoder = nnx.Linear(\n",
    "            in_features=token_size, out_features=encoded_dim, rngs=rngs\n",
    "        )\n",
    "\n",
    "        # LRU+MLP block\n",
    "        # lrL1, sigmaL1= compute_lr_sigma(\"input\",encoded_dim,layer_dim,0,1)\n",
    "        # lrL2, sigmaL2=compute_lr_sigma(\"output\",0,layer_dim,encoded_dim,1)\n",
    "        # self.linear1 = nnx.Linear(in_features=encoded_dim, out_features=layer_dim, rngs=rngs,kernel_init=sigmaL1*jax.random.normal))\n",
    "        # self.linear2 = nnx.Linear(in_features=layer_dim//2,out_features=encoded_dim,rngs=rngs,kernel_init=sigmaL2*jax.random.normal))\n",
    "        self.rnn = LRU(\n",
    "            in_features=encoded_dim,\n",
    "            hidden_features=hidden_dim,\n",
    "            r_min=r_min,\n",
    "            r_max=r_max,\n",
    "            max_phase=max_phase,\n",
    "        )\n",
    "        self.linear1 = nnx.Linear(\n",
    "            in_features=encoded_dim, out_features=layer_dim, rngs=rngs\n",
    "        )\n",
    "        self.linear2 = nnx.Linear(\n",
    "            in_features=layer_dim // 2, out_features=encoded_dim, rngs=rngs\n",
    "        )\n",
    "        self.batchnorm = nnx.BatchNorm(\n",
    "            num_features=encoded_dim, rngs=rngs,use_running_average=True\n",
    "        )\n",
    "\n",
    "        # Linear layers\n",
    "        if pool:  # If pooling layer takes the average over the token sequence length\n",
    "            self.linear3 = lambda x: jnp.mean(x, axis=1)\n",
    "        else:  # learn the parameters of the linear transformation\n",
    "            self.linear3 = nnx.Linear(in_features=token_len, out_features=1, rngs=rngs)\n",
    "        self.linear4 = nnx.Linear(\n",
    "            in_features=encoded_dim, out_features=out_dim, rngs=rngs\n",
    "        )\n",
    "        self.out_dim = out_dim\n",
    "        self.token_len = token_len\n",
    "        self.dropout = nnx.Dropout(0.1, rngs=rngs)\n",
    "\n",
    "    @nnx.vmap(in_axes=(None, 0,None))\n",
    "    def __call__(self, x,training=True):\n",
    "        x = self.lin_encoder(x)\n",
    "        # x=x@self.lin_encoder\n",
    "        y = x.copy()\n",
    "\n",
    "        # LRU+MLP block\n",
    "        x = self.batchnorm(x)  # batch normalization\n",
    "        x = self.rnn(x)\n",
    "        x = self.linear1(x)\n",
    "        x = nnx.glu(x, axis=-1)\n",
    "        x = self.dropout(x,deterministic=not training)\n",
    "        x = self.linear2(x)\n",
    "        x += y  # Skip connection -> p.21 adding for each block\n",
    "\n",
    "        # x = x.T@self.weight #+ self.bias #project from L*H to H*1\n",
    "        # x = self.weight2@x #+ self.bias2#project from H*1 to out_dim\n",
    "        x = self.linear3(x.T)\n",
    "        x = self.linear4(x.T)\n",
    "        return x.reshape(self.out_dim)\n",
    "    \n",
    "\n",
    "\n",
    "model = MLP(\n",
    "    train_x_size,\n",
    "    train_x_len,\n",
    "    encoded_size,\n",
    "    hidden_size,\n",
    "    hidden_neuron,\n",
    "    train_y_class,\n",
    "    rngs=rngs1,\n",
    ")  # eager initialization\n",
    "\n",
    "#nnx.display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_tuples_to_nested_dict(params):\n",
    "    nested_dict = {}\n",
    "    for outer_key, inner_key in params:\n",
    "        if outer_key not in nested_dict:\n",
    "            nested_dict[outer_key] = {}\n",
    "        nested_dict[outer_key][inner_key] = inner_key\n",
    "    return nested_dict\n",
    "param = nnx.state(model,nnx.Param).flat_state()\n",
    "gr=group_tuples_to_nested_dict(list(param.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_lr=optax.warmup_cosine_decay_schedule(init_value=1e-7*lr_factor, peak_value=learning_rate*lr_factor, warmup_steps=train_steps//10, decay_steps=train_steps, end_value=1e-7*lr_factor)\n",
    "lin_lr=optax.warmup_cosine_decay_schedule(init_value=1e-7, peak_value=learning_rate, warmup_steps=train_steps//10, decay_steps=train_steps, end_value=1e-7)\n",
    "\n",
    "\n",
    "d={\"B_re\":optax.adamw(rnn_lr),\n",
    "   \"B_im\":optax.adamw(rnn_lr),\n",
    "    'C_im': optax.adamw(rnn_lr,weight_decay=0.05), \n",
    "    'C_re': optax.adamw(rnn_lr,weight_decay=0.05),\n",
    "    'D': optax.adamw(rnn_lr,weight_decay=0.05),\n",
    "    'gamma_log': optax.adamw(rnn_lr),\n",
    "    'nu_log': optax.adamw(rnn_lr),\n",
    "    'theta_log': optax.adamw(rnn_lr),\n",
    "     \"kernel\": optax.adamw(lin_lr,weight_decay=0.05),\n",
    "     \"bias\": optax.adamw(lin_lr,weight_decay=0.05),\n",
    "     \"scale\": optax.adamw(lin_lr,weight_decay=0.05),\n",
    "     }\n",
    "#d={\"rnn\":optax.adamw(rnn_lr,weight_decay=0.05),\n",
    "#     \"lin_encoder\": optax.adamw(lin_lr,weight_decay=0.05),\n",
    "#     \"linear1\": optax.adamw(lin_lr,weight_decay=0.05),\n",
    "#     \"linear2\": optax.adamw(lin_lr,weight_decay=0.05),\n",
    "#     \"linear3\": optax.adamw(lin_lr,weight_decay=0.05),\n",
    "#     \"linear4\": optax.adamw(lin_lr,weight_decay=0.05),\n",
    "#     \"batchnorm\":optax.adamw(lin_lr,weight_decay=0.05)\n",
    "#     }\n",
    "tx=optax.multi_transform(d,nnx.State(gr))\n",
    "\n",
    "optimizer = nnx.Optimizer(model, tx)\n",
    "metrics = nnx.MultiMetric(\n",
    "    accuracy=nnx.metrics.Accuracy(),\n",
    "    loss=nnx.metrics.Average(\"loss\"),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00417244 -0.5448938  -0.7880246  -0.718174   -0.5749979  -0.3858759\n",
      "   0.30355892  0.12561405 -0.18305002 -0.60288304]\n",
      " [-0.01683209  0.16001454  0.20303409  0.24338025  0.30292118  0.0843908\n",
      "  -0.10410761 -0.20256622 -0.10985458  0.16654444]\n",
      " [ 0.24903734 -0.15455863  0.12092126  0.19560708 -0.03951555  0.06586774\n",
      "   0.09457376  0.1877837   0.15991202  0.17515282]\n",
      " [-0.47500896 -0.38607943 -0.75190586 -0.9977317  -0.7056675  -0.47016376\n",
      "  -0.01603683  0.33833957  0.06632543 -0.8641846 ]\n",
      " [ 0.04374841  0.03909424  0.30815768  0.44992045  0.37836912  0.06050877\n",
      "   0.06190279  0.04574795 -0.01577151  0.2441518 ]\n",
      " [ 0.13167934  0.8129354   0.88661414  0.8310031   0.8715719   0.45467776\n",
      "  -0.2831421  -0.4010855  -0.11503629  0.7085773 ]\n",
      " [-0.27337107 -0.00634999 -0.06001839 -0.12553851 -0.05005673 -0.1200382\n",
      "  -0.15075767 -0.07320248  0.0214056  -0.14598255]\n",
      " [-0.1923623  -0.35674447 -0.33973745 -0.5039718  -0.2438131  -0.30314067\n",
      "  -0.07375639 -0.00553811 -0.0155467  -0.4035784 ]\n",
      " [ 0.03859833  0.0989415   0.18975443  0.3916663   0.28387386  0.1328374\n",
      "   0.11110488 -0.18599282 -0.14765678  0.19530696]\n",
      " [-0.35722703 -0.4229482  -0.8815038  -1.073051   -0.65850854 -0.4968103\n",
      "   0.05561747  0.20649034 -0.04582794 -0.8678227 ]\n",
      " [-0.0445532  -0.24924748 -0.2739406  -0.00275162 -0.04959059 -0.17213906\n",
      "   0.15965357 -0.1258246  -0.14664339 -0.20775867]\n",
      " [-0.12818459 -0.18743509 -0.2051814  -0.07405976 -0.17869855 -0.1333794\n",
      "   0.00705189  0.04749073 -0.03659559 -0.24069433]\n",
      " [ 0.62049603  0.31410888  0.71744764  1.3237567   0.5783206   0.6211101\n",
      "   0.22648856 -0.2790602  -0.15795135  0.83685195]\n",
      " [ 0.22468439 -0.12648426 -0.05203918  0.17679872 -0.02111725  0.08683424\n",
      "   0.07147783 -0.01247377 -0.15020625  0.06702644]\n",
      " [ 0.11080897 -0.17657137 -0.28491277 -0.01648985 -0.25494334  0.01138097\n",
      "   0.2627478   0.02171886 -0.08390812 -0.14284167]\n",
      " [-0.32381326 -0.71421534 -0.62470824 -0.926554   -0.74493927 -0.5507874\n",
      "   0.04793155  0.3556506   0.2298314  -0.796251  ]\n",
      " [-0.01506425 -0.46393162 -0.59570605 -0.4090675  -0.33001837 -0.2874372\n",
      "   0.19162317  0.12894002 -0.0457034  -0.44573605]\n",
      " [-0.22309452 -0.526799   -0.58679074 -0.589803   -0.39093435 -0.31598765\n",
      "  -0.05360962  0.03586859 -0.01053034 -0.5926624 ]\n",
      " [-0.08836556 -0.2013508  -0.16251205 -0.23993035 -0.14356053 -0.19157153\n",
      "  -0.0507186  -0.07424133 -0.02313167 -0.23570748]\n",
      " [-0.05156532 -0.4569735  -0.4601414  -0.3368267  -0.48902783 -0.30511892\n",
      "   0.29603812  0.25844148  0.01612437 -0.3852122 ]\n",
      " [ 0.14272338 -0.4775169  -0.30107346 -0.30443344 -0.31052342 -0.23410738\n",
      "   0.03552856  0.13976592  0.04455964 -0.26240063]\n",
      " [ 0.1672292  -0.4358271  -0.43477407 -0.09265432 -0.39878228 -0.07719292\n",
      "   0.46230885  0.15366356 -0.11754746 -0.26696107]\n",
      " [ 0.11344324 -0.20996833 -0.15978928 -0.02698601 -0.21987785 -0.0716766\n",
      "   0.13355096  0.06829424 -0.0699334  -0.16359133]\n",
      " [-0.13601257 -0.1002714  -0.02179197 -0.2709007  -0.05411694 -0.18470217\n",
      "  -0.03773799  0.18727632  0.13197976 -0.09971585]\n",
      " [-0.02827108 -0.1485841   0.07952094  0.20835677 -0.04956599 -0.05840376\n",
      "   0.17961067  0.21425445  0.15094547 -0.04500287]\n",
      " [ 0.1196131   0.86978704  0.9203944   1.2485855   1.1122854   0.5266196\n",
      "  -0.36196122 -0.44520858 -0.17126617  0.91378313]\n",
      " [ 0.04100969 -0.46418026 -0.2608694  -0.05935562 -0.19862466 -0.2001547\n",
      "   0.17580979  0.26782084  0.02123745 -0.21693082]\n",
      " [ 0.06807976  0.4164692   0.38038194  0.33079723  0.20474958  0.1818211\n",
      "  -0.08208727 -0.08285759  0.10010993  0.20476986]\n",
      " [ 0.3588865   0.04840628  0.14044568  0.55589175  0.1719676   0.19389625\n",
      "   0.27087882  0.10862304 -0.10486311  0.27512148]\n",
      " [ 0.24718608 -0.52305293 -0.2336296  -0.21876651 -0.46014494 -0.30531994\n",
      "   0.35028628  0.28508702  0.1537498  -0.24408536]\n",
      " [ 0.2748669   0.03088876  0.34559536  0.60286295  0.24993387  0.18731402\n",
      "   0.17001535 -0.08140427 -0.01571679  0.42518938]\n",
      " [-0.46899426 -0.1543877  -0.05357715 -0.48848662 -0.08203395 -0.34516954\n",
      "  -0.2687571   0.09776906  0.312964   -0.36558104]\n",
      " [-0.28461212 -0.13598579 -0.27631497 -0.5088373  -0.17641193 -0.22207372\n",
      "  -0.07494023 -0.02969964 -0.08948785 -0.30479562]\n",
      " [ 0.1366107   0.5445368   0.67170215  0.78380555  0.9076921   0.39155328\n",
      "  -0.14949413 -0.37831026 -0.20948917  0.69758916]\n",
      " [ 0.34230217  0.17179061  0.39975393  0.7147896   0.46294427  0.24384427\n",
      "   0.11140931 -0.05722065 -0.07363147  0.39529607]\n",
      " [-0.24672678  0.3817755   0.6008443   0.49460006  0.7394739   0.14799415\n",
      "  -0.3660969   0.08012915  0.1344215   0.4155027 ]\n",
      " [ 0.50979984 -0.27268386 -0.17731664  0.01005575 -0.23421599 -0.02632436\n",
      "   0.3701964   0.18478164 -0.1078405   0.01907635]\n",
      " [-0.29151598 -0.33296782 -0.30895546 -0.4795463  -0.3221068  -0.2628634\n",
      "  -0.12281642  0.11502202  0.09770076 -0.42860332]\n",
      " [ 0.2198382   0.0917453   0.30687532  0.50724757  0.42148086  0.16892193\n",
      "   0.05180692 -0.11143092 -0.04224622  0.3368882 ]\n",
      " [ 0.35874683 -0.23864608  0.03668012  0.07179642 -0.038522   -0.09042991\n",
      "   0.205059    0.11479815 -0.05427213  0.09059734]\n",
      " [ 0.17371865  0.15755324  0.23290162  0.44877982  0.0632401   0.14642058\n",
      "   0.1393256  -0.10459642 -0.02316044  0.18910687]\n",
      " [-0.06215592 -0.7934576  -0.685918   -0.6045646  -0.7104608  -0.48846933\n",
      "   0.31235301  0.10935559 -0.00210876 -0.66805863]\n",
      " [ 0.03784088  0.49972528  0.46544454  0.6177137   0.612381    0.21140364\n",
      "  -0.18438956 -0.3868541  -0.23614243  0.40139636]\n",
      " [-0.21027352 -1.039787   -1.1267823  -0.8503135  -0.96605957 -0.57119405\n",
      "   0.22648877  0.20634481 -0.05571046 -1.0061724 ]\n",
      " [ 0.47782028 -0.02240129  0.17996569  0.6780073   0.23852032  0.242165\n",
      "   0.33700192 -0.02319144 -0.10452356  0.3570363 ]\n",
      " [-0.20064467 -0.26315367 -0.25616792 -0.16575162 -0.13676885 -0.14729099\n",
      "  -0.00895215  0.08457928 -0.02613747 -0.22179018]\n",
      " [ 0.17063223  0.03722639  0.19726686  0.37110093  0.08987354  0.17057362\n",
      "   0.01918039  0.14882514  0.07112504  0.1416823 ]\n",
      " [ 0.179318   -0.09324035 -0.12457614  0.07013915 -0.22155097  0.05089801\n",
      "   0.24693719  0.05776792 -0.00549217 -0.02047876]\n",
      " [ 0.60264224 -0.11305462  0.11636201  0.73894566  0.0508784   0.24572143\n",
      "   0.4081785  -0.0986489  -0.19479707  0.38406235]\n",
      " [ 0.3266227  -0.19800435  0.15039629  0.3841712  -0.09288619 -0.00248969\n",
      "   0.35098165  0.18898183  0.13867973  0.10747418]]\n"
     ]
    }
   ],
   "source": [
    "# Test the model with the first batch\n",
    "for step, batch in enumerate(train_ds.as_numpy_iterator()):\n",
    "    batch1 = batch\n",
    "    break\n",
    "a = model(batch1[0],training=False)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "def loss_fn(model: MLP, batch,training):\n",
    "  logits = model(batch[0],training=training)\n",
    "  loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "    logits=logits, labels=batch[1]\n",
    "  ).mean()\n",
    "  #print(logits.shape)\n",
    "  #print(batch[1].shape)\n",
    "  return loss, logits\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(model: MLP, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
    "  \"\"\"Train for a single step.\"\"\"\n",
    "  grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
    "  (loss, logits), grads = grad_fn(model, batch,True)\n",
    "  metrics.update(loss=loss, logits=logits, labels=batch[1])  # In-place updates.\n",
    "  optimizer.update(grads)  # In-place updates.\n",
    "  #predicted_labels = jnp.argmax(logits, axis=-1)\n",
    "  #actual_labels = batch[1]\n",
    "  #jax.debug.print(\"Predictions: {}\",predicted_labels[:5].astype(int))\n",
    "  #jax.debug.print(\"Actual Labels: {}\",actual_labels[:5].astype(int))\n",
    "\n",
    "@nnx.jit\n",
    "def eval_step(model: MLP, metrics: nnx.MultiMetric, batch):\n",
    "  loss, logits = loss_fn(model, batch,False)\n",
    "  metrics.update(loss=loss, logits=logits, labels=batch[1])  # In-place updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\seori\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\lax\\lax.py:3373: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  x_bar = _convert_element_type(x_bar, x.aval.dtype, x.aval.weak_type)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 14\u001b[0m\n\u001b[0;32m      2\u001b[0m metrics_history \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[0;32m      7\u001b[0m }\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_ds\u001b[38;5;241m.\u001b[39mas_numpy_iterator()):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Run the optimization for one step and make a stateful update to the following:\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# - The train state's model parameters\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# - The optimizer state\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# - The training loss and accuracy batch metrics\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m     \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m         step \u001b[38;5;241m%\u001b[39m eval_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m step \u001b[38;5;241m==\u001b[39m train_steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     18\u001b[0m     ):  \u001b[38;5;66;03m# One training epoch has passed.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;66;03m# Log the training metrics.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m metric, value \u001b[38;5;129;01min\u001b[39;00m metrics\u001b[38;5;241m.\u001b[39mcompute()\u001b[38;5;241m.\u001b[39mitems():  \u001b[38;5;66;03m# Compute the metrics.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\seori\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\flax\\nnx\\graph.py:1041\u001b[0m, in \u001b[0;36mUpdateContextManager.__call__.<locals>.update_context_manager_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_context_manager_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1040\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m-> 1041\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\seori\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\flax\\nnx\\transforms\\compilation.py:345\u001b[0m, in \u001b[0;36mjit.<locals>.jit_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fun)\n\u001b[0;32m    336\u001b[0m \u001b[38;5;129m@graph\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_context(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjit\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjit_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    338\u001b[0m   pure_args, pure_kwargs \u001b[38;5;241m=\u001b[39m extract\u001b[38;5;241m.\u001b[39mto_tree(\n\u001b[0;32m    339\u001b[0m     (args, kwargs),\n\u001b[0;32m    340\u001b[0m     prefix\u001b[38;5;241m=\u001b[39m(in_shardings, kwarg_shardings),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    343\u001b[0m     ctxtag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjit\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    344\u001b[0m   )\n\u001b[1;32m--> 345\u001b[0m   pure_args_out, pure_kwargs_out, pure_out \u001b[38;5;241m=\u001b[39m \u001b[43mjitted_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpure_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpure_kwargs\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m   _args_out, _kwargs_out, out \u001b[38;5;241m=\u001b[39m extract\u001b[38;5;241m.\u001b[39mfrom_tree(\n\u001b[0;32m    349\u001b[0m     (pure_args_out, pure_kwargs_out, pure_out), ctxtag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjit\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    350\u001b[0m   )\n\u001b[0;32m    351\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\seori\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\tree_util.py:1084\u001b[0m, in \u001b[0;36mregister_static.<locals>.<lambda>\u001b[1;34m(obj, empty_iter_children)\u001b[0m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Registers `cls` as a pytree with no leaves.\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \n\u001b[0;32m   1055\u001b[0m \u001b[38;5;124;03mInstances are treated as static by :func:`jax.jit`, :func:`jax.pmap`, etc. This can\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;124;03m  Array(3, dtype=int32, weak_type=True)\u001b[39;00m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m flatten \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m obj: ((), obj)\n\u001b[1;32m-> 1084\u001b[0m unflatten \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m obj, empty_iter_children: obj\n\u001b[0;32m   1085\u001b[0m register_pytree_with_keys(\u001b[38;5;28mcls\u001b[39m, flatten, unflatten)\n\u001b[0;32m   1086\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model + evaluation with the test data\n",
    "metrics_history = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_accuracy\": [],\n",
    "    \"test_loss\": [],\n",
    "    \"test_accuracy\": [],\n",
    "}\n",
    "\n",
    "for step, batch in enumerate(train_ds.as_numpy_iterator()):\n",
    "    # Run the optimization for one step and make a stateful update to the following:\n",
    "    # - The train state's model parameters\n",
    "    # - The optimizer state\n",
    "    # - The training loss and accuracy batch metrics\n",
    "    train_step(model, optimizer, metrics, batch)\n",
    "\n",
    "    if step > 0 and (\n",
    "        step % eval_every == 0 or step == train_steps - 1\n",
    "    ):  # One training epoch has passed.\n",
    "        # Log the training metrics.\n",
    "        for metric, value in metrics.compute().items():  # Compute the metrics.\n",
    "            metrics_history[f\"train_{metric}\"].append(value)  # Record the metrics.\n",
    "        metrics.reset()  # Reset the metrics for the test set.\n",
    "\n",
    "        # Compute the metrics on the test set after each training epoch.\n",
    "        for test_batch in test_ds.as_numpy_iterator():\n",
    "            eval_step(model, metrics, test_batch)\n",
    "\n",
    "        # Log the test metrics.\n",
    "        for metric, value in metrics.compute().items():\n",
    "            metrics_history[f\"test_{metric}\"].append(value)\n",
    "        metrics.reset()  # Reset the metrics for the next training epoch.\n",
    "\n",
    "        print(\n",
    "            f\"[train] step: {step}, \"\n",
    "            f\"loss: {metrics_history['train_loss'][-1]}, \"\n",
    "            f\"accuracy: {metrics_history['train_accuracy'][-1] * 100}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"[test] step: {step}, \"\n",
    "            f\"loss: {metrics_history['test_loss'][-1]}, \"\n",
    "            f\"accuracy: {metrics_history['test_accuracy'][-1] * 100}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training results into csv file\n",
    "import pandas as pd\n",
    "\n",
    "if leave_data:\n",
    "    data = pd.DataFrame(\n",
    "        {\n",
    "            \"step\": np.arange(eval_every, train_steps + eval_every, eval_every),\n",
    "            \"train_loss\": metrics_history[\"train_loss\"],\n",
    "            \"test_loss\": metrics_history[\"test_loss\"],\n",
    "            \"train_accuracy\": metrics_history[\"train_accuracy\"],\n",
    "            \"test_accuracy\": metrics_history[\"test_accuracy\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    data.to_csv(\n",
    "        method_name\n",
    "        + \"_enc\"\n",
    "        + str(encoded_size)\n",
    "        + \"_nr\"\n",
    "        + str(hidden_neuron)\n",
    "        + \"_d\"\n",
    "        + str(hidden_size)\n",
    "        + \"_lr\"\n",
    "        + str(learning_rate)\n",
    "        +\"_\"\n",
    "        + dataset_name\n",
    "        + \"_step\"\n",
    "        + str(train_steps)\n",
    "        + \"r_min_\"\n",
    "        + str(r_min)\n",
    "        + \"r_max\"\n",
    "        + str(r_max)\n",
    "        + \".csv\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(\n",
    "    np.arange(eval_every, train_steps + eval_every, eval_every),\n",
    "    metrics_history[\"train_loss\"],\n",
    "    label=\"train loss\",\n",
    ")\n",
    "plt.plot(\n",
    "    np.arange(eval_every, train_steps + eval_every, eval_every),\n",
    "    metrics_history[\"test_loss\"],\n",
    "    label=\"test loss\",\n",
    ")\n",
    "plt.title(\n",
    "    \"Train loss of \"\n",
    "    + dataset_name\n",
    "    + \" dataset with \"\n",
    "    + method_name\n",
    "    + \", \\nhidden dimension=\"\n",
    "    + str(hidden_size)\n",
    "    + \", number of neuron=\"\n",
    "    + str(hidden_neuron)\n",
    ")\n",
    "plt.xlabel(\"Training step\")\n",
    "plt.ylabel(\"Train loss (cross entropy)\")\n",
    "plt.legend()\n",
    "if leave_data:\n",
    "    plt.savefig(\n",
    "        \"loss_\"\n",
    "        + method_name\n",
    "        + \"_\"\n",
    "        + str(encoded_size)\n",
    "        + \"_\"\n",
    "        + str(hidden_neuron)\n",
    "        + \"_\"\n",
    "        + dataset_name\n",
    "        + \"_step\"\n",
    "        + str(train_steps)\n",
    "        + \"r_min_\"\n",
    "        + str(r_min)\n",
    "        + \"r_max\"\n",
    "        + str(r_max)\n",
    "        + \".jpg\"\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy\n",
    "plt.plot(\n",
    "    np.arange(eval_every, train_steps + eval_every, eval_every),\n",
    "    metrics_history[\"train_accuracy\"],\n",
    "    label=\"train\",\n",
    ")\n",
    "plt.plot(\n",
    "    np.arange(eval_every, train_steps + eval_every, eval_every),\n",
    "    metrics_history[\"test_accuracy\"],\n",
    "    label=\"test\",\n",
    ")\n",
    "plt.title(\n",
    "    \"Accuracy of \"\n",
    "    + dataset_name\n",
    "    + \" dataset with \"\n",
    "    + method_name\n",
    "    + \", \\nhidden dimension=\"\n",
    "    + str(hidden_size)\n",
    "    + \", number of neuron=\"\n",
    "    + str(hidden_neuron)\n",
    ")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "if leave_data:\n",
    "    plt.savefig(\n",
    "        \"accuracy_\"\n",
    "        + method_name\n",
    "        + \"_\"\n",
    "        + str(encoded_size)\n",
    "        + \"_\"\n",
    "        + str(hidden_neuron)\n",
    "        + \"_\"\n",
    "        + dataset_name\n",
    "        + \"_step\"\n",
    "        + str(train_steps)\n",
    "        + \"r_min_\"\n",
    "        + str(r_min)\n",
    "        + \"r_max\"\n",
    "        + str(r_max)\n",
    "        + \".jpg\"\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
