{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds  # TFDS to download MNIST.\n",
    "import tensorflow as tf  # TensorFlow / `tf.data` operations.\n",
    "from flax import nnx  # The Flax NNX API.\n",
    "from functools import partial\n",
    "import jax.numpy as jnp  # JAX NumPy\n",
    "import jax\n",
    "import optax\n",
    "from linearRNN import forward_h\n",
    "from linearRNN import forward\n",
    "from linearRNN import init_lru_parameters\n",
    "from linearRNN import binary_operator_diag\n",
    "from linearRNN import compute_lr_sigma\n",
    "from linearRNN import LRU\n",
    "import numpy as np\n",
    "from flax import linen as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = 0  # pooling layer after MLP is taking the average over the numbers\n",
    "transformation = 0  # transformation of the data from decimals between 0 and 255 to binary 8 bit numbers\n",
    "leave_data = 1  # download csv data of the results\n",
    "multi_opt = 0  # change the learning rate and the variance of the initialized weights per layer by using the multi-optimizer\n",
    "hidden_neuron = 128  # no details in the 2023 paper => 2024 paper fixed to 512\n",
    "encoded_size = 256\n",
    "hidden_size = 128\n",
    "learning_rate = 0.004\n",
    "momentum = 0.9\n",
    "train_steps = 3000\n",
    "eval_every = 50\n",
    "batch_size = 50\n",
    "r_min = 0.9\n",
    "r_max = 0.999\n",
    "max_phase = 6.28\n",
    "depth = 1\n",
    "method_name = \"LRUMLP\"\n",
    "dataset_name = \"MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_bin_array(arr, m):\n",
    "    # https://stackoverflow.com/questions/22227595/convert-integer-to-binary-array-with-suitable-padding\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    arr: Numpy array of positive integers\n",
    "    m: Number of bits of each integer to retain\n",
    "\n",
    "    Returns a copy of arr with every element replaced with a bit vector.\n",
    "    Bits encoded as int8's.\n",
    "    \"\"\"\n",
    "    to_str_func = np.vectorize(lambda x: np.binary_repr(x).zfill(m))\n",
    "    strs = to_str_func(arr)\n",
    "    ret = np.zeros(list(arr.shape) + [m], dtype=np.int8)\n",
    "    for bit_ix in range(0, m):\n",
    "        fetch_bit_func = np.vectorize(lambda x: x[bit_ix] == \"1\")\n",
    "        ret[..., bit_ix] = fetch_bit_func(strs).astype(\"int8\")\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784, 1)\n",
      "(60000,)\n",
      "(10000, 784, 1)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "\n",
    "if dataset_name == \"MNIST\":\n",
    "    dataset = tf.keras.datasets.mnist.load_data()\n",
    "    train = dataset[0]\n",
    "    test = dataset[1]\n",
    "\n",
    "    train_x_seq = train[0].shape[0]\n",
    "    train_x_len = int(jnp.prod(jnp.array(train[0].shape[1:])))\n",
    "    train_x_size = 1\n",
    "    test_x_seq = test[0].shape[0]\n",
    "    test_x_len = int(jnp.prod(jnp.array(test[0].shape[1:])))\n",
    "    test_x_size = 1\n",
    "    if transformation:  # transform the information of the pixel to 8-bit binary numbers\n",
    "        train_x = train[0].reshape((train_x_seq, train_x_len, train_x_size))\n",
    "        train_x = vec_bin_array(train_x, 8)\n",
    "        train_y = train[1].reshape(train_x_seq)\n",
    "        train_y_class = len(jnp.unique(train_y))\n",
    "        test_x = test[0].reshape((test_x_seq, test_x_len, test_x_size))\n",
    "        test_x = vec_bin_array(test_x, 8)\n",
    "        test_y = test[1].reshape(test_x_seq)\n",
    "    else:\n",
    "        train_x = train[0].reshape((train_x_seq, train_x_len, train_x_size)) / 255\n",
    "        train_y = train[1].reshape(train_x_seq)\n",
    "        train_y_class = len(jnp.unique(train_y))\n",
    "        test_x = test[0].reshape((test_x_seq, test_x_len, test_x_size)) / 255\n",
    "        test_y = test[1].reshape(test_x_seq)\n",
    "\n",
    "if dataset_name == \"CIFAR10\":\n",
    "    dataset = tf.keras.datasets.cifar10.load_data()\n",
    "    train = dataset[0]\n",
    "    test = dataset[1]\n",
    "\n",
    "    train_x_seq = train[0].shape[0]\n",
    "    train_x_len = int(jnp.prod(jnp.array(train[0].shape[1:-1])))\n",
    "    train_x_size = int(jnp.prod(jnp.array(train[0].shape[-1])))\n",
    "\n",
    "    test_x_seq = test[0].shape[0]\n",
    "    test_x_len = int(jnp.prod(jnp.array(test[0].shape[1:-1])))\n",
    "    test_x_size = int(jnp.prod(jnp.array(train[0].shape[-1])))\n",
    "\n",
    "    if (\n",
    "        transformation\n",
    "    ):  # transform the information of the pixel to 3*8-bit binary numbers\n",
    "        train_x = train[0].reshape((train_x_seq, train_x_len, train_x_size))\n",
    "        train_x = vec_bin_array(train_x, 8)\n",
    "        train_y = train[1].reshape(train_x_seq)\n",
    "        train_y_class = len(jnp.unique(train_y))\n",
    "        test_x = test[0].reshape((test_x_seq, test_x_len, test_x_size))\n",
    "        test_x = vec_bin_array(test_x, 8)\n",
    "        test_y = test[1].reshape(test_x_seq)\n",
    "    else:\n",
    "        train_x = train[0].reshape((train_x_seq, train_x_len, train_x_size)) / 255\n",
    "        train_y = train[1].reshape(train_x_seq)\n",
    "        train_y_class = len(jnp.unique(train_y))\n",
    "        test_x = test[0].reshape((test_x_seq, test_x_len, test_x_size)) / 255\n",
    "        test_y = test[1].reshape(test_x_seq)\n",
    "\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (jnp.real(train_x), jnp.array(train_y, dtype=int))\n",
    ")\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (jnp.real(test_x), jnp.array(test_y, dtype=int))\n",
    ")\n",
    "\n",
    "train_ds = train_ds.repeat().shuffle(100)\n",
    "# Group into batches of `batch_size` and skip incomplete batches, prefetch the next sample to improve latency.\n",
    "train_ds = train_ds.batch(batch_size, drop_remainder=True).take(train_steps).prefetch(1)\n",
    "# Group into batches of `batch_size` and skip incomplete batches, prefetch the next sample to improve latency.\n",
    "test_ds = test_ds.batch(batch_size, drop_remainder=True).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  lin_encoder=Linear(\n",
      "    kernel=Param(\n",
      "      value=Array(shape=(1, 256), dtype=float32)\n",
      "    ),\n",
      "    bias=Param(\n",
      "      value=Array(shape=(256,), dtype=float32)\n",
      "    ),\n",
      "    in_features=1,\n",
      "    out_features=256,\n",
      "    use_bias=True,\n",
      "    dtype=None,\n",
      "    param_dtype=<class 'jax.numpy.float32'>,\n",
      "    precision=None,\n",
      "    kernel_init=<function variance_scaling.<locals>.init at 0x0000018C513EEC00>,\n",
      "    bias_init=<function zeros at 0x0000018C4E022160>,\n",
      "    dot_general=<function dot_general at 0x0000018C4DB80D60>\n",
      "  ),\n",
      "  rnn=LRU(\n",
      "    in_features=256,\n",
      "    hidden_features=128,\n",
      "    nu_log=Param(\n",
      "      value=Array(shape=(128,), dtype=float64)\n",
      "    ),\n",
      "    theta_log=Param(\n",
      "      value=Array(shape=(128,), dtype=float64)\n",
      "    ),\n",
      "    B_re=Param(\n",
      "      value=Array(shape=(128, 256), dtype=float64)\n",
      "    ),\n",
      "    B_im=Param(\n",
      "      value=Array(shape=(128, 256), dtype=float64)\n",
      "    ),\n",
      "    C_re=Param(\n",
      "      value=Array(shape=(256, 128), dtype=float64)\n",
      "    ),\n",
      "    C_im=Param(\n",
      "      value=Array(shape=(256, 128), dtype=float64)\n",
      "    ),\n",
      "    D=Param(\n",
      "      value=Array(shape=(256,), dtype=float64)\n",
      "    ),\n",
      "    gamma_log=Param(\n",
      "      value=Array(shape=(128,), dtype=float64)\n",
      "    )\n",
      "  ),\n",
      "  linear1=Linear(\n",
      "    kernel=Param(\n",
      "      value=Array(shape=(256, 128), dtype=float32)\n",
      "    ),\n",
      "    bias=Param(\n",
      "      value=Array(shape=(128,), dtype=float32)\n",
      "    ),\n",
      "    in_features=256,\n",
      "    out_features=128,\n",
      "    use_bias=True,\n",
      "    dtype=None,\n",
      "    param_dtype=<class 'jax.numpy.float32'>,\n",
      "    precision=None,\n",
      "    kernel_init=<function variance_scaling.<locals>.init at 0x0000018C513EEC00>,\n",
      "    bias_init=<function zeros at 0x0000018C4E022160>,\n",
      "    dot_general=<function dot_general at 0x0000018C4DB80D60>\n",
      "  ),\n",
      "  linear2=Linear(\n",
      "    kernel=Param(\n",
      "      value=Array(shape=(64, 256), dtype=float32)\n",
      "    ),\n",
      "    bias=Param(\n",
      "      value=Array(shape=(256,), dtype=float32)\n",
      "    ),\n",
      "    in_features=64,\n",
      "    out_features=256,\n",
      "    use_bias=True,\n",
      "    dtype=None,\n",
      "    param_dtype=<class 'jax.numpy.float32'>,\n",
      "    precision=None,\n",
      "    kernel_init=<function variance_scaling.<locals>.init at 0x0000018C513EEC00>,\n",
      "    bias_init=<function zeros at 0x0000018C4E022160>,\n",
      "    dot_general=<function dot_general at 0x0000018C4DB80D60>\n",
      "  ),\n",
      "  batchnorm=BatchNorm(\n",
      "    mean=BatchStat(\n",
      "      value=Array(shape=(256,), dtype=float32)\n",
      "    ),\n",
      "    var=BatchStat(\n",
      "      value=Array(shape=(256,), dtype=float32)\n",
      "    ),\n",
      "    scale=Param(\n",
      "      value=Array(shape=(256,), dtype=float32)\n",
      "    ),\n",
      "    bias=Param(\n",
      "      value=Array(shape=(256,), dtype=float32)\n",
      "    ),\n",
      "    num_features=256,\n",
      "    use_running_average=True,\n",
      "    axis=-1,\n",
      "    momentum=0.99,\n",
      "    epsilon=1e-05,\n",
      "    dtype=None,\n",
      "    param_dtype=<class 'jax.numpy.float32'>,\n",
      "    use_bias=True,\n",
      "    use_scale=True,\n",
      "    bias_init=<function zeros at 0x0000018C4E022160>,\n",
      "    scale_init=<function ones at 0x0000018C4E0AB060>,\n",
      "    axis_name=None,\n",
      "    axis_index_groups=None,\n",
      "    use_fast_variance=True\n",
      "  ),\n",
      "  linear3=Linear(\n",
      "    kernel=Param(\n",
      "      value=Array(shape=(784, 1), dtype=float32)\n",
      "    ),\n",
      "    bias=Param(\n",
      "      value=Array([0.], dtype=float32)\n",
      "    ),\n",
      "    in_features=784,\n",
      "    out_features=1,\n",
      "    use_bias=True,\n",
      "    dtype=None,\n",
      "    param_dtype=<class 'jax.numpy.float32'>,\n",
      "    precision=None,\n",
      "    kernel_init=<function variance_scaling.<locals>.init at 0x0000018C513EEC00>,\n",
      "    bias_init=<function zeros at 0x0000018C4E022160>,\n",
      "    dot_general=<function dot_general at 0x0000018C4DB80D60>\n",
      "  ),\n",
      "  linear4=Linear(\n",
      "    kernel=Param(\n",
      "      value=Array(shape=(256, 10), dtype=float32)\n",
      "    ),\n",
      "    bias=Param(\n",
      "      value=Array(shape=(10,), dtype=float32)\n",
      "    ),\n",
      "    in_features=256,\n",
      "    out_features=10,\n",
      "    use_bias=True,\n",
      "    dtype=None,\n",
      "    param_dtype=<class 'jax.numpy.float32'>,\n",
      "    precision=None,\n",
      "    kernel_init=<function variance_scaling.<locals>.init at 0x0000018C513EEC00>,\n",
      "    bias_init=<function zeros at 0x0000018C4E022160>,\n",
      "    dot_general=<function dot_general at 0x0000018C4DB80D60>\n",
      "  ),\n",
      "  out_dim=10,\n",
      "  token_len=784,\n",
      "  dropout=Dropout(rate=0.1, broadcast_dims=(), deterministic=False, rng_collection='dropout', rngs=Rngs(\n",
      "    default=RngStream(\n",
      "      key=RngKey(\n",
      "        value=Array((), dtype=key<fry>) overlaying:\n",
      "        [0 0],\n",
      "        tag='default'\n",
      "      ),\n",
      "      count=RngCount(\n",
      "        value=Array(12, dtype=uint32),\n",
      "        tag='default'\n",
      "      )\n",
      "    )\n",
      "  ))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from flax import nnx\n",
    "import optax\n",
    "from flax.nnx.nn.recurrent import LSTMCell, GRUCell\n",
    "import copy\n",
    "import random\n",
    "\n",
    "\n",
    "class MLP(nnx.Module):\n",
    "    # DON'T FORGET TO CHANGE THE MODEL NAME BEFORE RUNNING\n",
    "    # According to the scheme of the paper (Figure 1), input_size=M, encoded_size=H,layer_dim=number of neurons in MLP, out_dim=number of classes\n",
    "    def __init__(\n",
    "        self,\n",
    "        token_size,\n",
    "        token_len,\n",
    "        encoded_dim,\n",
    "        hidden_dim,\n",
    "        layer_dim,\n",
    "        out_dim,\n",
    "        rngs: nnx.Rngs,\n",
    "    ):\n",
    "\n",
    "        # linear encoder\n",
    "        # lrE,sigmaE=compute_lr_sigma(\"input\",token_size,encoded_dim,0,1)\n",
    "        # self.lin_encoder = nnx.Linear(in_features=token_size, out_features=encoded_dim,rngs=rngs,kernel_init=sigmaE*jax.random.normal)\n",
    "        self.lin_encoder = nnx.Linear(\n",
    "            in_features=token_size, out_features=encoded_dim, rngs=rngs\n",
    "        )\n",
    "\n",
    "        # LRU+MLP block\n",
    "        # lrL1, sigmaL1= compute_lr_sigma(\"input\",encoded_dim,layer_dim,0,1)\n",
    "        # lrL2, sigmaL2=compute_lr_sigma(\"output\",0,layer_dim,encoded_dim,1)\n",
    "        # self.linear1 = nnx.Linear(in_features=encoded_dim, out_features=layer_dim, rngs=rngs,kernel_init=sigmaL1*jax.random.normal))\n",
    "        # self.linear2 = nnx.Linear(in_features=layer_dim//2,out_features=encoded_dim,rngs=rngs,kernel_init=sigmaL2*jax.random.normal))\n",
    "        self.rnn = LRU(\n",
    "            in_features=encoded_dim,\n",
    "            hidden_features=hidden_dim,\n",
    "            r_min=r_min,\n",
    "            r_max=r_max,\n",
    "            max_phase=max_phase,\n",
    "        )\n",
    "        self.linear1 = nnx.Linear(\n",
    "            in_features=encoded_dim, out_features=layer_dim, rngs=rngs\n",
    "        )\n",
    "        self.linear2 = nnx.Linear(\n",
    "            in_features=layer_dim // 2, out_features=encoded_dim, rngs=rngs\n",
    "        )\n",
    "        self.batchnorm = nnx.BatchNorm(\n",
    "            num_features=encoded_dim, rngs=rngs, use_running_average=True\n",
    "        )\n",
    "\n",
    "        # Linear layers\n",
    "        if pool:  # If pooling layer takes the average over the token sequence length\n",
    "            self.linear3 = lambda x: jnp.mean(x, axis=1)\n",
    "        else:  # learn the parameters of the linear transformation\n",
    "            self.linear3 = nnx.Linear(in_features=token_len, out_features=1, rngs=rngs)\n",
    "        self.linear4 = nnx.Linear(\n",
    "            in_features=encoded_dim, out_features=out_dim, rngs=rngs\n",
    "        )\n",
    "        self.out_dim = out_dim\n",
    "        self.token_len = token_len\n",
    "        self.dropout = nnx.Dropout(0.1, rngs=rngs)\n",
    "\n",
    "    @nnx.vmap(in_axes=(None, 0))\n",
    "    def __call__(self, x):\n",
    "        x = self.lin_encoder(x)\n",
    "        # x=x@self.lin_encoder\n",
    "        y = x.copy()\n",
    "\n",
    "        # LRU+MLP block\n",
    "        for i in range(depth):\n",
    "            x = self.rnn(x)\n",
    "            x = self.linear1(x)\n",
    "            x = nnx.glu(x, axis=-1)\n",
    "            x = self.linear2(x)\n",
    "            x = self.dropout(x)\n",
    "            x += y  # Skip connection -> p.21 adding for each block\n",
    "            x = self.batchnorm(x)  # batch normalization\n",
    "\n",
    "        # x = x.T@self.weight #+ self.bias #project from L*H to H*1\n",
    "        # x = self.weight2@x #+ self.bias2#project from H*1 to out_dim\n",
    "        x = self.linear3(x.T)\n",
    "        x = self.linear4(x.T)\n",
    "        return x.reshape(self.out_dim)\n",
    "\n",
    "\n",
    "model = MLP(\n",
    "    train_x_size,\n",
    "    train_x_len,\n",
    "    encoded_size,\n",
    "    hidden_size,\n",
    "    hidden_neuron,\n",
    "    train_y_class,\n",
    "    rngs=nnx.Rngs(0),\n",
    ")  # eager initialization\n",
    "\n",
    "#nnx.display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with the first batch\n",
    "#for step, batch in enumerate(train_ds.as_numpy_iterator()):\n",
    "#    batch1 = batch\n",
    "#    break\n",
    "#a = model(batch[0])\n",
    "#print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "\n",
    "#scheduler1 = optax.schedules.warmup_constant_schedule(init_value=learning_rate, peak_value=,warmup_steps=train_steps//10)\n",
    "scheduler2=optax.schedules.warmup_cosine_decay_schedule(init_value=learning_rate,peak_value=1e-7,warmup_steps=train_steps//10,decay_steps=train_steps,end_value=)                                                        )\n",
    "#chained_transform=optax.chain(scheduler1,scheduler2)\n",
    "optimizer = nnx.Optimizer(model, optax.adamw(scheduler2, momentum,weight_decay=0.05))\n",
    "#optimizer = nnx.Optimizer(model, optax.adamw(learning_rate, momentum, weight_decay=0.05))\n",
    "metrics = nnx.MultiMetric(\n",
    "    accuracy=nnx.metrics.Accuracy(),\n",
    "    loss=nnx.metrics.Average(\"loss\"),\n",
    ")\n",
    "\n",
    "#nnx.display(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "\n",
    "def loss_fn(model: MLP, batch):\n",
    "    logits = model(batch[0])\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "        logits=logits, labels=batch[1]\n",
    "    ).mean()\n",
    "    # print(logits.shape)\n",
    "    # print(batch[1].shape)\n",
    "    return loss, logits\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(model: MLP, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
    "    \"\"\"Train for a single step.\"\"\"\n",
    "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, logits), grads = grad_fn(model, batch)\n",
    "    metrics.update(loss=loss, logits=logits, labels=batch[1])  # In-place updates.\n",
    "    optimizer.update(grads)  # In-place updates.\n",
    "\n",
    "    # Print the predicted labels and the actual labels of the first five images from the batch\n",
    "    # predicted_labels = jnp.argmax(logits, axis=-1)\n",
    "    # actual_labels = batch[1]\n",
    "    # jax.debug.print(\"Predictions: {}\",predicted_labels[:5].astype(int))\n",
    "    # jax.debug.print(\"Actual Labels: {}\",actual_labels[:5].astype(int))\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def eval_step(model: MLP, metrics: nnx.MultiMetric, batch):\n",
    "    loss, logits = loss_fn(model, batch)\n",
    "    metrics.update(loss=loss, logits=logits, labels=batch[1])  # In-place updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model + evaluation with the test data\n",
    "metrics_history = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_accuracy\": [],\n",
    "    \"test_loss\": [],\n",
    "    \"test_accuracy\": [],\n",
    "}\n",
    "\n",
    "for step, batch in enumerate(train_ds.as_numpy_iterator()):\n",
    "    # Run the optimization for one step and make a stateful update to the following:\n",
    "    # - The train state's model parameters\n",
    "    # - The optimizer state\n",
    "    # - The training loss and accuracy batch metrics\n",
    "    train_step(model, optimizer, metrics, batch)\n",
    "\n",
    "    if step > 0 and (\n",
    "        step % eval_every == 0 or step == train_steps - 1\n",
    "    ):  # One training epoch has passed.\n",
    "        # Log the training metrics.\n",
    "        for metric, value in metrics.compute().items():  # Compute the metrics.\n",
    "            metrics_history[f\"train_{metric}\"].append(value)  # Record the metrics.\n",
    "        metrics.reset()  # Reset the metrics for the test set.\n",
    "\n",
    "        # Compute the metrics on the test set after each training epoch.\n",
    "        for test_batch in test_ds.as_numpy_iterator():\n",
    "            eval_step(model, metrics, test_batch)\n",
    "\n",
    "        # Log the test metrics.\n",
    "        for metric, value in metrics.compute().items():\n",
    "            metrics_history[f\"test_{metric}\"].append(value)\n",
    "        metrics.reset()  # Reset the metrics for the next training epoch.\n",
    "\n",
    "        print(\n",
    "            f\"[train] step: {step}, \"\n",
    "            f\"loss: {metrics_history['train_loss'][-1]}, \"\n",
    "            f\"accuracy: {metrics_history['train_accuracy'][-1] * 100}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"[test] step: {step}, \"\n",
    "            f\"loss: {metrics_history['test_loss'][-1]}, \"\n",
    "            f\"accuracy: {metrics_history['test_accuracy'][-1] * 100}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training results into csv file\n",
    "import pandas as pd\n",
    "\n",
    "if leave_data:\n",
    "    data = pd.DataFrame(\n",
    "        {\n",
    "            \"step\": np.arange(eval_every, train_steps + eval_every, eval_every),\n",
    "            \"train_loss\": metrics_history[\"train_loss\"],\n",
    "            \"test_loss\": metrics_history[\"test_loss\"],\n",
    "            \"train_accuracy\": metrics_history[\"train_accuracy\"],\n",
    "            \"test_accuracy\": metrics_history[\"test_accuracy\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    data.to_csv(\n",
    "        method_name\n",
    "        + \"_enc\"\n",
    "        + str(encoded_size)\n",
    "        + \"_nr\"\n",
    "        + str(hidden_neuron)\n",
    "        + \"_d\"\n",
    "        + str(hidden_size)\n",
    "        + \"_\"\n",
    "        + dataset_name\n",
    "        + \"_step\"\n",
    "        + str(train_steps)\n",
    "        + \"r_min_\"\n",
    "        + str(r_min)\n",
    "        + \"r_max\"\n",
    "        + str(r_max)\n",
    "        + \".csv\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(\n",
    "    np.arange(eval_every, train_steps + eval_every, eval_every),\n",
    "    metrics_history[\"train_loss\"],\n",
    "    label=\"train loss\",\n",
    ")\n",
    "plt.plot(\n",
    "    np.arange(eval_every, train_steps + eval_every, eval_every),\n",
    "    metrics_history[\"test_loss\"],\n",
    "    label=\"test loss\",\n",
    ")\n",
    "plt.title(\n",
    "    \"Train loss of \"\n",
    "    + dataset_name\n",
    "    + \" dataset with \"\n",
    "    + method_name\n",
    "    + \", \\nhidden dimension=\"\n",
    "    + str(hidden_size)\n",
    "    + \", number of neuron=\"\n",
    "    + str(hidden_neuron)\n",
    ")\n",
    "plt.xlabel(\"Training step\")\n",
    "plt.ylabel(\"Train loss (cross entropy)\")\n",
    "plt.legend()\n",
    "if leave_data:\n",
    "    plt.savefig(\n",
    "        \"loss_\"\n",
    "        + method_name\n",
    "        + \"_\"\n",
    "        + str(encoded_size)\n",
    "        + \"_\"\n",
    "        + str(hidden_neuron)\n",
    "        + \"_\"\n",
    "        + dataset_name\n",
    "        + \"_step\"\n",
    "        + str(train_steps)\n",
    "        + \"r_min_\"\n",
    "        + str(r_min)\n",
    "        + \"r_max\"\n",
    "        + str(r_max)\n",
    "        + \".jpg\"\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy\n",
    "plt.plot(\n",
    "    np.arange(eval_every, train_steps + eval_every, eval_every),\n",
    "    metrics_history[\"train_accuracy\"],\n",
    "    label=\"train\",\n",
    ")\n",
    "plt.plot(\n",
    "    np.arange(eval_every, train_steps + eval_every, eval_every),\n",
    "    metrics_history[\"test_accuracy\"],\n",
    "    label=\"test\",\n",
    ")\n",
    "plt.title(\n",
    "    \"Accuracy of \"\n",
    "    + dataset_name\n",
    "    + \" dataset with \"\n",
    "    + method_name\n",
    "    + \", \\nhidden dimension=\"\n",
    "    + str(hidden_size)\n",
    "    + \", number of neuron=\"\n",
    "    + str(hidden_neuron)\n",
    ")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "if leave_data:\n",
    "    plt.savefig(\n",
    "        \"accuracy_\"\n",
    "        + method_name\n",
    "        + \"_\"\n",
    "        + str(encoded_size)\n",
    "        + \"_\"\n",
    "        + str(hidden_neuron)\n",
    "        + \"_\"\n",
    "        + dataset_name\n",
    "        + \"_step\"\n",
    "        + str(train_steps)\n",
    "        + \"r_min_\"\n",
    "        + str(r_min)\n",
    "        + \"r_max\"\n",
    "        + str(r_max)\n",
    "        + \".jpg\"\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
