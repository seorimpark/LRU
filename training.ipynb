{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds  # TFDS to download MNIST.\n",
    "import tensorflow as tf  # TensorFlow / `tf.data` operations.\n",
    "from flax import nnx  # The Flax NNX API.\n",
    "from functools import partial\n",
    "import jax.numpy as jnp  # JAX NumPy\n",
    "import jax\n",
    "import optax\n",
    "from linearRNN import forward_h\n",
    "from linearRNN import forward\n",
    "from linearRNN import init_lru_parameters\n",
    "from linearRNN import binary_operator_diag\n",
    "from linearRNN import LRU\n",
    "import numpy as np\n",
    "from flax import linen as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn=1 #rnn=0:transformation of the inputs with fixed RNN weights, rnn=1: adding the RNN module on the model to learn the weight matrices\n",
    "mlp=1\n",
    "hidden_neuron=256\n",
    "encoded_size=512\n",
    "hidden_size=256\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "train_steps=1200\n",
    "eval_every = 100\n",
    "batch_size=64\n",
    "r_min = 0\n",
    "r_max = 1\n",
    "max_phase = 6.28\n",
    "method_name=\"LRU+MLP\"\n",
    "dataset_name=\"MNIST\"\n",
    "leave_data=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import data\n",
    "\n",
    "if dataset_name==\"MNIST\":\n",
    "    dataset=tf.keras.datasets.mnist.load_data()\n",
    "    train=dataset[0]\n",
    "    test=dataset[1]\n",
    "\n",
    "    train_x_seq=train[0].shape[0]\n",
    "    train_x_len=int(jnp.prod(jnp.array(train[0].shape[1:])))\n",
    "    train_x_size=1\n",
    "    test_x_seq=test[0].shape[0]\n",
    "    test_x_len=int(jnp.prod(jnp.array(test[0].shape[1:])))\n",
    "    test_x_size=1\n",
    "\n",
    "    train_x=train[0].reshape((train_x_seq,train_x_len,train_x_size))/255\n",
    "    train_y=train[1].reshape(train_x_seq)\n",
    "    train_y_class=len(jnp.unique(train_y))\n",
    "\n",
    "    test_x=test[0].reshape((test_x_seq,test_x_len,test_x_size))/255\n",
    "    test_y=test[1].reshape(test_x_seq)\n",
    "\n",
    "if dataset_name==\"CIFAR10\":\n",
    "    dataset=tf.keras.datasets.cifar10.load_data()\n",
    "    train=dataset[0]\n",
    "    test=dataset[1]\n",
    "\n",
    "    train_x_seq=train[0].shape[0]\n",
    "    train_x_len=int(jnp.prod(jnp.array(train[0].shape[1:-1])))\n",
    "    train_x_size=int(jnp.prod(jnp.array(train[0].shape[-1])))\n",
    "\n",
    "    test_x_seq=test[0].shape[0]\n",
    "    test_x_len=int(jnp.prod(jnp.array(test[0].shape[1:-1])))\n",
    "    test_x_size=int(jnp.prod(jnp.array(train[0].shape[-1])))\n",
    "\n",
    "    train_x=train[0].reshape((train_x_seq,train_x_len,train_x_size))/255\n",
    "\n",
    "    train_y=train[1].reshape(train_x_seq)\n",
    "    train_y_class=len(jnp.unique(train_y))\n",
    "\n",
    "    test_x=test[0].reshape((test_x_seq,test_x_len,test_x_size))/255\n",
    "    test_y=test[1].reshape(test_x_seq)\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rnn:\n",
    "    train_hidden_state=train_x\n",
    "    test_hidden_state=test_x\n",
    "else:\n",
    "    param=init_lru_parameters(hidden_size,train_x_len,r_min=0.999,r_max=0.9999,max_phase=6.28)\n",
    "    param2=init_lru_parameters(hidden_size,train_x_size,r_min=0.999,r_max=0.9999,max_phase=6.28)\n",
    "    train_hidden_state=jnp.real(forward_h(param,train_x))\n",
    "    test_hidden_state=jnp.real(forward_h(param2,test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds=tf.data.Dataset.from_tensor_slices((jnp.real(train_hidden_state),jnp.array(train_y,dtype=int)))\n",
    "test_ds=tf.data.Dataset.from_tensor_slices((jnp.real(test_hidden_state),jnp.array(test_y,dtype=int)))\n",
    "\n",
    "train_ds = train_ds.repeat().shuffle(100)\n",
    "# Group into batches of `batch_size` and skip incomplete batches, prefetch the next sample to improve latency.\n",
    "train_ds = train_ds.batch(batch_size, drop_remainder=True).take(train_steps).prefetch(1)\n",
    "# Group into batches of `batch_size` and skip incomplete batches, prefetch the next sample to improve latency.\n",
    "test_ds = test_ds.batch(batch_size, drop_remainder=True).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_hidden_state.shape)\n",
    "print(test_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import nnx\n",
    "import optax\n",
    "from flax.nnx.nn.recurrent import LSTMCell,GRUCell\n",
    "import copy\n",
    "import random\n",
    "class MLP(nnx.Module):\n",
    "  #DON'T FORGET TO CHANGE THE MODEL NAME BEFORE RUNNING\n",
    "  #According to the scheme of the paper (Figure 1), input_size=M, encoded_size=H,layer_dim=number of neurons in MLP, out_dim=number of classes\n",
    "  def __init__(self, token_size, token_len,encoded_dim,hidden_dim, layer_dim, out_dim, rngs: nnx.Rngs):\n",
    "\n",
    "    #linear encoder\n",
    "    self.lin_encoder = nnx.Linear(in_features=token_size, out_features=encoded_dim,rngs=rngs)\n",
    "\n",
    "    #LRU+MLP block\n",
    "    self.rnn = LRU(in_features=encoded_dim, hidden_features=hidden_dim, r_min=r_min,r_max=r_max,max_phase=max_phase)\n",
    "    self.linear1 = nnx.Linear(in_features=encoded_dim, out_features=layer_dim, rngs=rngs)\n",
    "    self.linear2 = nnx.Linear(in_features=layer_dim,out_features=encoded_dim,rngs=rngs)\n",
    "\n",
    "    #Linear layers\n",
    "    self.weight = nnx.Param(jnp.array(np.random.rand(token_len,1)))\n",
    "    self.bias = nnx.Param(jnp.array(np.random.rand(encoded_dim,1)))\n",
    "    self.weight2 = nnx.Param(jnp.array(np.random.rand(out_dim,encoded_dim)))\n",
    "    self.bias2 = nnx.Param(jnp.array(np.random.rand(out_dim,1)))\n",
    "    self.out_dim = out_dim\n",
    "\n",
    "    \n",
    "  @nnx.vmap(in_axes=(None,0)) \n",
    "  def __call__(self, x):\n",
    "    x = self.lin_encoder(x)\n",
    "    y = x.copy()\n",
    "    #LRU+MLP block\n",
    "    if rnn:\n",
    "      x = self.rnn(x)\n",
    "    x = self.linear1(x)\n",
    "    x = nnx.softplus(x)\n",
    "    x = self.linear2(x)\n",
    "\n",
    "    x += y #Skip connection\n",
    "    x = x.T@self.weight + self.bias #project from L*H to H*1\n",
    "    x = self.weight2@x + self.bias2#project from H*1 to out_dim\n",
    "    return x.reshape(self.out_dim)\n",
    "\n",
    "\n",
    "model = MLP(train_x_size,train_x_len,encoded_size,hidden_size, hidden_neuron, train_y_class, rngs=nnx.Rngs(0))  # eager initialization\n",
    "\n",
    "nnx.display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step,batch in enumerate(train_ds.as_numpy_iterator()):\n",
    "    batch1=batch\n",
    "    break\n",
    "\n",
    "a=model(batch1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "optimizer = nnx.Optimizer(model, optax.adamw(learning_rate, momentum))\n",
    "metrics = nnx.MultiMetric(\n",
    "  accuracy=nnx.metrics.Accuracy(),\n",
    "  loss=nnx.metrics.Average('loss'),\n",
    ")\n",
    "\n",
    "nnx.display(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "def loss_fn(model: MLP, batch):\n",
    "  logits = model(batch[0])\n",
    "  loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "    logits=logits, labels=batch[1]\n",
    "  ).mean()\n",
    "  #print(logits.shape)\n",
    "  #print(batch[1].shape)\n",
    "  return loss, logits\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(model: MLP, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
    "  \"\"\"Train for a single step.\"\"\"\n",
    "  grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
    "  (loss, logits), grads = grad_fn(model, batch)\n",
    "  metrics.update(loss=loss, logits=logits, labels=batch[1])  # In-place updates.\n",
    "  optimizer.update(grads)  # In-place updates.\n",
    "  predicted_labels = jnp.argmax(logits, axis=-1)\n",
    "  actual_labels = batch[1]\n",
    "  #jax.debug.print(\"Predictions: {}\",predicted_labels[:5].astype(int))\n",
    "  #jax.debug.print(\"Actual Labels: {}\",actual_labels[:5].astype(int))\n",
    "  \n",
    "@nnx.jit\n",
    "def eval_step(model: MLP, metrics: nnx.MultiMetric, batch):\n",
    "  loss, logits = loss_fn(model, batch)\n",
    "  metrics.update(loss=loss, logits=logits, labels=batch[1])  # In-place updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model + evaluation with the test data\n",
    "metrics_history = {\n",
    "    'train_loss': [],\n",
    "    'train_accuracy': [],\n",
    "    'test_loss': [],\n",
    "    'test_accuracy': [],\n",
    "}\n",
    "\n",
    "for step, batch in enumerate(train_ds.as_numpy_iterator()):\n",
    "  # Run the optimization for one step and make a stateful update to the following:\n",
    "  # - The train state's model parameters\n",
    "  # - The optimizer state\n",
    "  # - The training loss and accuracy batch metrics\n",
    "  train_step(model, optimizer, metrics, batch)\n",
    "\n",
    "  if step > 0 and (step % eval_every == 0 or step == train_steps - 1):  # One training epoch has passed.\n",
    "    # Log the training metrics.\n",
    "    for metric, value in metrics.compute().items():  # Compute the metrics.\n",
    "      metrics_history[f'train_{metric}'].append(value)  # Record the metrics.\n",
    "    metrics.reset()  # Reset the metrics for the test set.\n",
    "\n",
    "    # Compute the metrics on the test set after each training epoch.\n",
    "    for test_batch in test_ds.as_numpy_iterator():\n",
    "      eval_step(model, metrics, test_batch)\n",
    "\n",
    "    # Log the test metrics.\n",
    "    for metric, value in metrics.compute().items():\n",
    "      metrics_history[f'test_{metric}'].append(value)\n",
    "    metrics.reset()  # Reset the metrics for the next training epoch.\n",
    "\n",
    "    print(\n",
    "      f\"[train] step: {step}, \"\n",
    "      f\"loss: {metrics_history['train_loss'][-1]}, \"\n",
    "      f\"accuracy: {metrics_history['train_accuracy'][-1] * 100}\"\n",
    "    )\n",
    "    print(\n",
    "      f\"[test] step: {step}, \"\n",
    "      f\"loss: {metrics_history['test_loss'][-1]}, \"\n",
    "      f\"accuracy: {metrics_history['test_accuracy'][-1] * 100}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the training results into csv\n",
    "import pandas as pd\n",
    "\n",
    "if leave_data:\n",
    "    data=pd.DataFrame({\"step\":np.arange(eval_every,train_steps+eval_every,eval_every),\"train_loss\":metrics_history['train_loss'],\n",
    "                       \"test_loss\":metrics_history['test_loss'],\"train_accuracy\":metrics_history['train_accuracy'],\n",
    "                       \"test_accuracy\":metrics_history['test_accuracy']})\n",
    "    if mlp:\n",
    "        data.to_csv(method_name+\"_enc\"+str(encoded_size)+\"_nr\"+str(hidden_neuron)+\"_d\"+str(hidden_size)+\"_\"+dataset_name+\"_step\"+str(train_steps)+\"r_min_\"+str(r_min)+\"r_max\"+str(r_max)+\".csv\")\n",
    "    #else:\n",
    "    #    data.to_csv(method_name+\"_d\"+str(hidden_size)+\"_\"+dataset_name+\"_step\"+str(train_steps)+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.arange(eval_every,train_steps+eval_every,eval_every),metrics_history['train_loss'],label=\"train loss\")\n",
    "plt.plot(np.arange(eval_every,train_steps+eval_every,eval_every),metrics_history['test_loss'],label=\"test loss\")\n",
    "if mlp:\n",
    "    plt.title(\"Train loss of \"+dataset_name+\" dataset with \"+method_name+\n",
    "              \", \\nhidden dimension=\"+str(hidden_size)+\", number of neuron=\"+str(hidden_neuron))\n",
    "#else:\n",
    "#    plt.title(\"Train loss of MNIST dataset with GRU+MLP, \\nhidden dimension=\"+str(hidden_size))\n",
    "plt.xlabel(\"Training step\")\n",
    "plt.ylabel(\"Train loss (cross entropy)\")\n",
    "plt.legend()\n",
    "if leave_data:\n",
    "    plt.savefig(\"loss_\"+method_name+\"_\"+str(encoded_size)+\"_\"+str(hidden_neuron)+\"_\"+dataset_name+\"_step\"+str(train_steps)+\"r_min_\"+str(r_min)+\"r_max\"+str(r_max)+\".jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the accuracy\n",
    "plt.plot(np.arange(eval_every,train_steps+eval_every,eval_every),metrics_history['train_accuracy'],label=\"train\")\n",
    "plt.plot(np.arange(eval_every,train_steps+eval_every,eval_every),metrics_history['test_accuracy'],label=\"test\")\n",
    "plt.title(\"Accuracy of \"+dataset_name+\" dataset with \"+method_name+\"MLP, \\nhidden dimension=\"+\n",
    "          str(hidden_size)+\", number of neuron=\"+str(hidden_neuron))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "if leave_data:\n",
    "    plt.savefig(\"accuracy_\"+method_name+\"_\"+str(encoded_size)+\"_\"+str(hidden_neuron)+\"_\"+dataset_name+\"_step\"+str(train_steps)+\"r_min_\"+str(r_min)+\"r_max\"+str(r_max)+\".jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
