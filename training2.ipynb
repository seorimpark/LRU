{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "8_1sdNbVoZSS",
    "outputId": "b99a14ee-f7f2-43d5-816b-59d24ce44ab5"
   },
   "outputs": [],
   "source": [
    "!pip install flax --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "VJVdrh0soEFi",
    "outputId": "9f2424f2-c1ab-419f-b1c6-65102350dec5"
   },
   "outputs": [],
   "source": [
    "!pip install jax --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "I9r8E1NtqHZD",
    "outputId": "0b632c77-f9d4-4389-d7a9-c60205f22e74"
   },
   "outputs": [],
   "source": [
    "#check if version >= 0.10.0\n",
    "!pip show flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "xuPgciH_qP5v",
    "outputId": "6f43ba17-1de5-416c-c388-6eeb11da8623"
   },
   "outputs": [],
   "source": [
    "#check if version >= 0.4.35\n",
    "!pip show jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EWLRsVQum8X3"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds  # TFDS to download MNIST.\n",
    "import tensorflow as tf  # TensorFlow / `tf.data` operations.\n",
    "from flax import nnx  # The Flax NNX API.\n",
    "import jax.numpy as jnp  # JAX NumPy\n",
    "import jax\n",
    "import optax\n",
    "import copy\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XY6XHBLHnBXx"
   },
   "outputs": [],
   "source": [
    "\n",
    "parallel_scan = jax.lax.associative_scan\n",
    "\n",
    "# From Orvieto et al., 2023, (https://arxiv.org/abs/2303.06349)\n",
    "\n",
    "\n",
    "def forward(lru_parameters, input_sequence):\n",
    "    \"\"\"Forward pass of the LRU layer. Output y and input_sequence are of shape (L, H).\"\"\"\n",
    "\n",
    "    # All LRU parameters\n",
    "    nu_log, theta_log, B_re, B_im, C_re, C_im, D, gamma_log = lru_parameters\n",
    "\n",
    "    # Materializing the diagonal of Lambda and projections\n",
    "    Lambda = jnp.exp(-jnp.exp(nu_log) + 1j * jnp.exp(theta_log))\n",
    "    B_norm = (B_re + 1j * B_im) * jnp.expand_dims(jnp.exp(gamma_log), axis=-1)\n",
    "    C = C_re + 1j * C_im\n",
    "\n",
    "    # Running the LRU + output projection\n",
    "    # For details on parallel scan, check discussion in Smith et al (2022).\n",
    "    Lambda_elements = jnp.repeat(Lambda[None, ...], input_sequence.shape[0], axis=0)\n",
    "    Bu_elements = jax.vmap(lambda u: B_norm @ u)(input_sequence)\n",
    "    elements = (Lambda_elements, Bu_elements)\n",
    "    _, inner_states = parallel_scan(binary_operator_diag, elements)  # all x_k\n",
    "    y = jax.vmap(lambda x, u: (C @ x).real + D * u)(inner_states, input_sequence)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def init_lru_parameters(N, H, r_min=0, r_max=1, max_phase=0.314):\n",
    "    \"\"\"Initialize parameters of the LRU layer.\"\"\"\n",
    "\n",
    "    # N: state dimension, H: model dimension\n",
    "    # Initialization of Lambda is complex valued distributed uniformly on ring\n",
    "    # between r_min and r_max, with phase in [0, max_phase].\n",
    "    u1 = np.random.uniform(size=(N,))\n",
    "    u2 = np.random.uniform(size=(N,))\n",
    "    nu_log = np.log(-0.5 * np.log(u1 * (r_max**2 - r_min**2) + r_min**2))\n",
    "    theta_log = np.log(max_phase * u2)\n",
    "\n",
    "    # Glorot initialized Input/Output projection matrices\n",
    "    B_re = np.random.normal(size=(N, H)) / np.sqrt(2 * H)\n",
    "    B_im = np.random.normal(size=(N, H)) / np.sqrt(2 * H)\n",
    "    C_re = np.random.normal(size=(H, N)) / np.sqrt(N)\n",
    "    C_im = np.random.normal(size=(H, N)) / np.sqrt(N)\n",
    "    D = np.random.normal(size=(H,))\n",
    "\n",
    "    # Normalization factor\n",
    "    diag_lambda = np.exp(-np.exp(nu_log) + 1j * np.exp(theta_log))\n",
    "    gamma_log = np.log(np.sqrt(1 - np.abs(diag_lambda) ** 2))\n",
    "\n",
    "    return nu_log, theta_log, B_re, B_im, C_re, C_im, D, gamma_log\n",
    "\n",
    "\n",
    "def binary_operator_diag(element_i, element_j):\n",
    "    # Binary operator for parallel scan of linear recurrence.\n",
    "    a_i, bu_i = element_i\n",
    "    a_j, bu_j = element_j\n",
    "    return a_j * a_i, a_j * bu_i + bu_j\n",
    "\n",
    "\n",
    "Array = jax.Array\n",
    "\n",
    "\n",
    "class LRU(nnx.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        hidden_features: int,  # not inferred from carry for now\n",
    "        *,\n",
    "        r_min=0,\n",
    "        r_max=1,\n",
    "        max_phase=6.28,\n",
    "    ):\n",
    "        self.in_features = in_features\n",
    "        self.hidden_features = hidden_features\n",
    "        nu_log, theta_log, B_re, B_im, C_re, C_im, D, gamma_log = init_lru_parameters(\n",
    "            hidden_features, in_features, r_min=r_min, r_max=r_max, max_phase=max_phase\n",
    "        )\n",
    "\n",
    "        self.nu_log = nnx.Param(nu_log)\n",
    "        self.theta_log = nnx.Param(theta_log)\n",
    "        self.B_re = nnx.Param(B_re)\n",
    "        self.B_im = nnx.Param(B_im)\n",
    "        self.C_re = nnx.Param(C_re)\n",
    "        self.C_im = nnx.Param(C_im)\n",
    "        self.D = nnx.Param(D)\n",
    "        self.gamma_log = nnx.Param(gamma_log)\n",
    "\n",
    "    def __call__(self, inputs: Array):  # type: ignore[override]\n",
    "        # jax.debug.print(\"test:{}\", jnp.sin(self.nu_log + self.theta_log))\n",
    "        Lambda = jnp.exp(\n",
    "            -jnp.exp(self.nu_log.value) + 1j * jnp.exp(self.theta_log.value)\n",
    "        )\n",
    "        B_norm = (self.B_re.value + 1j * self.B_im.value) * jnp.expand_dims(\n",
    "            jnp.exp(self.gamma_log.value), axis=-1\n",
    "        )\n",
    "        # Running the LRU + output projection\n",
    "        # For details on parallel scan, check discussion in Smith et al (2022).\n",
    "        Lambda_elements = jnp.repeat(Lambda[None, ...], inputs.shape[0], axis=0)\n",
    "        Bu_elements = jax.vmap(lambda u: B_norm @ u)(inputs)\n",
    "        elements = (Lambda_elements, Bu_elements)\n",
    "        C = self.C_re + 1j * self.C_im\n",
    "        _, h = parallel_scan(binary_operator_diag, elements)\n",
    "        y = jax.vmap(lambda x, u: (C @ x).real + self.D * u)(h, inputs)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Het4U2Cm8X7"
   },
   "outputs": [],
   "source": [
    "rnn=1 #rnn=0:transformation of the inputs with fixed RNN weights, rnn=1: adding the RNN module on the model to learn the weight matrices\n",
    "pool=0 #pooling layer after MLP is taking the average over the numbers\n",
    "transformation=0 #transformation of the data from decimals between 0 and 256 to binary 8 bit numbers\n",
    "leave_data=1 #download csv data of the results\n",
    "hidden_neuron=512 #no details in the 2023 paper => 2024 paper fixed to 512 with LV system\n",
    "encoded_size=512\n",
    "hidden_size=384\n",
    "learning_rate = 5e-4\n",
    "momentum = 0.9\n",
    "train_steps=30000\n",
    "eval_every = 50\n",
    "batch_size=50\n",
    "r_min = 0.9\n",
    "r_max = 0.999\n",
    "max_phase = 6.28\n",
    "depth=6\n",
    "lr_factor=0.25\n",
    "dropout=0.1\n",
    "method_name=\"LRUMLP6\"\n",
    "dataset_name=\"CIFAR10\"\n",
    "rand=random.randint(0,10000)\n",
    "rngs1=nnx.Rngs(rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BEhCTOMq7tBJ",
    "outputId": "fc8b8a6b-1146-40a0-ac96-1966fbaa8b0d"
   },
   "outputs": [],
   "source": [
    "def vec_bin_array(arr, m): #https://stackoverflow.com/questions/22227595/convert-integer-to-binary-array-with-suitable-padding\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    arr: Numpy array of positive integers\n",
    "    m: Number of bits of each integer to retain\n",
    "\n",
    "    Returns a copy of arr with every element replaced with a bit vector.\n",
    "    Bits encoded as int8's.12\n",
    "    \"\"\"\n",
    "\n",
    "    to_str_func = np.vectorize(lambda x: np.binary_repr(x).zfill(m))\n",
    "    strs = to_str_func(arr)\n",
    "    ret = np.zeros(list(arr.shape) + [m], dtype=np.int8)\n",
    "    for bit_ix in range(0, m):\n",
    "        fetch_bit_func = np.vectorize(lambda x: x[bit_ix] == '1')\n",
    "        ret[...,bit_ix] = fetch_bit_func(strs).astype(\"int8\")\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "#Import data\n",
    "\n",
    "if dataset_name==\"MNIST\":\n",
    "    dataset=tf.keras.datasets.mnist.load_data()\n",
    "    train=dataset[0]\n",
    "    test=dataset[1]\n",
    "\n",
    "    train_x_seq=train[0].shape[0]\n",
    "    train_x_len=int(jnp.prod(jnp.array(train[0].shape[1:])))\n",
    "    test_x_seq=test[0].shape[0]\n",
    "    test_x_len=int(jnp.prod(jnp.array(test[0].shape[1:])))\n",
    "    if transformation:\n",
    "        train_x_size=8\n",
    "        test_x_size=8\n",
    "\n",
    "        train_x=vec_bin_array(train[0],train_x_size)\n",
    "        train_x=train_x.reshape((train_x_seq,train_x_len,train_x_size))\n",
    "\n",
    "        train_y=train[1].reshape(train_x_seq)\n",
    "\n",
    "        train_y_class=len(jnp.unique(train_y))\n",
    "\n",
    "        test_x=vec_bin_array(test[0],test_x_size)\n",
    "        test_x=test_x.reshape((test_x_seq,test_x_len,test_x_size))\n",
    "\n",
    "        test_y=test[1].reshape(test_x_seq)\n",
    "\n",
    "    else:\n",
    "        train_x_size=1\n",
    "        test_x_size=1\n",
    "        train_x=train[0].reshape((train_x_seq,train_x_len,train_x_size))/255\n",
    "        train_y=train[1].reshape(train_x_seq)\n",
    "        train_y_class=len(jnp.unique(train_y))\n",
    "        test_x=test[0].reshape((test_x_seq,test_x_len,test_x_size))/255\n",
    "        test_y=test[1].reshape(test_x_seq)\n",
    "\n",
    "\n",
    "if dataset_name==\"CIFAR10\":\n",
    "    dataset=tf.keras.datasets.cifar10.load_data()\n",
    "    train=dataset[0]\n",
    "    test=dataset[1]\n",
    "\n",
    "    train_x_seq=train[0].shape[0]\n",
    "    train_x_len=int(jnp.prod(jnp.array(train[0].shape[1:-1])))\n",
    "    train_x_size=int(jnp.prod(jnp.array(train[0].shape[-1])))\n",
    "\n",
    "    test_x_seq=test[0].shape[0]\n",
    "    test_x_len=int(jnp.prod(jnp.array(test[0].shape[1:-1])))\n",
    "    test_x_size=int(jnp.prod(jnp.array(train[0].shape[-1])))\n",
    "\n",
    "    if transformation:\n",
    "        train_x_size=24\n",
    "        test_x_size=24\n",
    "        train_x=vec_bin_array(train[0],8)\n",
    "        train_x=train_x.reshape((train_x_seq,train_x_len,test_x_size))\n",
    "\n",
    "        train_y=train[1].reshape(train_x_seq)\n",
    "        train_y_class=len(jnp.unique(train_y))\n",
    "\n",
    "        test_x=vec_bin_array(test[0],8)\n",
    "        test_x=test_x.reshape((test_x_seq,test_x_len,test_x_size))\n",
    "\n",
    "        test_y=test[1].reshape(test_x_seq)\n",
    "\n",
    "    else:\n",
    "        train_x=train[0].reshape((train_x_seq,train_x_len,train_x_size))/255\n",
    "        train_y=train[1].reshape(train_x_seq)\n",
    "        train_y_class=len(jnp.unique(train_y))\n",
    "        test_x=test[0].reshape((test_x_seq,test_x_len,test_x_size))/255\n",
    "        test_y=test[1].reshape(test_x_seq)\n",
    "        train_x_size=int(jnp.prod(jnp.array(train[0].shape[-1])))\n",
    "        test_x_size=int(jnp.prod(jnp.array(train[0].shape[-1])))\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2ofiQ9am8X_"
   },
   "outputs": [],
   "source": [
    "train_ds=tf.data.Dataset.from_tensor_slices((jnp.real(train_x),jnp.array(train_y,dtype=int)))\n",
    "test_ds=tf.data.Dataset.from_tensor_slices((jnp.real(test_x),jnp.array(test_y,dtype=int)))\n",
    "\n",
    "train_ds = train_ds.repeat().shuffle(100)\n",
    "\n",
    "# Group into batches of `batch_size` and skip incomplete batches, prefetch the next sample to improve latency.\n",
    "train_ds = train_ds.batch(batch_size, drop_remainder=True).take(train_steps).prefetch(1)\n",
    "# Group into batches of `batch_size` and skip incomplete batches, prefetch the next sample to improve latency.\n",
    "test_ds = test_ds.batch(batch_size, drop_remainder=True).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "t8KBq4Yy5j0B"
   },
   "outputs": [],
   "source": [
    "class MLP(nnx.Module):\n",
    "    # DON'T FORGET TO CHANGE THE MODEL NAME BEFORE RUNNING\n",
    "    # According to the scheme of the paper (Figure 1), input_size=M, encoded_size=H,layer_dim=number of neurons in MLP, out_dim=number of classes\n",
    "    def __init__(\n",
    "        self,\n",
    "        token_size,\n",
    "        token_len,\n",
    "        encoded_dim,\n",
    "        hidden_dim,\n",
    "        layer_dim,\n",
    "        out_dim,\n",
    "        rngs: nnx.Rngs,\n",
    "    ):\n",
    "\n",
    "        # linear encoder\n",
    "        # lrE,sigmaE=compute_lr_sigma(\"input\",token_size,encoded_dim,0,1)\n",
    "        # self.lin_encoder = nnx.Linear(in_features=token_size, out_features=encoded_dim,rngs=rngs,kernel_init=sigmaE*jax.random.normal)\n",
    "        self.lin_encoder = nnx.Linear(\n",
    "            in_features=token_size, out_features=encoded_dim, rngs=rngs\n",
    "        )\n",
    "\n",
    "        # LRU+MLP block\n",
    "        # lrL1, sigmaL1= compute_lr_sigma(\"input\",encoded_dim,layer_dim,0,1)\n",
    "        # lrL2, sigmaL2=compute_lr_sigma(\"output\",0,layer_dim,encoded_dim,1)\n",
    "        # self.linear1 = nnx.Linear(in_features=encoded_dim, out_features=layer_dim, rngs=rngs,kernel_init=sigmaL1*jax.random.normal))\n",
    "        # self.linear2 = nnx.Linear(in_features=layer_dim//2,out_features=encoded_dim,rngs=rngs,kernel_init=sigmaL2*jax.random.normal))\n",
    "        self.rnn1 = LRU(\n",
    "            in_features=encoded_dim,\n",
    "            hidden_features=hidden_dim,\n",
    "            r_min=r_min,\n",
    "            r_max=r_max,\n",
    "            max_phase=max_phase,\n",
    "        )\n",
    "        self.rnn2 = LRU(\n",
    "            in_features=encoded_dim,\n",
    "            hidden_features=hidden_dim,\n",
    "            r_min=r_min,\n",
    "            r_max=r_max,\n",
    "            max_phase=max_phase,\n",
    "        )\n",
    "        self.rnn3 = LRU(\n",
    "            in_features=encoded_dim,\n",
    "            hidden_features=hidden_dim,\n",
    "            r_min=r_min,\n",
    "            r_max=r_max,\n",
    "            max_phase=max_phase,\n",
    "        )\n",
    "        self.rnn4 = LRU(\n",
    "            in_features=encoded_dim,\n",
    "            hidden_features=hidden_dim,\n",
    "            r_min=r_min,\n",
    "            r_max=r_max,\n",
    "            max_phase=max_phase,\n",
    "        )\n",
    "        self.rnn5 = LRU(\n",
    "            in_features=encoded_dim,\n",
    "            hidden_features=hidden_dim,\n",
    "            r_min=r_min,\n",
    "            r_max=r_max,\n",
    "            max_phase=max_phase,\n",
    "        )\n",
    "        self.rnn6 = LRU(\n",
    "            in_features=encoded_dim,\n",
    "            hidden_features=hidden_dim,\n",
    "            r_min=r_min,\n",
    "            r_max=r_max,\n",
    "            max_phase=max_phase,\n",
    "        )\n",
    "        self.linear1_1 = nnx.Linear(\n",
    "            in_features=encoded_dim, out_features=layer_dim, rngs=rngs\n",
    "        )\n",
    "        self.linear1_2 = nnx.Linear(\n",
    "            in_features=layer_dim // 2, out_features=encoded_dim, rngs=rngs\n",
    "        )\n",
    "        self.linear2_1 = nnx.Linear(\n",
    "            in_features=encoded_dim, out_features=layer_dim, rngs=rngs\n",
    "        )\n",
    "        self.linear2_2 = nnx.Linear(\n",
    "            in_features=layer_dim // 2, out_features=encoded_dim, rngs=rngs\n",
    "        )\n",
    "        self.linear3_1 = nnx.Linear(\n",
    "            in_features=encoded_dim, out_features=layer_dim, rngs=rngs\n",
    "        )\n",
    "        self.linear3_2 = nnx.Linear(\n",
    "            in_features=layer_dim // 2, out_features=encoded_dim, rngs=rngs\n",
    "        )\n",
    "        self.linear4_1 = nnx.Linear(\n",
    "            in_features=encoded_dim, out_features=layer_dim, rngs=rngs\n",
    "        )\n",
    "        self.linear4_2 = nnx.Linear(\n",
    "            in_features=layer_dim // 2, out_features=encoded_dim, rngs=rngs\n",
    "        )\n",
    "        self.linear5_1 = nnx.Linear(\n",
    "            in_features=encoded_dim, out_features=layer_dim, rngs=rngs\n",
    "        )\n",
    "        self.linear5_2 = nnx.Linear(\n",
    "            in_features=layer_dim // 2, out_features=encoded_dim, rngs=rngs\n",
    "        )\n",
    "        self.linear6_1 = nnx.Linear(\n",
    "            in_features=encoded_dim, out_features=layer_dim, rngs=rngs\n",
    "        )\n",
    "        self.linear6_2 = nnx.Linear(\n",
    "            in_features=layer_dim // 2, out_features=encoded_dim, rngs=rngs\n",
    "        )\n",
    "        self.batchnorm1 = nnx.BatchNorm(\n",
    "            num_features=encoded_dim, rngs=rngs, use_running_average=True\n",
    "        )\n",
    "        self.batchnorm2 = nnx.BatchNorm(\n",
    "            num_features=encoded_dim, rngs=rngs, use_running_average=True\n",
    "        )\n",
    "        self.batchnorm3 = nnx.BatchNorm(\n",
    "            num_features=encoded_dim, rngs=rngs, use_running_average=True\n",
    "        )\n",
    "        self.batchnorm4 = nnx.BatchNorm(\n",
    "            num_features=encoded_dim, rngs=rngs, use_running_average=True\n",
    "        )\n",
    "        self.batchnorm5 = nnx.BatchNorm(\n",
    "            num_features=encoded_dim, rngs=rngs, use_running_average=True\n",
    "        )\n",
    "        self.batchnorm6 = nnx.BatchNorm(\n",
    "            num_features=encoded_dim, rngs=rngs, use_running_average=True\n",
    "        )\n",
    "\n",
    "        # Linear layers\n",
    "        if pool:  # If pooling layer takes the average over the token sequence length\n",
    "            self.linear3 = lambda x: jnp.mean(x, axis=1)\n",
    "        else:  # learn the parameters of the linear transformation\n",
    "            self.linear3 = nnx.Linear(in_features=token_len, out_features=1, rngs=rngs)\n",
    "        self.linear4 = nnx.Linear(\n",
    "            in_features=encoded_dim, out_features=out_dim, rngs=rngs\n",
    "        )\n",
    "        self.out_dim = out_dim\n",
    "        self.token_len = token_len\n",
    "        self.dropout = nnx.Dropout(dropout, rngs=rngs)\n",
    "\n",
    "    @nnx.vmap(in_axes=(None, 0,None))\n",
    "    def __call__(self, x,training):\n",
    "        x = self.lin_encoder(x)\n",
    "        # x=x@self.lin_encoder\n",
    "\n",
    "        y = x.copy()\n",
    "\n",
    "        # LRU+MLP block\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.rnn1(x)\n",
    "        x = self.linear1_1(x)\n",
    "        x = nnx.glu(x, axis=-1)\n",
    "        x = self.dropout(x,deterministic=not training)\n",
    "        x = self.linear1_2(x)\n",
    "        x += y  # Skip connection -> p.21 adding for each block\n",
    "\n",
    "        x = self.batchnorm2(x)  # batch normalization\n",
    "        x = self.rnn2(x)\n",
    "        x = self.linear2_1(x)\n",
    "        x = nnx.glu(x, axis=-1)\n",
    "        x = self.dropout(x,deterministic=not training)\n",
    "        x = self.linear2_2(x)\n",
    "        x += y  # Skip connection -> p.21 adding for each block\n",
    "\n",
    "        x = self.batchnorm3(x)  # batch normalization\n",
    "        x = self.rnn3(x)\n",
    "        x = self.linear3_1(x)\n",
    "        x = nnx.glu(x, axis=-1)\n",
    "        x = self.dropout(x,deterministic=not training)\n",
    "        x = self.linear3_2(x)\n",
    "        x += y  # Skip connection -> p.21 adding for each block\n",
    "\n",
    "        x = self.batchnorm4(x)  # batch normalization\n",
    "        x = self.rnn4(x)\n",
    "        x = self.linear4_1(x)\n",
    "        x = nnx.glu(x, axis=-1)\n",
    "        x = self.dropout(x,deterministic=not training)\n",
    "        x = self.linear4_2(x)\n",
    "        x += y  # Skip connection -> p.21 adding for each block\n",
    "\n",
    "        x = self.batchnorm5(x)  # batch normalization\n",
    "        x = self.rnn5(x)\n",
    "        x = self.linear5_1(x)\n",
    "        x = nnx.glu(x, axis=-1)\n",
    "        x = self.dropout(x,deterministic=not training)\n",
    "        x = self.linear5_2(x)\n",
    "        x += y  # Skip connection -> p.21 adding for each block\n",
    "\n",
    "        x = self.batchnorm6(x)  # batch normalization\n",
    "        x = self.rnn6(x)\n",
    "        x = self.linear6_1(x)\n",
    "        x = nnx.glu(x, axis=-1)\n",
    "        x = self.dropout(x,deterministic=not training)\n",
    "        x = self.linear6_2(x)\n",
    "        x += y  # Skip connection -> p.21 adding for each block\n",
    "\n",
    "        # x = x.T@self.weight #+ self.bias #project from L*H to H*1\n",
    "        # x = self.weight2@x #+ self.bias2#project from H*1 to out_dim\n",
    "        x = self.linear3(x.T)\n",
    "        x = self.linear4(x.T)\n",
    "        return x.reshape(self.out_dim)\n",
    "\n",
    "\n",
    "model = MLP(\n",
    "    train_x_size,\n",
    "    train_x_len,\n",
    "    encoded_size,\n",
    "    hidden_size,\n",
    "    hidden_neuron,\n",
    "    train_y_class,\n",
    "    rngs=rngs1,\n",
    ")\n",
    "\n",
    "#nnx.display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ur7p-0rlBbO"
   },
   "outputs": [],
   "source": [
    "def group_tuples_to_nested_dict(params):\n",
    "    nested_dict = {}\n",
    "    for outer_key, inner_key in params:\n",
    "        if outer_key not in nested_dict:\n",
    "            nested_dict[outer_key] = {}\n",
    "        nested_dict[outer_key][inner_key] = inner_key\n",
    "    return nested_dict\n",
    "param = nnx.state(model,nnx.Param).flat_state()\n",
    "gr=group_tuples_to_nested_dict(list(param.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wnzxIgYEkbNK"
   },
   "outputs": [],
   "source": [
    "#Set optimization method per layer\n",
    "rnn_lr=optax.warmup_cosine_decay_schedule(init_value=1e-7*lr_factor, peak_value=learning_rate*lr_factor, warmup_steps=train_steps//10, decay_steps=train_steps, end_value=1e-7*lr_factor)\n",
    "lin_lr=optax.warmup_cosine_decay_schedule(init_value=1e-7, peak_value=learning_rate, warmup_steps=train_steps//10, decay_steps=train_steps, end_value=1e-7)\n",
    "\n",
    "\n",
    "d={\"B_re\":optax.adamw(rnn_lr),\n",
    "   \"B_im\":optax.adamw(rnn_lr),\n",
    "    'C_im': optax.adamw(rnn_lr,weight_decay=0.05),\n",
    "    'C_re': optax.adamw(rnn_lr,weight_decay=0.05),\n",
    "    'D': optax.adamw(rnn_lr,weight_decay=0.05),\n",
    "    'gamma_log': optax.adamw(rnn_lr),\n",
    "    'nu_log': optax.adamw(rnn_lr),\n",
    "    'theta_log': optax.adamw(rnn_lr),\n",
    "     \"kernel\": optax.adamw(lin_lr,weight_decay=0.05),\n",
    "     \"bias\": optax.adamw(lin_lr,weight_decay=0.05),\n",
    "     \"scale\": optax.adamw(lin_lr,weight_decay=0.05),\n",
    "     }\n",
    "\n",
    "tx=optax.multi_transform(d,nnx.State(gr))\n",
    "\n",
    "optimizer = nnx.Optimizer(model, tx)\n",
    "metrics = nnx.MultiMetric(\n",
    "    accuracy=nnx.metrics.Accuracy(),\n",
    "    loss=nnx.metrics.Average(\"loss\"),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9sJOB_owm8YF"
   },
   "outputs": [],
   "source": [
    "def loss_fn(model: MLP, batch, training):\n",
    "  logits = model(batch[0],training)\n",
    "  loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "    logits=logits, labels=batch[1]\n",
    "  ).mean()\n",
    "  #print(logits.shape)\n",
    "  #print(batch[1].shape)\n",
    "  return loss, logits\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(model: MLP, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
    "  \"\"\"Train for a single step.\"\"\"\n",
    "  grad_fn = nnx.value_and_grad(loss_fn,has_aux=True)\n",
    "  (loss, logits), grads = grad_fn(model, batch,True)\n",
    "  metrics.update(loss=loss, logits=logits, labels=batch[1])  # In-place updates.\n",
    "  optimizer.update(grads)  # In-place updates.\n",
    "  predicted_labels = jnp.argmax(logits, axis=-1)\n",
    "  actual_labels = batch[1]\n",
    "  #jax.debug.print(\"Predictions: {}\",predicted_labels[:5].astype(int))\n",
    "  #jax.debug.print(\"Actual Labels: {}\",actual_labels[:5].astype(int))\n",
    "\n",
    "@nnx.jit\n",
    "def eval_step(model: MLP, metrics: nnx.MultiMetric, batch):\n",
    "  loss, logits = loss_fn(model, batch,False)\n",
    "  metrics.update(loss=loss, logits=logits, labels=batch[1])  # In-place updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ybC9wSr7m8YF",
    "outputId": "306bc630-8094-4776-db41-53f282b7e030"
   },
   "outputs": [],
   "source": [
    "#Train the model + evaluation with the test data\n",
    "metrics_history = {\n",
    "    'train_loss': [],\n",
    "    'train_accuracy': [],\n",
    "    'test_loss': [],\n",
    "    'test_accuracy': [],\n",
    "}\n",
    "\n",
    "for step, batch in enumerate(train_ds.as_numpy_iterator()):\n",
    "  # Run the optimization for one step and make a stateful update to the following:\n",
    "  # - The train state's model parameters\n",
    "  # - The optimizer state\n",
    "  # - The training loss and accuracy batch metrics\n",
    "  train_step(model, optimizer, metrics, batch)\n",
    "\n",
    "  if step > 0 and (step % eval_every == 0 or step == train_steps - 1):  # One training epoch has passed.\n",
    "    # Log the training metrics.\n",
    "    for metric, value in metrics.compute().items():  # Compute the metrics.\n",
    "      metrics_history[f'train_{metric}'].append(value)  # Record the metrics.\n",
    "    metrics.reset()  # Reset the metrics for the test set.\n",
    "\n",
    "    # Compute the metrics on the test set after each training epoch.\n",
    "    for test_batch in test_ds.as_numpy_iterator():\n",
    "      eval_step(model, metrics, test_batch)\n",
    "\n",
    "    # Log the test metrics.\n",
    "    for metric, value in metrics.compute().items():\n",
    "      metrics_history[f'test_{metric}'].append(value)\n",
    "    metrics.reset()  # Reset the metrics for the next training epoch.\n",
    "\n",
    "    print(\n",
    "      f\"[train] step: {step}       , \"\n",
    "      f\"loss: {metrics_history['train_loss'][-1]}      , \"\n",
    "      f\"accuracy: {metrics_history['train_accuracy'][-1] * 100}     \"\n",
    "    )\n",
    "    print(\n",
    "      f\"[test] step: {step}        , \"\n",
    "      f\"loss: {metrics_history['test_loss'][-1]}       , \"\n",
    "      f\"accuracy: {metrics_history['test_accuracy'][-1] * 100}      \"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXQS3ikfm8YG"
   },
   "outputs": [],
   "source": [
    "#Save the training results into csv\n",
    "import pandas as pd\n",
    "\n",
    "if leave_data:\n",
    "    data=pd.DataFrame({\"step\":np.arange(eval_every,train_steps+eval_every,eval_every),\"train_loss\":metrics_history['train_loss'],\n",
    "                       \"test_loss\":metrics_history['test_loss'],\"train_accuracy\":metrics_history['train_accuracy'],\n",
    "                       \"test_accuracy\":metrics_history['test_accuracy']})\n",
    "    data.to_csv(method_name+\"_H\"+str(encoded_size)+\"_nr\"+str(hidden_neuron)+\"_D\"+str(hidden_size)+\"_\"+dataset_name+\"_lr\"+str(learning_rate)+\"_step\"+str(train_steps)+\"r_min_\"+str(r_min)+\"r_max\"+str(r_max)+\"_rand\"+str(rand)+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZqz17Evm8YH"
   },
   "outputs": [],
   "source": [
    "#Plot the loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.arange(eval_every,train_steps+eval_every,eval_every),metrics_history['train_loss'],label=\"train loss\")\n",
    "plt.plot(np.arange(eval_every,train_steps+eval_every,eval_every),metrics_history['test_loss'],label=\"test loss\")\n",
    "plt.title(\"Train loss of \"+dataset_name+\" dataset with \"+method_name+\n",
    "              \", \\nhidden dimension=\"+str(hidden_size)+\", number of neuron=\"+str(hidden_neuron))\n",
    "#else:\n",
    "#    plt.title(\"Train loss of MNIST dataset with GRU+MLP, \\nhidden dimension=\"+str(hidden_size))\n",
    "plt.xlabel(\"Training step\")\n",
    "plt.ylabel(\"Train loss (cross entropy)\")\n",
    "plt.legend()\n",
    "if leave_data:\n",
    "    plt.savefig(\"loss_\"+method_name+\"_\"+str(encoded_size)+\"_\"+str(hidden_neuron)+\"_\"+dataset_name+\"_lr\"+str(learning_rate)+\"_step\"+str(train_steps)+\"r_min_\"+str(r_min)+\"r_max\"+str(r_max)+\"_rand\"+str(rand)+\".jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idfocImUm8YH"
   },
   "outputs": [],
   "source": [
    "#Plot the accuracy\n",
    "plt.plot(np.arange(eval_every,train_steps+eval_every,eval_every),metrics_history['train_accuracy'],label=\"train\")\n",
    "plt.plot(np.arange(eval_every,train_steps+eval_every,eval_every),metrics_history['test_accuracy'],label=\"test\")\n",
    "plt.title(\"Accuracy of \"+dataset_name+\" dataset with \"+method_name+\", \\nhidden dimension=\"+\n",
    "          str(hidden_size)+\", number of neuron=\"+str(hidden_neuron))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "if leave_data:\n",
    "    plt.savefig(\"accuracy_\"+method_name+\"_\"+str(encoded_size)+\"_\"+str(hidden_neuron)+\"_\"+dataset_name+\"_lr\"+str(learning_rate)+\"_step\"+str(train_steps)+\"r_min_\"+str(r_min)+\"r_max\"+str(r_max)+\"_rand\"+str(rand)+\".jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pY7u9HdXdETB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
