{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flax --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jax --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if version >= 0.4.35\n",
    "!pip show flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if version >= 0.4.35\n",
    "!pip show jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EWLRsVQum8X3"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds  # TFDS to download MNIST.\n",
    "import tensorflow as tf  # TensorFlow / `tf.data` operations.\n",
    "from flax import nnx  # The Flax NNX API.\n",
    "import jax.numpy as jnp  # JAX NumPy\n",
    "import jax\n",
    "import optax\n",
    "import copy\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change name of the parameters for layer-wise parameterization of the learning rate\n",
    "from __future__ import annotations\n",
    "\n",
    "import typing as tp\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from jax import lax\n",
    "import opt_einsum\n",
    "\n",
    "from flax.core.frozen_dict import FrozenDict\n",
    "from flax import nnx\n",
    "from flax.nnx import rnglib, variablelib\n",
    "from flax.nnx.module import Module, first_from\n",
    "from flax.nnx.nn import dtypes, initializers\n",
    "from flax.typing import (\n",
    "    Dtype,\n",
    "    Shape,\n",
    "    Initializer,\n",
    "    PrecisionLike,\n",
    "    DotGeneralT,\n",
    "    ConvGeneralDilatedT,\n",
    "    PaddingLike,\n",
    "    LaxPadding,\n",
    ")\n",
    "\n",
    "Array = jax.Array\n",
    "Axis = int\n",
    "Size = int\n",
    "\n",
    "\n",
    "default_kernel_init = initializers.lecun_normal()\n",
    "default_bias_init = initializers.zeros_init()\n",
    "\n",
    "\n",
    "class Linear_encoder(Module):\n",
    "    \"\"\"A linear transformation applied over the last dimension of the input.\n",
    "\n",
    "    Example usage::\n",
    "\n",
    "      >>> from flax import nnx\n",
    "      >>> import jax, jax.numpy as jnp\n",
    "\n",
    "      >>> layer = nnx.Linear(in_features=3, out_features=4, rngs=nnx.Rngs(0))\n",
    "      >>> jax.tree.map(jnp.shape, nnx.state(layer))\n",
    "      State({\n",
    "        'bias': VariableState(\n",
    "          type=Param,\n",
    "          value=(4,)\n",
    "        ),\n",
    "        'kernel': VariableState(\n",
    "          type=Param,\n",
    "          value=(3, 4)\n",
    "        )\n",
    "      })\n",
    "\n",
    "    Attributes:\n",
    "      in_features: the number of input features.\n",
    "      out_features: the number of output features.\n",
    "      use_bias: whether to add a bias to the output (default: True).\n",
    "      dtype: the dtype of the computation (default: infer from input and params).\n",
    "      param_dtype: the dtype passed to parameter initializers (default: float32).\n",
    "      precision: numerical precision of the computation see ``jax.lax.Precision``\n",
    "        for details.\n",
    "      kernel_init: initializer function for the weight matrix.\n",
    "      bias_init: initializer function for the bias.\n",
    "      dot_general: dot product function.\n",
    "      rngs: rng key.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        *,\n",
    "        use_bias: bool = True,\n",
    "        dtype: tp.Optional[Dtype] = None,\n",
    "        param_dtype: Dtype = jnp.float32,\n",
    "        precision: PrecisionLike = None,\n",
    "        kernel_init: Initializer = default_kernel_init,\n",
    "        bias_init: Initializer = default_bias_init,\n",
    "        dot_general: DotGeneralT = lax.dot_general,\n",
    "        rngs: rnglib.Rngs,\n",
    "    ):\n",
    "        kernel_key = rngs.params()\n",
    "        self.lin_encoder_kernel = nnx.Param(\n",
    "            kernel_init(kernel_key, (in_features, out_features), param_dtype)\n",
    "        )\n",
    "        if use_bias:\n",
    "            bias_key = rngs.params()\n",
    "            self.lin_encoder_bias = nnx.Param(\n",
    "                bias_init(bias_key, (out_features,), param_dtype)\n",
    "            )\n",
    "        else:\n",
    "            self.lin_encoder_bias = nnx.Param(None)\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.use_bias = use_bias\n",
    "        self.dtype = dtype\n",
    "        self.param_dtype = param_dtype\n",
    "        self.precision = precision\n",
    "        self.kernel_init = kernel_init\n",
    "        self.bias_init = bias_init\n",
    "        self.dot_general = dot_general\n",
    "\n",
    "    def __call__(self, inputs: Array) -> Array:\n",
    "        \"\"\"Applies a linear transformation to the inputs along the last dimension.\n",
    "\n",
    "        Args:\n",
    "          inputs: The nd-array to be transformed.\n",
    "\n",
    "        Returns:\n",
    "          The transformed input.\n",
    "        \"\"\"\n",
    "        kernel = self.lin_encoder_kernel.value\n",
    "        bias = self.lin_encoder_bias.value\n",
    "\n",
    "        inputs, kernel, bias = dtypes.promote_dtype(\n",
    "            (inputs, kernel, bias), dtype=self.dtype\n",
    "        )\n",
    "        y = self.dot_general(\n",
    "            inputs,\n",
    "            kernel,\n",
    "            (((inputs.ndim - 1,), (0,)), ((), ())),\n",
    "            precision=self.precision,\n",
    "        )\n",
    "        assert self.use_bias == (bias is not None)\n",
    "        if bias is not None:\n",
    "            y += jnp.reshape(bias, (1,) * (y.ndim - 1) + (-1,))\n",
    "        return y\n",
    "\n",
    "\n",
    "class Linear_MLP1(Module):\n",
    "    \"\"\"A linear transformation applied over the last dimension of the input.\n",
    "\n",
    "    Example usage::\n",
    "\n",
    "      >>> from flax import nnx\n",
    "      >>> import jax, jax.numpy as jnp\n",
    "\n",
    "      >>> layer = nnx.Linear(in_features=3, out_features=4, rngs=nnx.Rngs(0))\n",
    "      >>> jax.tree.map(jnp.shape, nnx.state(layer))\n",
    "      State({\n",
    "        'bias': VariableState(\n",
    "          type=Param,\n",
    "          value=(4,)\n",
    "        ),\n",
    "        'kernel': VariableState(\n",
    "          type=Param,\n",
    "          value=(3, 4)\n",
    "        )\n",
    "      })\n",
    "\n",
    "    Attributes:\n",
    "      in_features: the number of input features.\n",
    "      out_features: the number of output features.\n",
    "      use_bias: whether to add a bias to the output (default: True).\n",
    "      dtype: the dtype of the computation (default: infer from input and params).\n",
    "      param_dtype: the dtype passed to parameter initializers (default: float32).\n",
    "      precision: numerical precision of the computation see ``jax.lax.Precision``\n",
    "        for details.\n",
    "      kernel_init: initializer function for the weight matrix.\n",
    "      bias_init: initializer function for the bias.\n",
    "      dot_general: dot product function.\n",
    "      rngs: rng key.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        *,\n",
    "        use_bias: bool = True,\n",
    "        dtype: tp.Optional[Dtype] = None,\n",
    "        param_dtype: Dtype = jnp.float32,\n",
    "        precision: PrecisionLike = None,\n",
    "        kernel_init: Initializer = default_kernel_init,\n",
    "        bias_init: Initializer = default_bias_init,\n",
    "        dot_general: DotGeneralT = lax.dot_general,\n",
    "        rngs: rnglib.Rngs,\n",
    "    ):\n",
    "        kernel_key = rngs.params()\n",
    "        self.MLP1_kernel = nnx.Param(\n",
    "            kernel_init(kernel_key, (in_features, out_features), param_dtype)\n",
    "        )\n",
    "        if use_bias:\n",
    "            bias_key = rngs.params()\n",
    "            self.MLP1_bias = nnx.Param(\n",
    "                bias_init(bias_key, (out_features,), param_dtype)\n",
    "            )\n",
    "        else:\n",
    "            self.MLP1_bias = nnx.Param(None)\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.use_bias = use_bias\n",
    "        self.dtype = dtype\n",
    "        self.param_dtype = param_dtype\n",
    "        self.precision = precision\n",
    "        self.kernel_init = kernel_init\n",
    "        self.bias_init = bias_init\n",
    "        self.dot_general = dot_general\n",
    "\n",
    "    def __call__(self, inputs: Array) -> Array:\n",
    "        \"\"\"Applies a linear transformation to the inputs along the last dimension.\n",
    "\n",
    "        Args:\n",
    "          inputs: The nd-array to be transformed.\n",
    "\n",
    "        Returns:\n",
    "          The transformed input.\n",
    "        \"\"\"\n",
    "        kernel = self.MLP1_kernel.value\n",
    "        bias = self.MLP1_bias.value\n",
    "\n",
    "        inputs, kernel, bias = dtypes.promote_dtype(\n",
    "            (inputs, kernel, bias), dtype=self.dtype\n",
    "        )\n",
    "        y = self.dot_general(\n",
    "            inputs,\n",
    "            kernel,\n",
    "            (((inputs.ndim - 1,), (0,)), ((), ())),\n",
    "            precision=self.precision,\n",
    "        )\n",
    "        assert self.use_bias == (bias is not None)\n",
    "        if bias is not None:\n",
    "            y += jnp.reshape(bias, (1,) * (y.ndim - 1) + (-1,))\n",
    "        return y\n",
    "\n",
    "\n",
    "class Linear_MLP2(Module):\n",
    "    \"\"\"A linear transformation applied over the last dimension of the input.\n",
    "\n",
    "    Example usage::\n",
    "\n",
    "      >>> from flax import nnx\n",
    "      >>> import jax, jax.numpy as jnp\n",
    "\n",
    "      >>> layer = nnx.Linear(in_features=3, out_features=4, rngs=nnx.Rngs(0))\n",
    "      >>> jax.tree.map(jnp.shape, nnx.state(layer))\n",
    "      State({\n",
    "        'bias': VariableState(\n",
    "          type=Param,\n",
    "          value=(4,)\n",
    "        ),\n",
    "        'kernel': VariableState(\n",
    "          type=Param,\n",
    "          value=(3, 4)\n",
    "        )\n",
    "      })\n",
    "\n",
    "    Attributes:\n",
    "      in_features: the number of input features.\n",
    "      out_features: the number of output features.\n",
    "      use_bias: whether to add a bias to the output (default: True).\n",
    "      dtype: the dtype of the computation (default: infer from input and params).\n",
    "      param_dtype: the dtype passed to parameter initializers (default: float32).\n",
    "      precision: numerical precision of the computation see ``jax.lax.Precision``\n",
    "        for details.\n",
    "      kernel_init: initializer function for the weight matrix.\n",
    "      bias_init: initializer function for the bias.\n",
    "      dot_general: dot product function.\n",
    "      rngs: rng key.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        *,\n",
    "        use_bias: bool = True,\n",
    "        dtype: tp.Optional[Dtype] = None,\n",
    "        param_dtype: Dtype = jnp.float32,\n",
    "        precision: PrecisionLike = None,\n",
    "        kernel_init: Initializer = default_kernel_init,\n",
    "        bias_init: Initializer = default_bias_init,\n",
    "        dot_general: DotGeneralT = lax.dot_general,\n",
    "        rngs: rnglib.Rngs,\n",
    "    ):\n",
    "        kernel_key = rngs.params()\n",
    "        self.MLP2_kernel = nnx.Param(\n",
    "            kernel_init(kernel_key, (in_features, out_features), param_dtype)\n",
    "        )\n",
    "        if use_bias:\n",
    "            bias_key = rngs.params()\n",
    "            self.MLP2_bias = nnx.Param(\n",
    "                bias_init(bias_key, (out_features,), param_dtype)\n",
    "            )\n",
    "        else:\n",
    "            self.MLP2_bias = nnx.Param(None)\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.use_bias = use_bias\n",
    "        self.dtype = dtype\n",
    "        self.param_dtype = param_dtype\n",
    "        self.precision = precision\n",
    "        self.kernel_init = kernel_init\n",
    "        self.bias_init = bias_init\n",
    "        self.dot_general = dot_general\n",
    "\n",
    "    def __call__(self, inputs: Array) -> Array:\n",
    "        \"\"\"Applies a linear transformation to the inputs along the last dimension.\n",
    "\n",
    "        Args:\n",
    "          inputs: The nd-array to be transformed.\n",
    "\n",
    "        Returns:\n",
    "          The transformed input.\n",
    "        \"\"\"\n",
    "        kernel = self.MLP2_kernel.value\n",
    "        bias = self.MLP2_bias.value\n",
    "\n",
    "        inputs, kernel, bias = dtypes.promote_dtype(\n",
    "            (inputs, kernel, bias), dtype=self.dtype\n",
    "        )\n",
    "        y = self.dot_general(\n",
    "            inputs,\n",
    "            kernel,\n",
    "            (((inputs.ndim - 1,), (0,)), ((), ())),\n",
    "            precision=self.precision,\n",
    "        )\n",
    "        assert self.use_bias == (bias is not None)\n",
    "        if bias is not None:\n",
    "            y += jnp.reshape(bias, (1,) * (y.ndim - 1) + (-1,))\n",
    "        return y\n",
    "\n",
    "\n",
    "class Linear_out(Module):\n",
    "    \"\"\"A linear transformation applied over the last dimension of the input.\n",
    "\n",
    "    Example usage::\n",
    "\n",
    "      >>> from flax import nnx\n",
    "      >>> import jax, jax.numpy as jnp\n",
    "\n",
    "      >>> layer = nnx.Linear(in_features=3, out_features=4, rngs=nnx.Rngs(0))\n",
    "      >>> jax.tree.map(jnp.shape, nnx.state(layer))\n",
    "      State({\n",
    "        'bias': VariableState(\n",
    "          type=Param,\n",
    "          value=(4,)\n",
    "        ),\n",
    "        'kernel': VariableState(\n",
    "          type=Param,\n",
    "          value=(3, 4)\n",
    "        )\n",
    "      })\n",
    "\n",
    "    Attributes:\n",
    "      in_features: the number of input features.\n",
    "      out_features: the number of output features.\n",
    "      use_bias: whether to add a bias to the output (default: True).\n",
    "      dtype: the dtype of the computation (default: infer from input and params).\n",
    "      param_dtype: the dtype passed to parameter initializers (default: float32).\n",
    "      precision: numerical precision of the computation see ``jax.lax.Precision``\n",
    "        for details.\n",
    "      kernel_init: initializer function for the weight matrix.\n",
    "      bias_init: initializer function for the bias.\n",
    "      dot_general: dot product function.\n",
    "      rngs: rng key.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        *,\n",
    "        use_bias: bool = True,\n",
    "        dtype: tp.Optional[Dtype] = None,\n",
    "        param_dtype: Dtype = jnp.float32,\n",
    "        precision: PrecisionLike = None,\n",
    "        kernel_init: Initializer = default_kernel_init,\n",
    "        bias_init: Initializer = default_bias_init,\n",
    "        dot_general: DotGeneralT = lax.dot_general,\n",
    "        rngs: rnglib.Rngs,\n",
    "    ):\n",
    "        kernel_key = rngs.params()\n",
    "        self.out_kernel = nnx.Param(\n",
    "            kernel_init(kernel_key, (in_features, out_features), param_dtype)\n",
    "        )\n",
    "        if use_bias:\n",
    "            bias_key = rngs.params()\n",
    "            self.out_bias = nnx.Param(bias_init(bias_key, (out_features,), param_dtype))\n",
    "        else:\n",
    "            self.out_bias = nnx.Param(None)\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.use_bias = use_bias\n",
    "        self.dtype = dtype\n",
    "        self.param_dtype = param_dtype\n",
    "        self.precision = precision\n",
    "        self.kernel_init = kernel_init\n",
    "        self.bias_init = bias_init\n",
    "        self.dot_general = dot_general\n",
    "\n",
    "    def __call__(self, inputs: Array) -> Array:\n",
    "        \"\"\"Applies a linear transformation to the inputs along the last dimension.\n",
    "\n",
    "        Args:\n",
    "          inputs: The nd-array to be transformed.\n",
    "\n",
    "        Returns:\n",
    "          The transformed input.\n",
    "        \"\"\"\n",
    "        kernel = self.out_kernel.value\n",
    "        bias = self.out_bias.value\n",
    "\n",
    "        inputs, kernel, bias = dtypes.promote_dtype(\n",
    "            (inputs, kernel, bias), dtype=self.dtype\n",
    "        )\n",
    "        y = self.dot_general(\n",
    "            inputs,\n",
    "            kernel,\n",
    "            (((inputs.ndim - 1,), (0,)), ((), ())),\n",
    "            precision=self.precision,\n",
    "        )\n",
    "        assert self.use_bias == (bias is not None)\n",
    "        if bias is not None:\n",
    "            y += jnp.reshape(bias, (1,) * (y.ndim - 1) + (-1,))\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XY6XHBLHnBXx"
   },
   "outputs": [],
   "source": [
    "\n",
    "parallel_scan = jax.lax.associative_scan\n",
    "\n",
    "# From Orvieto et al., 2023, (https://arxiv.org/abs/2303.06349)\n",
    "def compute_lr_sigma(mode: str, d, m, k, L):\n",
    "    lr = 0\n",
    "    sigma = 0\n",
    "    if mode == \"input\":\n",
    "        lr = m / (jnp.power(L, 3 / 2) * d)\n",
    "        sigma = 1 / jnp.sqrt(d)\n",
    "    elif mode == \"hidden\":\n",
    "        lr = 1 / jnp.power(L, 3 / 2)\n",
    "        sigma = 2 / jnp.sqrt((m + d) / 2)\n",
    "    elif mode == \"output\":\n",
    "        lr = k / (jnp.power(L, 3 / 2) * m)\n",
    "        sigma = jnp.sqrt(k) / m\n",
    "    else:\n",
    "        raise ValueError\n",
    "    return float(lr), float(sigma)\n",
    "\n",
    "def forward(lru_parameters, input_sequence):\n",
    "    \"\"\"Forward pass of the LRU layer. Output y and input_sequence are of shape (L, H).\"\"\"\n",
    "\n",
    "    # All LRU parameters\n",
    "    nu_log, theta_log, B_re, B_im, C_re, C_im, D, gamma_log = lru_parameters\n",
    "\n",
    "    # Materializing the diagonal of Lambda and projections\n",
    "    Lambda = jnp.exp(-jnp.exp(nu_log) + 1j * jnp.exp(theta_log))\n",
    "    B_norm = (B_re + 1j * B_im) * jnp.expand_dims(jnp.exp(gamma_log), axis=-1)\n",
    "    C = C_re + 1j * C_im\n",
    "\n",
    "    # Running the LRU + output projection\n",
    "    # For details on parallel scan, check discussion in Smith et al (2022).\n",
    "    Lambda_elements = jnp.repeat(Lambda[None, ...], input_sequence.shape[0], axis=0)\n",
    "    Bu_elements = jax.vmap(lambda u: B_norm @ u)(input_sequence)\n",
    "    elements = (Lambda_elements, Bu_elements)\n",
    "    _, inner_states = parallel_scan(binary_operator_diag, elements)  # all x_k\n",
    "    y = jax.vmap(lambda x, u: (C @ x).real + D * u)(inner_states, input_sequence)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def init_lru_parameters(N, H, r_min=0, r_max=1, max_phase=0.314):\n",
    "    \"\"\"Initialize parameters of the LRU layer.\"\"\"\n",
    "\n",
    "    # N: state dimension, H: model dimension\n",
    "    # Initialization of Lambda is complex valued distributed uniformly on ring\n",
    "    # between r_min and r_max, with phase in [0, max_phase].\n",
    "    u1 = np.random.uniform(size=(N,))\n",
    "    u2 = np.random.uniform(size=(N,))\n",
    "    nu_log = np.log(-0.5 * np.log(u1 * (r_max**2 - r_min**2) + r_min**2))\n",
    "    theta_log = np.log(max_phase * u2)\n",
    "\n",
    "    # Glorot initialized Input/Output projection matrices\n",
    "    B_re = np.random.normal(size=(N, H)) / np.sqrt(2 * H)\n",
    "    B_im = np.random.normal(size=(N, H)) / np.sqrt(2 * H)\n",
    "    C_re = np.random.normal(size=(H, N)) / np.sqrt(N)\n",
    "    C_im = np.random.normal(size=(H, N)) / np.sqrt(N)\n",
    "    D = np.random.normal(size=(H,))\n",
    "\n",
    "    # Normalization factor\n",
    "    diag_lambda = np.exp(-np.exp(nu_log) + 1j * np.exp(theta_log))\n",
    "    gamma_log = np.log(np.sqrt(1 - np.abs(diag_lambda) ** 2))\n",
    "\n",
    "    return nu_log, theta_log, B_re, B_im, C_re, C_im, D, gamma_log\n",
    "\n",
    "\n",
    "def binary_operator_diag(element_i, element_j):\n",
    "    # Binary operator for parallel scan of linear recurrence.\n",
    "    a_i, bu_i = element_i\n",
    "    a_j, bu_j = element_j\n",
    "    return a_j * a_i, a_j * bu_i + bu_j\n",
    "\n",
    "\n",
    "Array = jax.Array\n",
    "\n",
    "\n",
    "class LRU(nnx.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        hidden_features: int,  # not inferred from carry for now\n",
    "        *,\n",
    "        r_min=0,\n",
    "        r_max=1,\n",
    "        max_phase=6.28,\n",
    "    ):\n",
    "        self.in_features = in_features\n",
    "        self.hidden_features = hidden_features\n",
    "        nu_log, theta_log, B_re, B_im, C_re, C_im, D, gamma_log = init_lru_parameters(\n",
    "            hidden_features, in_features, r_min=r_min, r_max=r_max, max_phase=max_phase\n",
    "        )\n",
    "\n",
    "        self.nu_log = nnx.Param(nu_log)\n",
    "        self.theta_log = nnx.Param(theta_log)\n",
    "        self.B_re = nnx.Param(B_re)\n",
    "        self.B_im = nnx.Param(B_im)\n",
    "        self.C_re = nnx.Param(C_re)\n",
    "        self.C_im = nnx.Param(C_im)\n",
    "        self.D = nnx.Param(D)\n",
    "        self.gamma_log = nnx.Param(gamma_log)\n",
    "\n",
    "    def __call__(self, inputs: Array):  # type: ignore[override]\n",
    "        # jax.debug.print(\"test:{}\", jnp.sin(self.nu_log + self.theta_log))\n",
    "        Lambda = jnp.exp(\n",
    "            -jnp.exp(self.nu_log.value) + 1j * jnp.exp(self.theta_log.value)\n",
    "        )\n",
    "        B_norm = (self.B_re.value + 1j * self.B_im.value) * jnp.expand_dims(\n",
    "            jnp.exp(self.gamma_log.value), axis=-1\n",
    "        )\n",
    "        # Running the LRU + output projection\n",
    "        # For details on parallel scan, check discussion in Smith et al (2022).\n",
    "        Lambda_elements = jnp.repeat(Lambda[None, ...], inputs.shape[0], axis=0)\n",
    "        Bu_elements = jax.vmap(lambda u: B_norm @ u)(inputs)\n",
    "        elements = (Lambda_elements, Bu_elements)\n",
    "        C = self.C_re + 1j * self.C_im\n",
    "        _, h = parallel_scan(binary_operator_diag, elements)\n",
    "        y = jax.vmap(lambda x, u: (C @ x).real + self.D * u)(h, inputs)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Het4U2Cm8X7"
   },
   "outputs": [],
   "source": [
    "rnn=1 #rnn=0:transformation of the inputs with fixed RNN weights, rnn=1: adding the RNN module on the model to learn the weight matrices\n",
    "pool=0 #pooling layer after MLP is taking the average over the numbers\n",
    "transformation=0 #transformation of the data from decimals between 0 and 256 to binary 8 bit numbers\n",
    "leave_data=1 #download csv data of the results\n",
    "hidden_neuron=384 #no details in the 2023 paper => 2024 paper fixed to 512 with LV system\n",
    "encoded_size=512\n",
    "hidden_size=384\n",
    "learning_rate = 1e-3\n",
    "momentum = 0.9\n",
    "train_steps=30000\n",
    "eval_every = 50\n",
    "batch_size=50\n",
    "r_min = 0.9\n",
    "r_max = 0.999\n",
    "max_phase = 6.28\n",
    "depth=6\n",
    "lr_factor=0.25\n",
    "dropout=0.1\n",
    "method_name=\"LRUMLP6\"\n",
    "dataset_name=\"CIFAR10\"\n",
    "folder_name=\"step30klayer\"\n",
    "rand=random.randint(0,10000)\n",
    "rngs1=nnx.Rngs(rand)\n",
    "print(rand)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BEhCTOMq7tBJ",
    "outputId": "fc8b8a6b-1146-40a0-ac96-1966fbaa8b0d"
   },
   "outputs": [],
   "source": [
    "def vec_bin_array(arr, m): #https://stackoverflow.com/questions/22227595/convert-integer-to-binary-array-with-suitable-padding\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    arr: Numpy array of positive integers\n",
    "    m: Number of bits of each integer to retain\n",
    "\n",
    "    Returns a copy of arr with every element replaced with a bit vector.\n",
    "    Bits encoded as int8's.12\n",
    "    \"\"\"\n",
    "\n",
    "    to_str_func = np.vectorize(lambda x: np.binary_repr(x).zfill(m))\n",
    "    strs = to_str_func(arr)\n",
    "    ret = np.zeros(list(arr.shape) + [m], dtype=np.int8)\n",
    "    for bit_ix in range(0, m):\n",
    "        fetch_bit_func = np.vectorize(lambda x: x[bit_ix] == '1')\n",
    "        ret[...,bit_ix] = fetch_bit_func(strs).astype(\"int8\")\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "#Import data\n",
    "\n",
    "if dataset_name==\"MNIST\":\n",
    "    dataset=tf.keras.datasets.mnist.load_data()\n",
    "    train=dataset[0]\n",
    "    test=dataset[1]\n",
    "\n",
    "    train_x_seq=train[0].shape[0]\n",
    "    train_x_len=int(jnp.prod(jnp.array(train[0].shape[1:])))\n",
    "    test_x_seq=test[0].shape[0]\n",
    "    test_x_len=int(jnp.prod(jnp.array(test[0].shape[1:])))\n",
    "    if transformation:\n",
    "        train_x_size=8\n",
    "        test_x_size=8\n",
    "\n",
    "        train_x=vec_bin_array(train[0],train_x_size)\n",
    "        train_x=train_x.reshape((train_x_seq,train_x_len,train_x_size))\n",
    "\n",
    "        train_y=train[1].reshape(train_x_seq)\n",
    "\n",
    "        train_y_class=len(jnp.unique(train_y))\n",
    "\n",
    "        test_x=vec_bin_array(test[0],test_x_size)\n",
    "        test_x=test_x.reshape((test_x_seq,test_x_len,test_x_size))\n",
    "\n",
    "        test_y=test[1].reshape(test_x_seq)\n",
    "\n",
    "    else:\n",
    "        train_x_size=1\n",
    "        test_x_size=1\n",
    "        train_x=train[0].reshape((train_x_seq,train_x_len,train_x_size))/255\n",
    "        train_y=train[1].reshape(train_x_seq)\n",
    "        train_y_class=len(jnp.unique(train_y))\n",
    "        test_x=test[0].reshape((test_x_seq,test_x_len,test_x_size))/255\n",
    "        test_y=test[1].reshape(test_x_seq)\n",
    "\n",
    "\n",
    "if dataset_name==\"CIFAR10\":\n",
    "    dataset=tf.keras.datasets.cifar10.load_data()\n",
    "    train=dataset[0]\n",
    "    test=dataset[1]\n",
    "\n",
    "    train_x_seq=train[0].shape[0]\n",
    "    train_x_len=int(jnp.prod(jnp.array(train[0].shape[1:-1])))\n",
    "    train_x_size=int(jnp.prod(jnp.array(train[0].shape[-1])))\n",
    "\n",
    "    test_x_seq=test[0].shape[0]\n",
    "    test_x_len=int(jnp.prod(jnp.array(test[0].shape[1:-1])))\n",
    "    test_x_size=int(jnp.prod(jnp.array(train[0].shape[-1])))\n",
    "\n",
    "    if transformation:\n",
    "        train_x_size=24\n",
    "        test_x_size=24\n",
    "        train_x=vec_bin_array(train[0],8)\n",
    "        train_x=train_x.reshape((train_x_seq,train_x_len,test_x_size))\n",
    "\n",
    "        train_y=train[1].reshape(train_x_seq)\n",
    "        train_y_class=len(jnp.unique(train_y))\n",
    "\n",
    "        test_x=vec_bin_array(test[0],8)\n",
    "        test_x=test_x.reshape((test_x_seq,test_x_len,test_x_size))\n",
    "\n",
    "        test_y=test[1].reshape(test_x_seq)\n",
    "\n",
    "    else:\n",
    "        train_x=train[0].reshape((train_x_seq,train_x_len,train_x_size))/255\n",
    "        train_y=train[1].reshape(train_x_seq)\n",
    "        train_y_class=len(jnp.unique(train_y))\n",
    "        test_x=test[0].reshape((test_x_seq,test_x_len,test_x_size))/255\n",
    "        test_y=test[1].reshape(test_x_seq)\n",
    "        train_x_size=int(jnp.prod(jnp.array(train[0].shape[-1])))\n",
    "        test_x_size=int(jnp.prod(jnp.array(train[0].shape[-1])))\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2ofiQ9am8X_"
   },
   "outputs": [],
   "source": [
    "train_ds=tf.data.Dataset.from_tensor_slices((jnp.real(train_x),jnp.array(train_y,dtype=int)))\n",
    "test_ds=tf.data.Dataset.from_tensor_slices((jnp.real(test_x),jnp.array(test_y,dtype=int)))\n",
    "\n",
    "train_ds = train_ds.repeat().shuffle(100)\n",
    "\n",
    "# Group into batches of `batch_size` and skip incomplete batches, prefetch the next sample to improve latency.\n",
    "train_ds = train_ds.batch(batch_size, drop_remainder=True).take(train_steps).prefetch(1)\n",
    "# Group into batches of `batch_size` and skip incomplete batches, prefetch the next sample to improve latency.\n",
    "test_ds = test_ds.batch(batch_size, drop_remainder=True).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_encoder_lr,lin_encoder_sigma=compute_lr_sigma(mode=\"input\",d=train_x_size,m=encoded_size,k=0,L=1)\n",
    "MLP1_lr,MLP1_sigma=compute_lr_sigma(\"input\",encoded_size,hidden_neuron//2,0,1)\n",
    "MLP2_lr,MLP2_sigma=compute_lr_sigma(\"output\",0,hidden_neuron//2,encoded_size,1)\n",
    "out_lr,out_sigma=compute_lr_sigma(\"output\",encoded_size,encoded_size,train_y_class,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "t8KBq4Yy5j0B"
   },
   "outputs": [],
   "source": [
    "class MLP(nnx.Module):\n",
    "    # DON'T FORGET TO CHANGE THE MODEL NAME BEFORE RUNNING\n",
    "    # According to the scheme of the paper (Figure 1), input_size=M, encoded_size=H,layer_dim=number of neurons in MLP, out_dim=number of classes\n",
    "    def __init__(\n",
    "        self,\n",
    "        token_size,\n",
    "        token_len,\n",
    "        encoded_dim,\n",
    "        hidden_dim,\n",
    "        layer_dim,\n",
    "        out_dim,\n",
    "        rngs: nnx.Rngs,\n",
    "    ):\n",
    "\n",
    "        self.lin_encoder = Linear_encoder(in_features=token_size, out_features=encoded_dim, rngs=rngs,kernel_init=jax.nn.initializers.variance_scaling(scale=lin_encoder_sigma,mode=\"fan_in\",distribution=\"truncated_normal\"))\n",
    "\n",
    "        self.rnn1 = LRU(\n",
    "            in_features=encoded_dim,\n",
    "            hidden_features=hidden_dim,\n",
    "            r_min=r_min,\n",
    "            r_max=r_max,\n",
    "            max_phase=max_phase,\n",
    "        )\n",
    "        self.rnn2 = LRU(\n",
    "            in_features=encoded_dim,\n",
    "            hidden_features=hidden_dim,\n",
    "            r_min=r_min,\n",
    "            r_max=r_max,\n",
    "            max_phase=max_phase,\n",
    "        )\n",
    "        self.rnn3 = LRU(\n",
    "            in_features=encoded_dim,\n",
    "            hidden_features=hidden_dim,\n",
    "            r_min=r_min,\n",
    "            r_max=r_max,\n",
    "            max_phase=max_phase,\n",
    "        )\n",
    "        self.rnn4 = LRU(\n",
    "            in_features=encoded_dim,\n",
    "            hidden_features=hidden_dim,\n",
    "            r_min=r_min,\n",
    "            r_max=r_max,\n",
    "            max_phase=max_phase,\n",
    "        )\n",
    "        self.rnn5 = LRU(\n",
    "            in_features=encoded_dim,\n",
    "            hidden_features=hidden_dim,\n",
    "            r_min=r_min,\n",
    "            r_max=r_max,\n",
    "            max_phase=max_phase,\n",
    "        )\n",
    "        self.rnn6 = LRU(\n",
    "            in_features=encoded_dim,\n",
    "            hidden_features=hidden_dim,\n",
    "            r_min=r_min,\n",
    "            r_max=r_max,\n",
    "            max_phase=max_phase,\n",
    "        )\n",
    "        self.linear1_1 = Linear_MLP1(in_features=encoded_dim, out_features=layer_dim, rngs=rngs,kernel_init=jax.nn.initializers.variance_scaling(scale=MLP1_sigma,mode=\"fan_in\",distribution=\"truncated_normal\"))\n",
    "        self.linear1_2 = Linear_MLP2(in_features=layer_dim // 2, out_features=encoded_dim, rngs=rngs,kernel_init=jax.nn.initializers.variance_scaling(scale=MLP2_sigma,mode=\"fan_in\",distribution=\"truncated_normal\"))\n",
    "        self.linear2_1 = Linear_MLP1(in_features=encoded_dim, out_features=layer_dim, rngs=rngs,kernel_init=jax.nn.initializers.variance_scaling(scale=MLP1_sigma,mode=\"fan_in\",distribution=\"truncated_normal\"))\n",
    "        self.linear2_2 = Linear_MLP2(in_features=layer_dim // 2, out_features=encoded_dim, rngs=rngs,kernel_init=jax.nn.initializers.variance_scaling(scale=MLP2_sigma,mode=\"fan_in\",distribution=\"truncated_normal\"))\n",
    "        self.linear3_1 = Linear_MLP1(in_features=encoded_dim, out_features=layer_dim, rngs=rngs,kernel_init=jax.nn.initializers.variance_scaling(scale=MLP1_sigma,mode=\"fan_in\",distribution=\"truncated_normal\"))\n",
    "        self.linear3_2 = Linear_MLP2(in_features=layer_dim // 2, out_features=encoded_dim, rngs=rngs,kernel_init=jax.nn.initializers.variance_scaling(scale=MLP2_sigma,mode=\"fan_in\",distribution=\"truncated_normal\"))\n",
    "        self.linear4_1 = Linear_MLP1(in_features=encoded_dim, out_features=layer_dim, rngs=rngs,kernel_init=jax.nn.initializers.variance_scaling(scale=MLP1_sigma,mode=\"fan_in\",distribution=\"truncated_normal\"))\n",
    "        self.linear4_2 = Linear_MLP2(in_features=layer_dim // 2, out_features=encoded_dim, rngs=rngs,kernel_init=jax.nn.initializers.variance_scaling(scale=MLP2_sigma,mode=\"fan_in\",distribution=\"truncated_normal\"))\n",
    "        self.linear5_1 = Linear_MLP1(in_features=encoded_dim, out_features=layer_dim, rngs=rngs,kernel_init=jax.nn.initializers.variance_scaling(scale=MLP1_sigma,mode=\"fan_in\",distribution=\"truncated_normal\"))\n",
    "        self.linear5_2 = Linear_MLP2(in_features=layer_dim // 2, out_features=encoded_dim, rngs=rngs,kernel_init=jax.nn.initializers.variance_scaling(scale=MLP2_sigma,mode=\"fan_in\",distribution=\"truncated_normal\"))\n",
    "        self.linear6_1 = Linear_MLP1(in_features=encoded_dim, out_features=layer_dim, rngs=rngs,kernel_init=jax.nn.initializers.variance_scaling(scale=MLP1_sigma,mode=\"fan_in\",distribution=\"truncated_normal\"))\n",
    "        self.linear6_2 = Linear_MLP2(in_features=layer_dim // 2, out_features=encoded_dim, rngs=rngs,kernel_init=jax.nn.initializers.variance_scaling(scale=MLP2_sigma,mode=\"fan_in\",distribution=\"truncated_normal\"))\n",
    "        self.batchnorm1 = nnx.BatchNorm(num_features=encoded_dim, rngs=rngs)\n",
    "        self.batchnorm2 = nnx.BatchNorm(num_features=encoded_dim, rngs=rngs)\n",
    "        self.batchnorm3 = nnx.BatchNorm(num_features=encoded_dim, rngs=rngs)\n",
    "        self.batchnorm4 = nnx.BatchNorm(num_features=encoded_dim, rngs=rngs)\n",
    "        self.batchnorm5 = nnx.BatchNorm(num_features=encoded_dim, rngs=rngs)\n",
    "        self.batchnorm6 = nnx.BatchNorm(num_features=encoded_dim, rngs=rngs)\n",
    "\n",
    "        # Linear layers\n",
    "        if pool:  # If pooling layer takes the average over the token sequence length\n",
    "            self.linear3 = lambda x: jnp.mean(x, axis=1)\n",
    "        else:  # learn the parameters of the linear transformation\n",
    "            self.linear3 = nnx.Linear(in_features=token_len, out_features=1, rngs=rngs)\n",
    "        self.linear4 = Linear_out(\n",
    "            in_features=encoded_dim, out_features=out_dim, rngs=rngs, kernel_init=jax.nn.initializers.variance_scaling(scale=out_sigma,mode=\"fan_in\",distribution=\"truncated_normal\")\n",
    "        )\n",
    "        self.out_dim = out_dim\n",
    "        self.token_len = token_len\n",
    "        self.dropout = nnx.Dropout(dropout, rngs=rngs)\n",
    "        \n",
    "    @nnx.vmap(in_axes=(None, 0))\n",
    "    def block_after_batchnorm1(self, x):\n",
    "        x = self.rnn1(x)\n",
    "        x = self.linear1_1(x)\n",
    "        x = nnx.glu(x, axis=-1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear1_2(x)\n",
    "        return x\n",
    "\n",
    "    @nnx.vmap(in_axes=(None, 0))\n",
    "    def block_after_batchnorm2(self, x):\n",
    "        x = self.rnn2(x)\n",
    "        x = self.linear2_1(x)\n",
    "        x = nnx.glu(x, axis=-1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2_2(x)\n",
    "        return x\n",
    "\n",
    "    @nnx.vmap(in_axes=(None, 0))\n",
    "    def block_after_batchnorm3(self, x):\n",
    "        x = self.rnn3(x)\n",
    "        x = self.linear3_1(x)\n",
    "        x = nnx.glu(x, axis=-1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear3_2(x)\n",
    "        return x\n",
    "\n",
    "    @nnx.vmap(in_axes=(None, 0))\n",
    "    def block_after_batchnorm4(self, x):\n",
    "        x = self.rnn4(x)\n",
    "        x = self.linear4_1(x)\n",
    "        x = nnx.glu(x, axis=-1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear4_2(x)\n",
    "        return x\n",
    "\n",
    "    @nnx.vmap(in_axes=(None, 0))\n",
    "    def block_after_batchnorm5(self, x):\n",
    "        x = self.rnn5(x)\n",
    "        x = self.linear5_1(x)\n",
    "        x = nnx.glu(x, axis=-1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear5_2(x)\n",
    "        return x\n",
    "\n",
    "    @nnx.vmap(in_axes=(None, 0))\n",
    "    def block_after_batchnorm6(self, x):\n",
    "        x = self.rnn6(x)\n",
    "        x = self.linear6_1(x)\n",
    "        x = nnx.glu(x, axis=-1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear6_2(x)\n",
    "        return x\n",
    "\n",
    "    @nnx.vmap(in_axes=(None, 0))\n",
    "    def final_linear_projections(self, x):\n",
    "        x = self.linear3(x.T)\n",
    "        x = self.linear4(x.T)\n",
    "        return x.reshape(self.out_dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.lin_encoder(x)\n",
    "        y = x.copy()\n",
    "\n",
    "        # LRU+MLP block*6\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.block_after_batchnorm1(x)\n",
    "        x += y\n",
    "\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.block_after_batchnorm2(x)\n",
    "        x += y\n",
    "\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.block_after_batchnorm3(x)\n",
    "        x += y\n",
    "\n",
    "        x = self.batchnorm4(x)\n",
    "        x = self.block_after_batchnorm4(x)\n",
    "        x += y\n",
    "\n",
    "        x = self.batchnorm5(x)\n",
    "        x = self.block_after_batchnorm5(x)\n",
    "        x += y\n",
    "\n",
    "        x = self.batchnorm6(x)\n",
    "        x = self.block_after_batchnorm6(x)\n",
    "        x += y\n",
    "\n",
    "        return self.final_linear_projections(x)\n",
    "\n",
    "\n",
    "\n",
    "model = MLP(\n",
    "    train_x_size,\n",
    "    train_x_len,\n",
    "    encoded_size,\n",
    "    hidden_size,\n",
    "    hidden_neuron,\n",
    "    train_y_class,\n",
    "    rngs=rngs1,\n",
    ")  # eager initialization\n",
    "\n",
    "#nnx.display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ur7p-0rlBbO"
   },
   "outputs": [],
   "source": [
    "def group_tuples_to_nested_dict(params):\n",
    "    nested_dict = {}\n",
    "    for outer_key, inner_key in params:\n",
    "        if outer_key not in nested_dict:\n",
    "            nested_dict[outer_key] = {}\n",
    "        nested_dict[outer_key][inner_key] = inner_key\n",
    "    return nested_dict\n",
    "param = nnx.state(model,nnx.Param).flat_state()\n",
    "gr=group_tuples_to_nested_dict(list(param.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wnzxIgYEkbNK"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Set optimization method per layer\n",
    "rnn_schedule=optax.warmup_cosine_decay_schedule(init_value=1e-7*lr_factor, peak_value=learning_rate*lr_factor, warmup_steps=train_steps//10, decay_steps=train_steps, end_value=1e-7*lr_factor)\n",
    "lin_schedule=optax.warmup_cosine_decay_schedule(init_value=1e-7, peak_value=learning_rate, warmup_steps=train_steps//10, decay_steps=train_steps, end_value=1e-7)\n",
    "lin_encoder_schedule=optax.warmup_cosine_decay_schedule(init_value=1e-7*lin_encoder_lr, peak_value=learning_rate*lin_encoder_lr, warmup_steps=train_steps//10, decay_steps=train_steps, end_value=1e-7*lin_encoder_lr)\n",
    "MLP1_schedule=optax.warmup_cosine_decay_schedule(init_value=1e-7*MLP1_lr, peak_value=learning_rate*MLP1_lr, warmup_steps=train_steps//10, decay_steps=train_steps, end_value=1e-7*MLP1_lr)\n",
    "MLP2_schedule=optax.warmup_cosine_decay_schedule(init_value=1e-7*MLP2_lr, peak_value=learning_rate*MLP2_lr, warmup_steps=train_steps//10, decay_steps=train_steps, end_value=1e-7*MLP2_lr)\n",
    "out_schedule=optax.warmup_cosine_decay_schedule(init_value=1e-7*out_lr, peak_value=learning_rate*out_lr, warmup_steps=train_steps//10, decay_steps=train_steps, end_value=1e-7*out_lr)\n",
    "\n",
    "#lr factor only on A and B so C,D same as the linear layers -> although going up slower, less overfitting -> why?\n",
    "#No sensitive fitting -> less overfitting?\n",
    "d={\"B_re\":optax.adamw(rnn_schedule),\n",
    "   \"B_im\":optax.adamw(rnn_schedule),\n",
    "    'C_im': optax.adamw(lin_schedule,weight_decay=0.05),\n",
    "    'C_re': optax.adamw(lin_schedule,weight_decay=0.05),\n",
    "    'D': optax.adamw(lin_schedule,weight_decay=0.05),\n",
    "    'gamma_log': optax.adamw(rnn_schedule),\n",
    "    'nu_log': optax.adamw(rnn_schedule),\n",
    "    'theta_log': optax.adamw(rnn_schedule),\n",
    "    \n",
    "    #if linear3 not pooling\n",
    "    'kernel':optax.adamw(lin_schedule,weight_decay=0.05),\n",
    "    'bias':optax.adamw(lin_schedule,weight_decay=0.05),\n",
    "\n",
    "     \"lin_encoder_kernel\": optax.adamw(lin_encoder_schedule,weight_decay=0.05),\n",
    "     \"MLP1_kernel\": optax.adamw(MLP1_schedule,weight_decay=0.05),\n",
    "     \"MLP2_kernel\": optax.adamw(MLP2_schedule,weight_decay=0.05),\n",
    "     \"out_kernel\": optax.adamw(out_schedule,weight_decay=0.05),\n",
    "\n",
    "     \"lin_encoder_bias\": optax.adamw(lin_encoder_schedule,weight_decay=0.05),\n",
    "     \"MLP1_bias\": optax.adamw(MLP1_schedule,weight_decay=0.05),\n",
    "     \"MLP2_bias\": optax.adamw(MLP2_schedule,weight_decay=0.05),\n",
    "     \"out_bias\": optax.adamw(out_schedule,weight_decay=0.05),\n",
    "\n",
    "     \"scale\": optax.adamw(lin_schedule,weight_decay=0.05),\n",
    "     }\n",
    "\n",
    "tx=optax.multi_transform(d,nnx.State(gr))\n",
    "\n",
    "optimizer = nnx.Optimizer(model, tx)\n",
    "metrics = nnx.MultiMetric(\n",
    "    accuracy=nnx.metrics.Accuracy(),\n",
    "    loss=nnx.metrics.Average(\"loss\"),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9sJOB_owm8YF"
   },
   "outputs": [],
   "source": [
    "def loss_fn(model: MLP, batch):\n",
    "  logits = model(batch[0])\n",
    "  loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "    logits=logits, labels=batch[1]\n",
    "  ).mean()\n",
    "  #print(logits.shape)\n",
    "  #print(batch[1].shape)\n",
    "  return loss, logits\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(model: MLP, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
    "  \"\"\"Train for a single step.\"\"\"\n",
    "  grad_fn = nnx.value_and_grad(loss_fn,has_aux=True)\n",
    "  (loss, logits), grads = grad_fn(model, batch)\n",
    "  metrics.update(loss=loss, logits=logits, labels=batch[1])  # In-place updates.\n",
    "  optimizer.update(grads)  # In-place updates.\n",
    "  predicted_labels = jnp.argmax(logits, axis=-1)\n",
    "  actual_labels = batch[1]\n",
    "  #jax.debug.print(\"Predictions: {}\",predicted_labels[:5].astype(int))\n",
    "  #jax.debug.print(\"Actual Labels: {}\",actual_labels[:5].astype(int))\n",
    "\n",
    "@nnx.jit\n",
    "def eval_step(model: MLP, metrics: nnx.MultiMetric, batch):\n",
    "  loss, logits = loss_fn(model, batch)\n",
    "  metrics.update(loss=loss, logits=logits, labels=batch[1])  # In-place updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ybC9wSr7m8YF",
    "outputId": "306bc630-8094-4776-db41-53f282b7e030"
   },
   "outputs": [],
   "source": [
    "#Train the model + evaluation with the test data\n",
    "metrics_history = {\n",
    "    'train_loss': [],\n",
    "    'train_accuracy': [],\n",
    "    'test_loss': [],\n",
    "    'test_accuracy': [],\n",
    "}\n",
    "\n",
    "for step, batch in enumerate(train_ds.as_numpy_iterator()):\n",
    "  # Run the optimization for one step and make a stateful update to the following:\n",
    "  # - The train state's model parameters\n",
    "  # - The optimizer state\n",
    "  # - The training loss and accuracy batch metrics\n",
    "  train_step(model, optimizer, metrics, batch)\n",
    "\n",
    "  if step > 0 and (step % eval_every == 0 or step == train_steps - 1):  # One training epoch has passed.\n",
    "    # Log the training metrics.\n",
    "    for metric, value in metrics.compute().items():  # Compute the metrics.\n",
    "      metrics_history[f'train_{metric}'].append(value)  # Record the metrics.\n",
    "    metrics.reset()  # Reset the metrics for the test set.\n",
    "\n",
    "    # Compute the metrics on the test set after each training epoch.\n",
    "    for test_batch in test_ds.as_numpy_iterator():\n",
    "      eval_step(model, metrics, test_batch)\n",
    "\n",
    "    # Log the test metrics.\n",
    "    for metric, value in metrics.compute().items():\n",
    "      metrics_history[f'test_{metric}'].append(value)\n",
    "    metrics.reset()  # Reset the metrics for the next training epoch.\n",
    "\n",
    "    print(\n",
    "      f\"[train] step: {step}       , \"\n",
    "      f\"loss: {metrics_history['train_loss'][-1]}      , \"\n",
    "      f\"accuracy: {metrics_history['train_accuracy'][-1] * 100}     \"\n",
    "    )\n",
    "    print(\n",
    "      f\"[test] step: {step}        , \"\n",
    "      f\"loss: {metrics_history['test_loss'][-1]}       , \"\n",
    "      f\"accuracy: {metrics_history['test_accuracy'][-1] * 100}      \"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXQS3ikfm8YG"
   },
   "outputs": [],
   "source": [
    "#Save the training results into csv\n",
    "import pandas as pd\n",
    "\n",
    "if leave_data:\n",
    "    data=pd.DataFrame({\"step\":np.arange(eval_every,train_steps+eval_every,eval_every),\"train_loss\":metrics_history['train_loss'],\n",
    "                       \"test_loss\":metrics_history['test_loss'],\"train_accuracy\":metrics_history['train_accuracy'],\n",
    "                       \"test_accuracy\":metrics_history['test_accuracy']})\n",
    "    data.to_csv(method_name+\"_H\"+str(encoded_size)+\"_nr\"+str(hidden_neuron)+\"_D\"+str(hidden_size)+\"_\"+dataset_name+\"_lr\"+str(learning_rate)+\"_step\"+str(train_steps)+\"r_min_\"+str(r_min)+\"r_max\"+str(r_max)+\"_rand\"+str(rand)+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZqz17Evm8YH"
   },
   "outputs": [],
   "source": [
    "#Plot the loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.arange(eval_every,train_steps+eval_every,eval_every),metrics_history['train_loss'],label=\"train loss\")\n",
    "plt.plot(np.arange(eval_every,train_steps+eval_every,eval_every),metrics_history['test_loss'],label=\"test loss\")\n",
    "plt.title(\"Train loss of \"+dataset_name+\" dataset with \"+method_name+\n",
    "              \", \\nhidden dimension=\"+str(hidden_size)+\", number of neuron=\"+str(hidden_neuron))\n",
    "plt.xlabel(\"Training step\")\n",
    "plt.ylabel(\"Train loss (cross entropy)\")\n",
    "plt.legend()\n",
    "if leave_data:\n",
    "    plt.savefig(\"loss_\"+method_name+\"_\"+str(encoded_size)+\"_\"+str(hidden_neuron)+\"_\"+dataset_name+\"_lr\"+str(learning_rate)+\"_step\"+str(train_steps)+\"r_min_\"+str(r_min)+\"r_max\"+str(r_max)+\"_rand\"+str(rand)+\".jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idfocImUm8YH"
   },
   "outputs": [],
   "source": [
    "#Plot the accuracy\n",
    "plt.plot(np.arange(eval_every,train_steps+eval_every,eval_every),metrics_history['train_accuracy'],label=\"train\")\n",
    "plt.plot(np.arange(eval_every,train_steps+eval_every,eval_every),metrics_history['test_accuracy'],label=\"test\")\n",
    "plt.title(\"Accuracy of \"+dataset_name+\" dataset with \"+method_name+\", \\nhidden dimension=\"+\n",
    "          str(hidden_size)+\", number of neuron=\"+str(hidden_neuron))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "if leave_data:\n",
    "    plt.savefig(\"accuracy_\"+method_name+\"_\"+str(encoded_size)+\"_\"+str(hidden_neuron)+\"_\"+dataset_name+\"_lr\"+str(learning_rate)+\"_step\"+str(train_steps)+\"r_min_\"+str(r_min)+\"r_max\"+str(r_max)+\"_rand\"+str(rand)+\".jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pY7u9HdXdETB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
